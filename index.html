<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: October 23, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Naomi Saphra"><meta name=description content="A natural language processing researcher."><link rel=alternate hreflang=en-us href=https://nsaphra.github.io/><link rel=canonical href=https://nsaphra.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nsaphra"><meta property="twitter:creator" content="@nsaphra"><meta property="twitter:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Naomi Saphra"><meta property="og:url" content="https://nsaphra.github.io/"><meta property="og:title" content="Naomi Saphra"><meta property="og:description" content="A natural language processing researcher."><meta property="og:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-10-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://nsaphra.github.io?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://nsaphra.github.io"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Naomi Saphra"><title>Naomi Saphra</title><link rel=me href=https://mastodon.online/@nsaphra></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper data-wc-page-id=3976528693a0108357f4928017600865><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Naomi Saphra</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Naomi Saphra</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts data-target=#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#featured data-target=#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks data-target=#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/uploads/academic_cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://sigmoid.social/@nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Mastodon" target=_blank rel=noopener aria-label="Follow me on Mastodon"><i class="fab fa-mastodon" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-v1/about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hu90b276abf902758fc55813a02decabb9_157010_270x270_fill_q75_lanczos_center.jpg alt="Naomi Saphra"><div class=portrait-title><h2>Naomi Saphra</h2><h3>Gradient Descent Spectator</h3><x-small id=pronouns value=first>they / she</x-small>
<script>window.onload=function(){$("#pronouns").on("click",function(){var t=/(they|she)/i.test($("#pronouns").attr("value")),n=/she/i.test($("#pronouns").attr("value"));$(".change-person").each(function(){$(this).text(t?n?$(this).attr("i-content"):$(this).attr("she-content"):$(this).attr("they-content"))}),$("#pronouns").attr("value",t?n?"first":"she":"they")}),$("#pronouns").css({cursor:"pointer"})}</script></div><ul class=network-icon aria-hidden=true><li><a href=mailto:nsaphra@nsaphra.net aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href=https://sigmoid.social/@nsaphra target=_blank rel=noopener aria-label=mastodon><i class="fab fa-mastodon big-icon"></i></a></li><li><a href=https://twitter.com/nsaphra target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.co.uk/citations?user=TPhVfX8AAAAJ" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://www.semanticscholar.org/author/Naomi-Saphra/2362960 target=_blank rel=noopener aria-label=semantic-scholar><i class="ai ai-semantic-scholar big-icon"></i></a></li><li><a href=https://github.com/nsaphra target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><div class=article-style><p style=text-align:justify><span class=change-person i-content="I am" they-content="Naomi Saphra is" she-content="Naomi Saphra is">I am</span> a research fellow at the <a href=https://www.harvard.edu/kempner-institute/ target=_blank rel=noopener>Kempner Institute at Harvard University</a>. <span class=change-person i-content="I am" they-content="They are" she-content="She is">I am</span> interested in NLP training dynamics: how models learn to encode linguistic patterns or other structure and how we can encode useful inductive biases into the training process. Previously, <span class=change-person i-content=I they-content=they she-content=she>I</span> earned a PhD from the University of Edinburgh on <a href=uploads/thesis.pdf>Training Dynamics of Neural Language Models</a>; worked at NYU, Google and Facebook; and attended Johns Hopkins and Carnegie Mellon University. Outside of research, <span class=change-person i-content="I play" they-content="they play" she-content="she plays">I play</span> roller derby under the name <a href=https://auldreekierollerderby.com/2019/08/10/the-one-gift-i-received-along-with-my-disability/ target=_blank rel=noopener>Gaussian Retribution</a>, <span class=change-person i-content=do they-content=do she-content=does>do</span> <a href="https://www.youtube.com/watch?v=BzNDdS-lcqM" target=_blank rel=noopener>standup comedy</a>, and <span class=change-person i-content=shepherd they-content=shepherd she-content=shepherds>shepherd</span> disabled programmers into the world of <a href=post/hands/>code dictation</a>.</p></div><div class=row><div class=col-md-5><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Artificial Intelligence</li><li>Natural Language Processing</li><li>Training Dynamics</li><li>Compositionality</li><li>Generalization</li><li>Loss Surfaces</li></ul></div><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in Informatics, 2021</p><p class=institution>University of Edinburgh</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MEng in Computer Science, 2015</p><p class=institution>Johns Hopkins University</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer Science, 2013</p><p class=institution>Carnegie Mellon University</p></div></li></ul></div></div></div></div></div></section><section id=posts class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Posts</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/prinia/>The Parable of the Prinia’s Egg: An Allegory for AI Science</a></div><a href=/post/prinia/ class=summary-link><div class=article-style>I discuss what counts as strong evidence for an explanation of model behavior.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Sep 17, 2023</span>
<span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/creationism/>Interpretability Creationism</a></div><a href=/post/creationism/ class=summary-link><div class=article-style>Nothing in Deep Learning Makes Sense Except in the Light of SGD.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><div><span>Naomi Saphra</span></div><span class=article-date>Jun 7, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>7 min read</span></div></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/post/monodomainism/>Against Monodomainism</a></div><a href=/post/monodomainism/ class=summary-link><div class=article-style>A petty rant on the exceptional treatment of computer vision applications, directed at the machine learning community.</div></a><div class="stream-meta article-metadata"><div class=article-metadata><span class=article-date>Apr 28, 2021</span>
<span class=middot-divider></span>
<span class=article-reading-time>3 min read</span></div></div></div><div class=ml-3></div></div><div class=see-all><a href=/post/>See all posts
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=featured class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Featured Publications</h1></div><div class="col-12 col-lg-8"><div class="card-simple view-card"><div class=article-metadata><div><span>Jeevesh Juneja</span>, <span>Rachit Bansal</span>, <span>Kyunghyun Cho</span>, <span>João Sedoc</span>, <span class=author-highlighted>Naomi Saphra</span></div><span class=article-date>May, 2023</span>
<span class=middot-divider></span>
<span class=pub-publication><em>ICLR</em></span></div><a href=/publication/juneja-linear-2022/><div class=img-hover-zoom><img src=/publication/juneja-linear-2022/featured_hu15b305882d8b2c5676d9e5649830647c_701967_808x455_fill_q75_h2_lanczos_smart1_3.webp height=455 width=808 class=article-banner alt="Linear Connectivity Reveals Generalization Strategies" loading=lazy></div></a><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/juneja-linear-2022/>Linear Connectivity Reveals Generalization Strategies</a></div><a href=/publication/juneja-linear-2022/ class=summary-link><div class=article-style><p>It is widely accepted in the mode connectivity literature that when two neural networks are trained similarly on the same data, they are connected by a path through parameter space over which test set accuracy is maintained. Under some circumstances, including transfer learning from pretrained models, these paths are presumed to be linear. In contrast to existing results, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster &ndash; models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we find that these clusters correspond to different generalization strategies: one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2205.12411 target=_blank rel=noopener>PDF</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span>Michael Hu</span>, <span>Angelica Chen</span>, <span class=author-highlighted>Naomi Saphra</span>, <span>Kyunghyun Cho</span></div><span class=article-date>January, 2023</span>
<span class=middot-divider></span>
<span class=pub-publication><em>ICML High-Dimensional Learning Dynamics Workshop</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/hu-latent/>Latent State Transitions in Training Dynamics</a></div><div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/hu-latent/cite.bib>Cite</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Naomi Saphra</span>, <span>Adam Lopez</span></div><span class=article-date>January, 2020</span>
<span class=middot-divider></span>
<span class=pub-publication><em>Findings of EMNLP</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/saphra-lstms-2020/>LSTMs Compose (and Learn) Bottom-Up</a></div><a href=/publication/saphra-lstms-2020/ class=summary-link><div class=article-style><p>Recent work in NLP shows that LSTM language models capture hierarchical structure in language data. In contrast to existing work, we consider the <em>learning</em> process that leads to their compositional behavior. For a closer look at how an LSTM&rsquo;s sequential representations are composed hierarchically, we present a related measure of Decompositional Interdependence (DI) between word meanings in an LSTM, based on their gate interactions. We connect this measure to syntax with experiments on English language data, where DI is higher on pairs of words with lower syntactic distance. To explore the inductive biases that cause these compositional representations to arise during training, we conduct simple experiments on synthetic data. These synthetic experiments support a specific hypothesis about how hierarchical structures are discovered over the course of training: that LSTM constituent representations are learned bottom-up, relying on effective representations of their shorter children, rather than learning the longer-range relations independently from children.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2010.04650 target=_blank rel=noopener>PDF</a></div></div><div class="card-simple view-card"><div class=article-metadata><div><span class=author-highlighted>Naomi Saphra</span>, <span>Adam Lopez</span></div><span class=article-date>January, 2019</span>
<span class=middot-divider></span>
<span class=pub-publication><em>NAACL</em></span></div><div class="section-subheading article-title mb-1 mt-3"><a href=/publication/saphra-understanding-2019/>Understanding Learning Dynamics Of Language Models with SVCCA</a></div><a href=/publication/saphra-understanding-2019/ class=summary-link><div class=article-style><p>Research has shown that neural models implicitly encode linguistic features, but there has been no research showing <em>how</em> these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations and find that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.</p></div></a><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.aclweb.org/anthology/N19-1329 target=_blank rel=noopener>PDF</a></div></div></div></div></div></section><section id=section-collection class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=./publication/>filtering publications</a>.</div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Dieuwke Hupkes</span>, <span>Mario Giulianelli</span>, <span>Verna Dankers</span>, <span>Mikel Artetxe</span>, <span>Yanai Elazar</span>, <span>Tiago Pimentel</span>, <span>Christos Christodoulopoulos</span>, <span>Karim Lasri</span>, <span class=author-highlighted>Naomi Saphra</span>, <span>Arabella Sinclair</span>, <span>Dennis Ulmer</span>, <span>Florian Schottmann</span>, <span>Khuyagbaatar Batsuren</span>, <span>Kaiser Sun</span>, <span>Koustuv Sinha</span>, <span>Leila Khalatbari</span>, <span>Maria Ryskina</span>, <span>Rita Frieske</span>, <span>Ryan Cotterell</span>, <span>Zhijing Jin</span></span>
(2023).
<a href=/publication/hupkes-state-art-2022/>State-of-the-art generalisation research in NLP: a taxonomy and review</a>.
<em>Nature Machine Intelligence</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/hupkes-state-art-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.48550/arXiv.2210.03050 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2210.03050 target=_blank rel=noopener>URL</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zachary Ankner</span>, <span class=author-highlighted>Naomi Saphra</span>, <span>Davis Blalock</span>, <span>Jonathan Frankle</span>, <span>Matthew L. Leavitt</span></span>
(2023).
<a href=/publication/ankner/>Dynamic Masking Rate Schedules for MLM Pretraining</a>.
<em>arXiv preprint arXiv:2305.15096</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ankner/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span class=author-highlighted>Naomi Saphra</span></span>
(2023).
<a href=/publication/saphra-2023-interp/>Interpretability Creationism</a>.
<em>The Gradient</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/saphra-2023-interp/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://thegradient.pub/interpretability-creationism target=_blank rel=noopener>URL</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bingchen Zhao</span>, <span>Yuling Gu</span>, <span>Jessica Zosa Forde</span>, <span class=author-highlighted>Naomi Saphra</span></span>
(2022).
<a href=/publication/zhao-one-2022/>One Venue, Two Conferences: The Separation of Chinese and American Citation Networks</a>.
<em>NeurIPS Workshop on Cultures in AI</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2211.12424 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhao-one-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Josef Valvoda</span>, <span class=author-highlighted>Naomi Saphra</span>, <span>Jonathan Rawski</span>, <span>Adina Williams</span>, <span>Ryan Cotterell</span></span>
(2022).
<a href=/publication/valvoda-benchmarking-2022/>Benchmarking Compositionality with Formal Languages</a>.
<em>COLING</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.coling-1.525 target=_blank rel=noopener>PDF</a></p></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=talks class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent & Upcoming Talks</h1></div><div class="col-12 col-lg-8"><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/sources-of-variance-in-pretraining-and-finetuning/>Sources of Variance in Pretraining and Finetuning</a></div><a href=/talk/sources-of-variance-in-pretraining-and-finetuning/ class=summary-link><div class=article-style><p>You have engaged in the very modern practice of transfer learning. You pretrained a model on a self-supervised objective, then you …</p></div></a><div class="stream-meta article-metadata"><div><span>Jun 20, 2022 1:00 PM</span>
<span class=middot-divider></span>
<span>Los Angeles, CA</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=Lni4PIlbJjI" target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/accessible-means-hackable-keynote/>Accessible Means Hackable (Keynote)</a></div><a href=/talk/accessible-means-hackable-keynote/ class=summary-link><div class=article-style>In 2015, after over a decade of programming, I lost the ability to type. Confronted with a programmer’s worst nightmare, I began the …</div></a><div class="stream-meta article-metadata"><div><span>Aug 15, 2020 1:00 PM</span>
<span class=middot-divider></span>
<span>Remote due to pandemic</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=BKYWt8B9hgs" target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/talk/learning-dynamics-of-lstms/>Learning Dynamics of LSTMs</a></div><div class="stream-meta article-metadata"><div><span>May 28, 2019 1:00 PM</span>
<span class=middot-divider></span>
<span>Remote</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=R0d_XC_NYb4" target=_blank rel=noopener>Video</a></div></div><div class=ml-3></div></div><div class=see-all><a href=/event/>See all events
<i class="fas fa-angle-right"></i></a></div></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>