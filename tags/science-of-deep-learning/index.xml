<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Science of Deep Learning | Naomi Saphra</title><link>https://nsaphra.github.io/tags/science-of-deep-learning/</link><atom:link href="https://nsaphra.github.io/tags/science-of-deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>Science of Deep Learning</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate><image><url>https://nsaphra.github.io/media/icon_hu_ecc3d54b494abbac.png</url><title>Science of Deep Learning</title><link>https://nsaphra.github.io/tags/science-of-deep-learning/</link></image><item><title>How to visualize training dynamics in neural networks</title><link>https://nsaphra.github.io/publication/hublog/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/hublog/</guid><description/></item><item><title>Attribute Diversity Determines the Systematicity Gap in VQA</title><link>https://nsaphra.github.io/publication/systematicity/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/systematicity/</guid><description/></item><item><title>Causation Does Not Imply Correlation: A Study of Circuit Mechanisms and Model Behaviors</title><link>https://nsaphra.github.io/publication/jenny/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/jenny/</guid><description/></item><item><title>Sometimes I am a Tree: Data drives fragile hierarchical generalization</title><link>https://nsaphra.github.io/publication/sunny/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/sunny/</guid><description/></item><item><title>Transcendence: Generative Models Can Outperform The Experts That Train Them</title><link>https://nsaphra.github.io/publication/zhang-2024-transcendence/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/zhang-2024-transcendence/</guid><description/></item><item><title>Delays, Detours, and Forks in the Road: Latent State Models of Training Dynamics</title><link>https://nsaphra.github.io/publication/hu/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/hu/</guid><description/></item><item><title>Linear Connectivity Reveals Generalization Strategies</title><link>https://nsaphra.github.io/publication/juneja-linear-2023/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/juneja-linear-2023/</guid><description/></item><item><title>Learning Transductions to Test Systematic Compositionality</title><link>https://nsaphra.github.io/publication/valvoda-learning-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/valvoda-learning-2022/</guid><description/></item><item><title>The MultiBERTs: BERT Reproductions for Robustness Analysis</title><link>https://nsaphra.github.io/publication/sellam-multiberts-2021/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/sellam-multiberts-2021/</guid><description/></item><item><title>LSTMs Compose---and Learn---Bottom-Up</title><link>https://nsaphra.github.io/publication/saphra-lstms-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/saphra-lstms-2020/</guid><description/></item><item><title>Sparsity Emerges Naturally in Neural Language Models</title><link>https://nsaphra.github.io/publication/saphra-sparsity/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/publication/saphra-sparsity/</guid><description/></item></channel></rss>