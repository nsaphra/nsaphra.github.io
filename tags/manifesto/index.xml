<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Manifesto | Naomi Saphra</title><link>https://nsaphra.github.io/tags/manifesto/</link><atom:link href="https://nsaphra.github.io/tags/manifesto/index.xml" rel="self" type="application/rss+xml"/><description>Manifesto</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 17 Sep 2023 00:00:00 +0000</lastBuildDate><image><url>https://nsaphra.github.io/media/icon_hu_ecc3d54b494abbac.png</url><title>Manifesto</title><link>https://nsaphra.github.io/tags/manifesto/</link></image><item><title>The Parable of the Prinia's Egg: An Allegory for AI Science</title><link>https://nsaphra.github.io/post/prinia/</link><pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/post/prinia/</guid><description>&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/eggs.jpg" alt="Prinia eggs" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>When European scientists first encountered the eggs of the tawny-flanked prinia &lt;em>Prinia subflava&lt;/em>, an African nesting bird, they believed they understood what they had found. The prinia lay eggs that exhibited swirls, speckles, and coloration unique to each individual bird. What purpose do such patterns serve in nature? Surely, scientists agreed, these markings allowed the eggs to camouflage and blend into the nest.&lt;/p>
&lt;p>One 19th century naturalist, Charles Francis Massey Swynnerton, offered an alternative explanation. Why would each bird have its own unique patterns? Swynnerton believed that the markings functioned as watermarks, allowing the nesting bird to differentiate its own eggs from those of the cuckoo finch &lt;em>Anomalospiza imberbis&lt;/em>. The cuckoo finch is an &lt;strong>obligate brood parasite&lt;/strong>, meaning that it does not build its own nest, but instead lays its eggs in the nests of other birds, particularly the prinia.&lt;/p>
&lt;p>Since its proposal, evidence for Swynnerton&amp;rsquo;s hypothesis has accumulated. The camouflage hypothesis was based on anecdotal observations of eggs, without correlational or experimental data. In contrast, evidence for the brood parasite hypothesis comes in diverse forms, using many different frameworks for reasoning about causality and explanation, adding up to an indisputable scientific arsenal. What can AI researchers learn from the mature science of biology to strengthen our own evidence and reliably interpret how neural network traits contribute to model behavior?&lt;/p>
&lt;h2 id="instance-level-interpretability">Instance level interpretability&lt;/h2>
&lt;p>Most interpretability work, and much work in science of deep learning, relies on passive observations of phenomena with post-hoc explanations for how artifacts in the model contribute to model outputs. Like the early ornithologists suggesting that prinia eggs were camouflaged, interpretability researchers often identify some phenomenon and link it to model decisions based solely on their intuitions. Some work attempts a more principled approach, by intervening on the trained model to demonstrate that a given feature or circuit has a predictable effect on model judgment. Both approaches might be classed as &lt;strong>instance-level&lt;/strong>, as they consider the behavior of a fully trained model on a given input sample.&lt;/p>
&lt;p>In the past, I have called this reliance on fully trained models &lt;a href="https://thegradient.pub/interpretability-creationism/" target="_blank" rel="noopener">Interpretability Creationism&lt;/a>, and suggested that interpretability researchers instead measure the indicators they focus on throughout training. However, I have not detailed how these creationist approaches can lead us astray, nor have I proposed a clear framework for understanding development. Such a framework falls under the philosophical field of &lt;strong>epistemology&lt;/strong>, the study of what makes a belief justified.&lt;/p>
&lt;h3 id="syntactic-attention-structure">Syntactic Attention Structure&lt;/h3>
&lt;p>To address the epistemology of interpretability, let us construct a case study of the scientific literature on an observed phenomenon in neural networks. One well-documented and intuitive behavior is how Transformer-based &lt;strong>masked language models&lt;/strong> (MLMs) have specialized attention heads that focus on a specific dependency relation, a trait we will call &lt;strong>Syntactic Attention Structure&lt;/strong> (SAS). We can illustrate SAS with an example sentence.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/parse_example.png" alt="My bird builds ugly nests" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>In the preceding sentence, the model might be called upon to predict the masked-out target word &lt;em>builds&lt;/em>. During inference, its specialized &lt;code>nsubj&lt;/code> head will place its highest weight on &lt;em>bird&lt;/em>, while its specialized &lt;code>dobj&lt;/code> head will place its highest weight on &lt;em>nests&lt;/em>. This specialization behavior emerges naturally, without any explicit inductive bias, over the normal course of training for models like BERT.&lt;/p>
&lt;p>SAS was discovered concurrently in two papers. First, &lt;a href="https://aclanthology.org/W19-4828/" target="_blank" rel="noopener">Clark et al. (2019)&lt;/a> observed that specialized attention heads provide an implicit parse, which aligns with prior notions of dependency syntax, as measured by &lt;strong>Unlabeled Attachment Score&lt;/strong> (UAS). This observation provides instance-level observational evidence for the role of SAS in masked language modeling. Second, &lt;a href="https://aclanthology.org/P19-1580/" target="_blank" rel="noopener">Voita et al. (2019)&lt;/a> discovered that pruning specialized syntactic heads damaged model performance more than pruning other heads, providing instance-level causal evidence for the role of SAS. What&amp;rsquo;s missing from these studies, from an epistemological standpoint?&lt;/p>
&lt;h3 id="when-does-instance-level-analysis-fail">When does instance-level analysis fail?&lt;/h3>
&lt;p>There is an assumption underlying claims in interpretability: that the observed phenomenon measured by the interpretable &lt;strong>indicator&lt;/strong> metric, such as implicit parse UAS, plays a role in determining some &lt;strong>target&lt;/strong> metric, such as MLM validation loss or grammatical capabilities. However, instance-level evidence like the prior work on SAS might not support this assumption.&lt;/p>
&lt;p>First, instance-level &lt;em>observational&lt;/em> evidence might highlight artifacts that arose as a side effect of training, rather than as a crucial element in model decisions. That is, the poorly-understood dynamics of training might lead to structures emerging that are not required at test time. In evolutionary biology, such artifacts are called &lt;a href="https://en.wikipedia.org/wiki/Spandrel_%28biology%29" target="_blank" rel="noopener">spandrels&lt;/a>; proposed examples include human chins and musicality.&lt;/p>
&lt;p>Second, instance-level &lt;em>causal&lt;/em> evidence may be stronger, but remains flawed as evidence for the effect of SAS. For example, what if SAS emerges early in training, and gradually the model develops a more complex representation that does not rely on SAS, but the specialized heads have already become entangled with the rest of the high dimensional representation space? Then these specialized heads are &lt;a href="https://en.wikipedia.org/wiki/Vestigiality" target="_blank" rel="noopener">vestigial&lt;/a>, like a human tailbone, but are integrated into distributed subnetworks that generate and cancel noise, so the network may be particularly brittle to their removal. Another weakness of instance-level causal interpretation is that it may claim that a given behavior is unimportant, when in fact the model relied on that trait to converge on its final solution, as webbing evolves for gliding before an organism can develop wings.&lt;/p>
&lt;h2 id="epistemology-and-evidence">Epistemology and evidence&lt;/h2>
&lt;p>As we see in the example of SAS, treating a model &lt;strong>atemporally&lt;/strong>, in a manner detached from its development, might yield intriguing phenomena and even suggest insights. However, in order to strengthen the evidence for these insights, we argue for applying &lt;strong>developmental&lt;/strong> analysis. Developmental history may shed more light on the effect of a given indicator on a target metric, compared to atemporal data.&lt;/p>
&lt;p>Beyond the question of whether we have access to development data, there are three widely used categories of evidence in the scientific literature. Any one of the following categories might apply in either atemporal or developmental settings.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Observational&lt;/strong>: For example, &lt;em>BERT exhibits SAS and also demonstrates grammatical capabilities&lt;/em>. This is evidence based on passive observation of a given phenomenon. Atemporal observations are not strong evidence of a relationship between that phenomenon and the target of analysis, like a generalization metric. However, observations of development may support a relationship between the indicator and target metrics.&lt;/li>
&lt;li>&lt;strong>Correlational&lt;/strong>: For example, &lt;em>SAS is correlated with grammatical capabilities across a population of multiple models&lt;/em>. This is evidence based on natural variation of a given phenomenon.&lt;/li>
&lt;li>&lt;strong>Causal&lt;/strong>: For example, &lt;em>grammatical capabilities are affected by intervening on SAS&lt;/em>. This is evidence based on experimental interventions on the indicator phenomenon.&lt;/li>
&lt;/ul>
&lt;h3 id="cracking-the-case-of-the-cuckoo-culprit">Cracking the case of the cuckoo culprit&lt;/h3>
&lt;p>Returning to the example of the prinia, the assumption that their egg markings served as camouflage was based on atemporal observational evidence. Brood parasites (the indicator) do, in fact, drive the evolution of egg markings (the target of analysis). There are several epistemically strong pieces of evidence that ornithologists have found for that influence, including:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Atemporal correlational&lt;/strong>: A brood parasite&amp;rsquo;s favored hosts, like the prinia, have more individualized markings than other nesting birds.&lt;/li>
&lt;li>&lt;strong>Developmental observational&lt;/strong>: On continents with a longer evolutionary history of brood parasitism, such as Africa and Australia, nesting birds have more elaborate egg patterns compared to Europe, which has a shorter history of parasitism, or North America, which has no obligate brood parasites at all. If we treat each continent as though we are observing a checkpoint at some point in the evolutionary arms race between parasite and host, we can see that egg markings appear to be dependent on brood parasitism during evolutionary development.&lt;/li>
&lt;li>&lt;strong>Developmental causal&lt;/strong>: Invasive species of nesting birds provide a natural experiment for testing the link between brood parasitism and egg markings. When a host species spreads from its native range to an island without brood parasites, Australian ornithologists have observed that the species gradually loses its intricate egg markings over subsequent generations.&lt;/li>
&lt;/ul>
&lt;h2 id="whats-the-evidence-for-syntactic-attention-structure">What&amp;rsquo;s the evidence for Syntactic Attention Structure?&lt;/h2>
&lt;p>Does SAS play a significant role in the capabilities of a masked language model? In our latest paper, &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>, we test the impact of SAS by mirroring the variety of evidence collected by ornithologists. Let&amp;rsquo;s walk through the relevant results.&lt;/p>
&lt;h3 id="atemporal-correlational">Atemporal correlational&lt;/h3>
&lt;p>Correlational evidence is stronger when the variation in a population is known to be random, other than the metrics in focus, because this scenario allows us to control for the effect of confounding factors. This is the reason why, for example, genetics studies that use adopted children are stronger than those that study children raised by their biological parents, thereby confounding nature and nurture. Fortunately, deep learning allows us to generate random variation across models easily, by varying the training seed.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/corr_plots.png" alt="Correlation scatterplots." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Unfortunately, the correlational results above seem to be bleak for the idea that SAS is crucial to model capabilities. Using 25 independently trained MLMs from &lt;a href="https://arxiv.org/abs/2106.16163" target="_blank" rel="noopener">MultiBERTs&lt;/a>, we see no significant correlation, or even a clear pattern, between UAS (our SAS metric) and either validation loss or linguistic capabilities (in the form of &lt;a href="https://aclanthology.org/2020.tacl-1.25/" target="_blank" rel="noopener">BLiMP score&lt;/a>).&lt;/p>
&lt;p>Should we abandon SAS as a phenomenon unrelated to performance in practice? Not so fast. It may be that random variation does not elicit strong enough differences in SAS to register a behavioral difference. In order to dig deeper, we now turn to using a developmental lens by considering the training process.&lt;/p>
&lt;h3 id="developmental-observational">Developmental observational&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/development_plots.png" alt="BERT training." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Next, we passively observe SAS in an MLM model (a retrained BERT_base run with 3 different random seeds), but consider it during the entire course of training rather than at a single checkpoint. Results here appear to be far more promising, thanks to the clarity of an abrupt breakthrough in UAS about 20K steps into training. First, we see that there is a precipitous drop in loss coinciding with this UAS spike (marked ▲). Then, after UAS reaches its peak and begins to plateau, we observe an immediate jump in the MLM&amp;rsquo;s particular linguistic capabilities, as measured by BLiMP (marked ⏺). It certainly appears that the former is dependent on the latter. While we can directly observe that internal SAS structure precedes the acquisition of linguistic capabilities, though, can we guarantee that SAS &lt;em>precipitates&lt;/em> linguistic capabilities?&lt;/p>
&lt;h3 id="developmental-causal">Developmental causal&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/causal_plots.png" alt="Bert training with causal interventions." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To study how connected these consecutive phase transitions are, we intervene on the training process to suppress and promote SAS, yielding models respectively called BERT_SAS- and BERT_SAS+. Although neither promoting nor suppressing helps long term MLM performance, observing the training process shows clear evidence of a dependency. Promoting SAS leads to an earlier UAS spike, which indeed precipitates an earlier spike in linguistic capabilities. Meanwhile, suppressing SAS prevents any UAS spike and leads to persistently poor linguistic capabilities throughout training.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>By monitoring MLM training, we have found epistemically strong evidence of a connection between SAS and model performance, in particular through linguistic capabilities. These results are described in a new paper led by &lt;a href="https://angie-chen55.github.io/" target="_blank" rel="noopener">Angelica Chen&lt;/a> working with &lt;a href="https://www.ravid-shwartz-ziv.com/" target="_blank" rel="noopener">Ravid Schwartz-Ziv&lt;/a>, &lt;a href="https://kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho&lt;/a>, &lt;a href="https://mleavitt.net/" target="_blank" rel="noopener">Matthew Leavitt&lt;/a>, and me: &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>.&lt;/p>
&lt;p>Beyond the results obtained through our approach to interpretability epistemology, this paper also presents a large number of related findings. The full paper is worth reading, I think, as the results described here might not even be the most interesting part! We link these interpretable training dynamics to the broader literature on simplicity bias, model complexity, and phase transitions. We even show how our methods can be used to improve MLM performance by suppressing SAS briefly at the beginning of training. By questioning a seemingly settled result in interpretability, we developed a deeper understanding of MLM training.&lt;/p>
&lt;p>&lt;em>All preceding discussion of the research on brood parasites comes from one of my favorite books, &lt;a href="https://www.goodreads.com/en/book/show/22529402" target="_blank" rel="noopener">Cuckoo: Cheating by Nature&lt;/a> by Nick Davies. I highly recommend it to anyone interested in a deep dive on the evolution of a single survival strategy.&lt;/em>&lt;/p></description></item><item><title>Interpretability Creationism</title><link>https://nsaphra.github.io/post/creationism/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/post/creationism/</guid><description>&lt;p>For centuries, Europeans agreed that the presence of a cuckoo egg was a great honor to a nesting bird, as it granted an opportunity to exhibit Christian hospitality. The devout bird enthusiastically fed her holy guest, even more so than she would her own (evicted) chicks &lt;a href="https://app.thestorygraph.com/books/37ed3b62-8a3a-448b-9e37-cd5e5f51c640" target="_blank" rel="noopener">(Davies, 2015)&lt;/a>. In 1859, Charles Darwin’s studies of another occasional brood parasite, finches, called into question any rosy, cooperative view of bird behavior &lt;a href="https://app.thestorygraph.com/books/44185106-8198-42ef-bacf-8a9bf691e654" target="_blank" rel="noopener">(Darwin, 1859)&lt;/a>. Without considering the evolution of the cuckoo’s role, it would have been difficult to recognize the nesting bird not as a gracious host to the cuckoo chick, but as an unfortunate dupe. The historical process is essential to understanding its biological consequences; as evolutionary biologist Theodosius Dobzhansky put it, &lt;a href="https://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution#cite_note-Dobz_Nothing-1" target="_blank" rel="noopener">Nothing in Biology Makes Sense Except in the Light of Evolution&lt;/a>.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://upload.wikimedia.org/wikipedia/commons/5/5c/Reed_warbler_cuckoo.jpg" alt="By Per Harald Olsen - Own work, CC BY-SA 3.0" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Certainly SGD is not literally biological evolution, but post-hoc analysis in machine learning &lt;a href="https://twitter.com/ch402/status/1533164918886703104" target="_blank" rel="noopener">has a lot in common&lt;/a> with scientific approaches in biology, and likewise often requires an understanding of the origin of model behavior. Therefore, the following holds whether looking at parasitic brooding behavior or at the inner representations of a neural network: if we do not consider how a system develops, it is difficult to distinguish a pleasing story from a useful analysis.&lt;/p>
&lt;h2 id="just-so-stories">Just-So Stories&lt;/h2>
&lt;p>We have many pleasing &lt;a href="https://en.wikipedia.org/wiki/Just_So_Stories" target="_blank" rel="noopener">just-so stories&lt;/a> in NLP. Much has been made of interpretable artifacts such as &lt;a href="https://aclanthology.org/2022.acl-long.269.pdf" target="_blank" rel="noopener">syntactic attention distributions&lt;/a> or &lt;a href="https://openai.com/blog/unsupervised-sentiment-neuron/" target="_blank" rel="noopener">selective neurons&lt;/a>. But how can we know if such a pattern of behavior is actually used by the model?
Causal modeling can help, but interventions to test the influence of particular features and patterns may target only particular types of behavior explicitly. In practice, it may be possible only to perform certain types of slight interventions on specific units within a representation, failing to reflect interactions between features properly. Furthermore, in staging these interventions, we create distribution shifts that a model may not be robust to, regardless of whether that behavior is part of a core strategy. Significant distribution shifts can cause erratic behavior, so why shouldn&amp;rsquo;t they cause spurious interpretable artifacts? In practice, we find &lt;a href="https://arxiv.org/pdf/2010.12016.pdf" target="_blank" rel="noopener">no shortage&lt;/a> of incidental observations construed as crucial.&lt;/p>
&lt;p>Fortunately, the study of evolution has provided a number of ways to interpret the artifacts produced by a model. They might be vestigial, like a human tailbone. They may have dependencies, with some features and structures relying on the presence of other properties earlier in training, like the requirement for light sensing before a complex eye can develop. Some artifacts might represent side effects of training, like how junk DNA constitutes a majority of our genetic code without influencing our phenotypes.&lt;/p>
&lt;p>We have a number of theories for how such unused artifacts might emerge while training models. For example, the &lt;a href="https://arxiv.org/abs/1703.00810" target="_blank" rel="noopener">Information Bottleneck Hypothesis&lt;/a> predicts how inputs may be memorized early in training, before representations are compressed to only retain information about the output. These early memorized interpolations may not ultimately be useful when generalizing to unseen data, but they are essential in order to eventually learn to specifically represent the output. We also can infer the possibility of vestigial features, because early training behavior is so distinct from late training: &lt;a href="http://arxiv.org/abs/1905.11604" target="_blank" rel="noopener">earlier models are more simplistic&lt;/a>. In the case of language models, they &lt;a href="http://arxiv.org/abs/2109.06096" target="_blank" rel="noopener">behave similarly to ngram models&lt;/a> early on and &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.16" target="_blank" rel="noopener">exhibit linguistic patterns&lt;/a> later. Side effects of such a heteroskedastic training process could easily be mistaken for crucial components of a trained model.&lt;/p>
&lt;h2 id="the-evolutionary-view">The Evolutionary View&lt;/h2>
&lt;p>I may be unimpressed by &amp;ldquo;interpretability creationist&amp;rdquo; explanations of static fully trained models, but I have engaged in similar analysis myself. I&amp;rsquo;ve published papers on &lt;a href="https://arxiv.org/pdf/2010.02180.pdf" target="_blank" rel="noopener">probing static representations&lt;/a>, and the results often seem intuitive and explanatory. However, the presence of a feature at the end of training is hardly informative about the inductive bias of a model on its own! Consider &lt;a href="https://openreview.net/forum?id=mNtmhaDkAr" target="_blank" rel="noopener">Lovering et al.&lt;/a>, who found that the ease of extracting a feature at the start of training, along with an analysis of the finetuning data, has deeper implications for finetuned performance than we get by simply probing at the end of training.&lt;/p>
&lt;p>Let us consider an explanation usually based on analyzing static models: hierarchical behavior in language models. An example of this approach is the claim that &lt;a href="https://nlp.stanford.edu/pubs/hewitt2019structural.pdf" target="_blank" rel="noopener">words that are closely linked on a syntax tree have representations that are closer together&lt;/a>, compared to words that are syntactically farther. How can we know that the model is behaving hierarchically by grouping words according to syntactic proximity? Alternatively, syntactic neighbors may be more strongly linked due to a strong correlation between nearby words because they have higher joint frequency distributions. For example, perhaps constituents like &amp;ldquo;football match&amp;rdquo; are more predictable due to the frequency of their co-occurrence, compared to more distant relations like that between &amp;ldquo;uncle&amp;rdquo; and &amp;ldquo;football&amp;rdquo; in the sentence, &amp;ldquo;My uncle drove me to a football match&amp;rdquo;. In fact, we can be more confident that some language models are hierarchical, because early models encode more local information in &lt;a href="https://arxiv.org/abs/1811.00225" target="_blank" rel="noopener">LSTMs&lt;/a> and &lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#argument-phase-change" target="_blank" rel="noopener">Transformers&lt;/a>, and they learn longer distance dependencies more easily when those dependencies can be &lt;a href="https://arxiv.org/abs/2010.04650" target="_blank" rel="noopener">stacked onto short familiar constituents&lt;/a> hierarchically.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>I recently had to manage the trap of interpretability creationism myself. My coauthors had found that, when training text classifiers repeatedly with different random seeds, &lt;a href="https://arxiv.org/abs/2205.12411" target="_blank" rel="noopener">models can occur in a number of distinct clusters&lt;/a>. Further, we could predict the generalization behavior of a model based on which other models it was connected to on the loss surface. Now, we suspected that different finetuning runs found models with different generalization behavior because their trajectories entered different basins on the loss surface.&lt;/p>
&lt;p>But could we actually make this claim? What if one cluster actually corresponded to earlier stages of a model? Eventually those models would leave for the cluster with better generalization, so our only real result would be that some finetuning runs were slower than others. We had to demonstrate that training trajectories could actually become trapped in a basin, providing an explanation for the diversity of generalization behavior in trained models. Indeed, when we looked at several checkpoints, we confirmed that models that were very central to either cluster would become &lt;em>even more&lt;/em> strongly connected to the rest of their cluster over the course of training. Instead of offering a just-so story based on a static model, we explored the evolution of observed behavior to confirm our hypothesis.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.github.io/images/qqp_training.png" alt="k" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="a-proposal">A Proposal&lt;/h2>
&lt;p>To be clear, not every question can be answered by &lt;em>only&lt;/em> observing the training process. Causal claims require interventions! In biology, for example, research about antibiotic resistance requires us to deliberately expose bacteria to antibiotics, rather than waiting and hoping to find a natural experiment. Even the claims currently being made based on observations of training dynamics may require experimental confirmation.&lt;/p>
&lt;p>Furthermore, not all claims require &lt;em>any&lt;/em> observation of the training process. Even to ancient humans, many organs had obvious purpose: eyes see, hearts pump blood, and &lt;a href="https://www.scientificamerican.com/article/aristotle-thought-the-brain-was-a-radiator/" target="_blank" rel="noopener">brains are refrigerators&lt;/a>. Likewise in NLP, just by analyzing static models we can make simple claims: that particular neurons activate in the presence of particular properties, or that some types of information remain accessible within a model. However, the training dimension can still clarify the meaning of many observations made in a static model.&lt;/p>
&lt;p>My proposal is simple. Are you developing a method of interpretation or analyzing some property of a trained model? Don&amp;rsquo;t just look at final checkpoint in training. Apply that analysis to several intermediate checkpoints. If you are finetuning a model, check several points both early and late in training. If you are analyzing a large language model, &lt;a href="https://arxiv.org/abs/2106.16163" target="_blank" rel="noopener">MultiBERTs&lt;/a> and &lt;a href="https://nlp.stanford.edu/mistral/getting_started/download.html" target="_blank" rel="noopener">Mistral&lt;/a> both provide intermediate checkpoints sampled from throughout training on masked and autoregressive language models, respectively. Does the behavior that you&amp;rsquo;ve analyzed change over the course of training? Does your belief about the model&amp;rsquo;s strategy actually make sense after observing what happens early in training? There&amp;rsquo;s very little overhead to an experiment like this, and you never know what you&amp;rsquo;ll find!&lt;/p></description></item><item><title>Against Monodomainism</title><link>https://nsaphra.github.io/post/monodomainism/</link><pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate><guid>https://nsaphra.github.io/post/monodomainism/</guid><description>&lt;p>Reaching the endpoint of a PhD studying how language models learn, I have spent several years telling people that I study &amp;ldquo;machine learning and natural language processing&amp;rdquo;. However, my colleagues who tried to understand or augment image classifiers would describe themselves only as working in &amp;ldquo;machine learning&amp;rdquo;. I argue that this pattern reflects thinking about what it means to be &amp;ldquo;application&amp;rdquo; work or &amp;ldquo;core&amp;rdquo; machine learning that damages our understanding of statistical modeling and deep learning as a whole.&lt;/p>
&lt;p>Why do we know so little about how language models learn? This gap is in part because consideration of NLP as a domain is historically rare in venues that publish most training dynamics research, or analytic work in learning theory. A current search&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> of ICML 2020 publications returned 169 papers with citations to “Association for Computational Linguistics” or “ACL”, even including citations to many potential sister conferences: NAACL, AACL, or EACL. A search for citations to a single vision conference, “Computer Vision and Pattern Recognition” or “CVPR”, turned up 541 papers. In COLT publications since 2017, the same searches turned up 13 and 23 papers, respectively. In ICML 2020, Wikitext-* or PTB references found only 16 results, while the most popular small corpus for image classification, MNIST, found 264 ICML publications&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Linguistics provides us with the salient concept of &lt;em>markedness&lt;/em> &lt;a href="https://www.degruyter.com/document/doi/10.1515/9783110862010.11/html" target="_blank" rel="noopener">(Andersen, 1989)&lt;/a>. In language, some forms of a word are the default form, while others are explicitly marked by some additional inflection. An example would be contrast between the word “marked”, which is an &lt;em>unmarked&lt;/em> form compared to “unmarked”, which is &lt;em>marked&lt;/em> by the prefix “un-”. In machine learning, we might call CV an unmarked domain by convention, in contrast to the &lt;em>marked&lt;/em> NLP. This convention means that certain tasks and architectures are considered the default environments to understand. Such a convention privileges understanding continuous data over discrete; ConvNets over LSTMs; ResNets over Transformers; geometric tasks over structured prediction.&lt;/p>
&lt;p>Understanding one machine learning domain will always extend analysis of others. For example, latent tree structure is inherent to both domains, but in CV, it is obscured by the image data from which we must compose eyes and mouth into a face—and subsequently, body and face into a cow &lt;a href="https://ieeexplore.ieee.org/document/6909858" target="_blank" rel="noopener">(Vedaldi et al., 2014)&lt;/a>. Image classification is also a language task, because it is our language that provides the intuitions which we use to construct ontologies that turn into image classes; English does not provide us with common distinctions for different packs of wolves, but it names every dog breed, and so the image labels are chosen according to available terminology.&lt;/p>
&lt;p>Many researchers think of text data as arcane, but the unmarked domain of CV displays many idiosyncrasies on which to overfit our understanding of statistical modeling. CV provides us with many interesting geometric phenomena, but the underlying structure of language &lt;em>without&lt;/em> the added noisy channel of an image can provide a clear and simple domain worth analyzing, as well. A true understanding of statistical models must be a multi-domain understanding, not a mono-domain view focused on one task and its peculiarities.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Searches were performed with Google Scholar.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>*CL venues have also become distanced from work in computational linguistics &lt;a href="https://www.aclweb.org/anthology/J07-2013.pdf" target="_blank" rel="noopener">(Reiter, 2007)&lt;/a>, leaving NLP as a field deprived of new scientific work in its data domain as well as new scientific work in its methodologies.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>