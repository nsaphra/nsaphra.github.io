<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Interpretability | Naomi Saphra</title><link>https://nsaphra.net/tags/interpretability/</link><atom:link href="https://nsaphra.net/tags/interpretability/index.xml" rel="self" type="application/rss+xml"/><description>Interpretability</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 May 2025 00:00:00 +0000</lastBuildDate><image><url>https://nsaphra.net/media/icon_hu_ecc3d54b494abbac.png</url><title>Interpretability</title><link>https://nsaphra.net/tags/interpretability/</link></image><item><title>How to visualize training dynamics in neural networks</title><link>https://nsaphra.net/publication/hublog/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/hublog/</guid><description/></item><item><title>Sometimes I am a Tree: Data drives fragile hierarchical generalization</title><link>https://nsaphra.net/publication/sunny/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sunny/</guid><description/></item><item><title>Causation Does Not Imply Correlation: A Study of Circuit Mechanisms and Model Behaviors</title><link>https://nsaphra.net/publication/jenny/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/jenny/</guid><description/></item><item><title>Loss in the Crowd: Hidden Breakthroughs in Language Model Training</title><link>https://nsaphra.net/publication/sara-loss/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sara-loss/</guid><description/></item><item><title>Mechanistic?</title><link>https://nsaphra.net/publication/sarah-mech/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sarah-mech/</guid><description/></item><item><title>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</title><link>https://nsaphra.net/publication/chen/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/chen/</guid><description/></item><item><title>The Parable of the Prinia's Egg: An Allegory for AI Science</title><link>https://nsaphra.net/post/prinia/</link><pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/prinia/</guid><description>&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/eggs.jpg" alt="Prinia eggs" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>When European scientists first encountered the eggs of the tawny-flanked prinia &lt;em>Prinia subflava&lt;/em>, an African nesting bird, they believed they understood what they had found. The prinia lay eggs that exhibited swirls, speckles, and coloration unique to each individual bird. What purpose do such patterns serve in nature? Surely, scientists agreed, these markings allowed the eggs to camouflage and blend into the nest.&lt;/p>
&lt;p>One 19th century naturalist, Charles Francis Massey Swynnerton, offered an alternative explanation. Why would each bird have its own unique patterns? Swynnerton believed that the markings functioned as watermarks, allowing the nesting bird to differentiate its own eggs from those of the cuckoo finch &lt;em>Anomalospiza imberbis&lt;/em>. The cuckoo finch is an &lt;strong>obligate brood parasite&lt;/strong>, meaning that it does not build its own nest, but instead lays its eggs in the nests of other birds, particularly the prinia.&lt;/p>
&lt;p>Since its proposal, evidence for Swynnerton&amp;rsquo;s hypothesis has accumulated. The camouflage hypothesis was based on anecdotal observations of eggs, without correlational or experimental data. In contrast, evidence for the brood parasite hypothesis comes in diverse forms, using many different frameworks for reasoning about causality and explanation, adding up to an indisputable scientific arsenal. What can AI researchers learn from the mature science of biology to strengthen our own evidence and reliably interpret how neural network traits contribute to model behavior?&lt;/p>
&lt;h2 id="instance-level-interpretability">Instance level interpretability&lt;/h2>
&lt;p>Most interpretability work, and much work in science of deep learning, relies on passive observations of phenomena with post-hoc explanations for how artifacts in the model contribute to model outputs. Like the early ornithologists suggesting that prinia eggs were camouflaged, interpretability researchers often identify some phenomenon and link it to model decisions based solely on their intuitions. Some work attempts a more principled approach, by intervening on the trained model to demonstrate that a given feature or circuit has a predictable effect on model judgment. Both approaches might be classed as &lt;strong>instance-level&lt;/strong>, as they consider the behavior of a fully trained model on a given input sample.&lt;/p>
&lt;p>In the past, I have called this reliance on fully trained models &lt;a href="https://thegradient.pub/interpretability-creationism/" target="_blank" rel="noopener">Interpretability Creationism&lt;/a>, and suggested that interpretability researchers instead measure the indicators they focus on throughout training. However, I have not detailed how these creationist approaches can lead us astray, nor have I proposed a clear framework for understanding development. Such a framework falls under the philosophical field of &lt;strong>epistemology&lt;/strong>, the study of what makes a belief justified.&lt;/p>
&lt;h3 id="syntactic-attention-structure">Syntactic Attention Structure&lt;/h3>
&lt;p>To address the epistemology of interpretability, let us construct a case study of the scientific literature on an observed phenomenon in neural networks. One well-documented and intuitive behavior is how Transformer-based &lt;strong>masked language models&lt;/strong> (MLMs) have specialized attention heads that focus on a specific dependency relation, a trait we will call &lt;strong>Syntactic Attention Structure&lt;/strong> (SAS). We can illustrate SAS with an example sentence.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/parse_example.png" alt="My bird builds ugly nests" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>In the preceding sentence, the model might be called upon to predict the masked-out target word &lt;em>builds&lt;/em>. During inference, its specialized &lt;code>nsubj&lt;/code> head will place its highest weight on &lt;em>bird&lt;/em>, while its specialized &lt;code>dobj&lt;/code> head will place its highest weight on &lt;em>nests&lt;/em>. This specialization behavior emerges naturally, without any explicit inductive bias, over the normal course of training for models like BERT.&lt;/p>
&lt;p>SAS was discovered concurrently in two papers. First, &lt;a href="https://aclanthology.org/W19-4828/" target="_blank" rel="noopener">Clark et al. (2019)&lt;/a> observed that specialized attention heads provide an implicit parse, which aligns with prior notions of dependency syntax, as measured by &lt;strong>Unlabeled Attachment Score&lt;/strong> (UAS). This observation provides instance-level observational evidence for the role of SAS in masked language modeling. Second, &lt;a href="https://aclanthology.org/P19-1580/" target="_blank" rel="noopener">Voita et al. (2019)&lt;/a> discovered that pruning specialized syntactic heads damaged model performance more than pruning other heads, providing instance-level causal evidence for the role of SAS. What&amp;rsquo;s missing from these studies, from an epistemological standpoint?&lt;/p>
&lt;h3 id="when-does-instance-level-analysis-fail">When does instance-level analysis fail?&lt;/h3>
&lt;p>There is an assumption underlying claims in interpretability: that the observed phenomenon measured by the interpretable &lt;strong>indicator&lt;/strong> metric, such as implicit parse UAS, plays a role in determining some &lt;strong>target&lt;/strong> metric, such as MLM validation loss or grammatical capabilities. However, instance-level evidence like the prior work on SAS might not support this assumption.&lt;/p>
&lt;p>First, instance-level &lt;em>observational&lt;/em> evidence might highlight artifacts that arose as a side effect of training, rather than as a crucial element in model decisions. That is, the poorly-understood dynamics of training might lead to structures emerging that are not required at test time. In evolutionary biology, such artifacts are called &lt;a href="https://en.wikipedia.org/wiki/Spandrel_%28biology%29" target="_blank" rel="noopener">spandrels&lt;/a>; proposed examples include human chins and musicality.&lt;/p>
&lt;p>Second, instance-level &lt;em>causal&lt;/em> evidence may be stronger, but remains flawed as evidence for the effect of SAS. For example, what if SAS emerges early in training, and gradually the model develops a more complex representation that does not rely on SAS, but the specialized heads have already become entangled with the rest of the high dimensional representation space? Then these specialized heads are &lt;a href="https://en.wikipedia.org/wiki/Vestigiality" target="_blank" rel="noopener">vestigial&lt;/a>, like a human tailbone, but are integrated into distributed subnetworks that generate and cancel noise, so the network may be particularly brittle to their removal. Another weakness of instance-level causal interpretation is that it may claim that a given behavior is unimportant, when in fact the model relied on that trait to converge on its final solution, as webbing evolves for gliding before an organism can develop wings.&lt;/p>
&lt;h2 id="epistemology-and-evidence">Epistemology and evidence&lt;/h2>
&lt;p>As we see in the example of SAS, treating a model &lt;strong>atemporally&lt;/strong>, in a manner detached from its development, might yield intriguing phenomena and even suggest insights. However, in order to strengthen the evidence for these insights, we argue for applying &lt;strong>developmental&lt;/strong> analysis. Developmental history may shed more light on the effect of a given indicator on a target metric, compared to atemporal data.&lt;/p>
&lt;p>Beyond the question of whether we have access to development data, there are three widely used categories of evidence in the scientific literature. Any one of the following categories might apply in either atemporal or developmental settings.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Observational&lt;/strong>: For example, &lt;em>BERT exhibits SAS and also demonstrates grammatical capabilities&lt;/em>. This is evidence based on passive observation of a given phenomenon. Atemporal observations are not strong evidence of a relationship between that phenomenon and the target of analysis, like a generalization metric. However, observations of development may support a relationship between the indicator and target metrics.&lt;/li>
&lt;li>&lt;strong>Correlational&lt;/strong>: For example, &lt;em>SAS is correlated with grammatical capabilities across a population of multiple models&lt;/em>. This is evidence based on natural variation of a given phenomenon.&lt;/li>
&lt;li>&lt;strong>Causal&lt;/strong>: For example, &lt;em>grammatical capabilities are affected by intervening on SAS&lt;/em>. This is evidence based on experimental interventions on the indicator phenomenon.&lt;/li>
&lt;/ul>
&lt;h3 id="cracking-the-case-of-the-cuckoo-culprit">Cracking the case of the cuckoo culprit&lt;/h3>
&lt;p>Returning to the example of the prinia, the assumption that their egg markings served as camouflage was based on atemporal observational evidence. Brood parasites (the indicator) do, in fact, drive the evolution of egg markings (the target of analysis). There are several epistemically strong pieces of evidence that ornithologists have found for that influence, including:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Atemporal correlational&lt;/strong>: A brood parasite&amp;rsquo;s favored hosts, like the prinia, have more individualized markings than other nesting birds.&lt;/li>
&lt;li>&lt;strong>Developmental observational&lt;/strong>: On continents with a longer evolutionary history of brood parasitism, such as Africa and Australia, nesting birds have more elaborate egg patterns compared to Europe, which has a shorter history of parasitism, or North America, which has no obligate brood parasites at all. If we treat each continent as though we are observing a checkpoint at some point in the evolutionary arms race between parasite and host, we can see that egg markings appear to be dependent on brood parasitism during evolutionary development.&lt;/li>
&lt;li>&lt;strong>Developmental causal&lt;/strong>: Invasive species of nesting birds provide a natural experiment for testing the link between brood parasitism and egg markings. When a host species spreads from its native range to an island without brood parasites, Australian ornithologists have observed that the species gradually loses its intricate egg markings over subsequent generations.&lt;/li>
&lt;/ul>
&lt;h2 id="whats-the-evidence-for-syntactic-attention-structure">What&amp;rsquo;s the evidence for Syntactic Attention Structure?&lt;/h2>
&lt;p>Does SAS play a significant role in the capabilities of a masked language model? In our latest paper, &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>, we test the impact of SAS by mirroring the variety of evidence collected by ornithologists. Let&amp;rsquo;s walk through the relevant results.&lt;/p>
&lt;h3 id="atemporal-correlational">Atemporal correlational&lt;/h3>
&lt;p>Correlational evidence is stronger when the variation in a population is known to be random, other than the metrics in focus, because this scenario allows us to control for the effect of confounding factors. This is the reason why, for example, genetics studies that use adopted children are stronger than those that study children raised by their biological parents, thereby confounding nature and nurture. Fortunately, deep learning allows us to generate random variation across models easily, by varying the training seed.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/corr_plots.png" alt="Correlation scatterplots." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Unfortunately, the correlational results above seem to be bleak for the idea that SAS is crucial to model capabilities. Using 25 independently trained MLMs from &lt;a href="https://arxiv.org/abs/2106.16163" target="_blank" rel="noopener">MultiBERTs&lt;/a>, we see no significant correlation, or even a clear pattern, between UAS (our SAS metric) and either validation loss or linguistic capabilities (in the form of &lt;a href="https://aclanthology.org/2020.tacl-1.25/" target="_blank" rel="noopener">BLiMP score&lt;/a>).&lt;/p>
&lt;p>Should we abandon SAS as a phenomenon unrelated to performance in practice? Not so fast. It may be that random variation does not elicit strong enough differences in SAS to register a behavioral difference. In order to dig deeper, we now turn to using a developmental lens by considering the training process.&lt;/p>
&lt;h3 id="developmental-observational">Developmental observational&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/development_plots.png" alt="BERT training." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Next, we passively observe SAS in an MLM model (a retrained BERT_base run with 3 different random seeds), but consider it during the entire course of training rather than at a single checkpoint. Results here appear to be far more promising, thanks to the clarity of an abrupt breakthrough in UAS about 20K steps into training. First, we see that there is a precipitous drop in loss coinciding with this UAS spike (marked ▲). Then, after UAS reaches its peak and begins to plateau, we observe an immediate jump in the MLM&amp;rsquo;s particular linguistic capabilities, as measured by BLiMP (marked ⏺). It certainly appears that the former is dependent on the latter. While we can directly observe that internal SAS structure precedes the acquisition of linguistic capabilities, though, can we guarantee that SAS &lt;em>precipitates&lt;/em> linguistic capabilities?&lt;/p>
&lt;h3 id="developmental-causal">Developmental causal&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/causal_plots.png" alt="Bert training with causal interventions." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To study how connected these consecutive phase transitions are, we intervene on the training process to suppress and promote SAS, yielding models respectively called BERT_SAS- and BERT_SAS+. Although neither promoting nor suppressing helps long term MLM performance, observing the training process shows clear evidence of a dependency. Promoting SAS leads to an earlier UAS spike, which indeed precipitates an earlier spike in linguistic capabilities. Meanwhile, suppressing SAS prevents any UAS spike and leads to persistently poor linguistic capabilities throughout training.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>By monitoring MLM training, we have found epistemically strong evidence of a connection between SAS and model performance, in particular through linguistic capabilities. These results are described in a new paper led by &lt;a href="https://angie-chen55.github.io/" target="_blank" rel="noopener">Angelica Chen&lt;/a> working with &lt;a href="https://www.ravid-shwartz-ziv.com/" target="_blank" rel="noopener">Ravid Schwartz-Ziv&lt;/a>, &lt;a href="https://kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho&lt;/a>, &lt;a href="https://mleavitt.net/" target="_blank" rel="noopener">Matthew Leavitt&lt;/a>, and me: &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>.&lt;/p>
&lt;p>Beyond the results obtained through our approach to interpretability epistemology, this paper also presents a large number of related findings. The full paper is worth reading, I think, as the results described here might not even be the most interesting part! We link these interpretable training dynamics to the broader literature on simplicity bias, model complexity, and phase transitions. We even show how our methods can be used to improve MLM performance by suppressing SAS briefly at the beginning of training. By questioning a seemingly settled result in interpretability, we developed a deeper understanding of MLM training.&lt;/p>
&lt;p>&lt;em>All preceding discussion of the research on brood parasites comes from one of my favorite books, &lt;a href="https://www.goodreads.com/en/book/show/22529402" target="_blank" rel="noopener">Cuckoo: Cheating by Nature&lt;/a> by Nick Davies. I highly recommend it to anyone interested in a deep dive on the evolution of a single survival strategy.&lt;/em>&lt;/p></description></item><item><title>Delays, Detours, and Forks in the Road: Latent State Models of Training Dynamics</title><link>https://nsaphra.net/publication/hu/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/hu/</guid><description/></item><item><title>Interpretability Creationism</title><link>https://nsaphra.net/publication/saphra-2023-interp/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-2023-interp/</guid><description/></item><item><title>Linear Connectivity Reveals Generalization Strategies</title><link>https://nsaphra.net/publication/juneja-linear-2023/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/juneja-linear-2023/</guid><description/></item><item><title>Shapley Interactions for Complex Feature Attribution</title><link>https://nsaphra.net/publication/attrib/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/attrib/</guid><description/></item><item><title>Towards out-of-distribution generalization in large-scale astronomical surveys: robust networks learn similar representations</title><link>https://nsaphra.net/publication/sultan/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sultan/</guid><description/></item><item><title>A Non-Linear Structural Probe</title><link>https://nsaphra.net/publication/white-nonlinear-2020/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/white-nonlinear-2020/</guid><description/></item><item><title>LSTMs Compose---and Learn---Bottom-Up</title><link>https://nsaphra.net/publication/saphra-lstms-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-lstms-2020/</guid><description/></item><item><title>Understanding Learning Dynamics Of Language Models with SVCCA</title><link>https://nsaphra.net/publication/saphra-understand-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-understand-2019/</guid><description/></item></channel></rss>