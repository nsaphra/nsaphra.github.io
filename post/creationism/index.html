<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: August 15, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Naomi Saphra"><meta name=description content="Nothing in Deep Learning Makes Sense Except in the Light of SGD."><link rel=alternate hreflang=en-us href=https://nsaphra.github.io/post/creationism/><link rel=canonical href=https://nsaphra.github.io/post/creationism/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nsaphra"><meta property="twitter:creator" content="@nsaphra"><meta property="twitter:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Naomi Saphra"><meta property="og:url" content="https://nsaphra.github.io/post/creationism/"><meta property="og:title" content="Interpretability Creationism | Naomi Saphra"><meta property="og:description" content="Nothing in Deep Learning Makes Sense Except in the Light of SGD."><meta property="og:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-06-07T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-07T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsaphra.github.io/post/creationism/"},"headline":"Interpretability Creationism","datePublished":"2022-06-07T00:00:00Z","dateModified":"2022-06-07T00:00:00Z","author":{"@type":"Person","name":"Naomi Saphra"},"publisher":{"@type":"Organization","name":"Naomi Saphra","logo":{"@type":"ImageObject","url":"https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_192x192_fill_lanczos_center_3.png"}},"description":"Nothing in Deep Learning Makes Sense Except in the Light of SGD."}</script><title>Interpretability Creationism | Naomi Saphra</title><link rel=me href=https://mastodon.online/@nsaphra></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1f4b7b7bc5f129bd99684def783b096a><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Naomi Saphra</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Naomi Saphra</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/uploads/academic_cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://sigmoid.social/@nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Mastodon" target=_blank rel=noopener aria-label="Follow me on Mastodon"><i class="fab fa-mastodon" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Interpretability Creationism</h1><div class=article-metadata><div><span>Naomi Saphra</span></div><span class=article-date>Jun 7, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>7 min read</span></div></div><div class=article-container><div class=article-style><p>For centuries, Europeans agreed that the presence of a cuckoo egg was a great honor to a nesting bird, as it granted an opportunity to exhibit Christian hospitality. The devout bird enthusiastically fed her holy guest, even more so than she would her own (evicted) chicks <a href=https://app.thestorygraph.com/books/37ed3b62-8a3a-448b-9e37-cd5e5f51c640 target=_blank rel=noopener>(Davies, 2015)</a>. In 1859, Charles Darwin’s studies of another occasional brood parasite, finches, called into question any rosy, cooperative view of bird behavior <a href=https://app.thestorygraph.com/books/44185106-8198-42ef-bacf-8a9bf691e654 target=_blank rel=noopener>(Darwin, 1859)</a>. Without considering the evolution of the cuckoo’s role, it would have been difficult to recognize the nesting bird not as a gracious host to the cuckoo chick, but as an unfortunate dupe. The historical process is essential to understanding its biological consequences; as evolutionary biologist Theodosius Dobzhansky put it, <a href=https://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution#cite_note-Dobz_Nothing-1 target=_blank rel=noopener>Nothing in Biology Makes Sense Except in the Light of Evolution</a>.</p><img src=https://upload.wikimedia.org/wikipedia/commons/5/5c/Reed_warbler_cuckoo.jpg alt="By Per Harald Olsen - Own work, CC BY-SA 3.0" width=200><p>Certainly SGD is not literally biological evolution, but post-hoc analysis in machine learning <a href=https://twitter.com/ch402/status/1533164918886703104 target=_blank rel=noopener>has a lot in common</a> with scientific approaches in biology, and likewise often requires an understanding of the origin of model behavior. Therefore, the following holds whether looking at parasitic brooding behavior or at the inner representations of a neural network: if we do not consider how a system develops, it is difficult to distinguish a pleasing story from a useful analysis.</p><h2 id=just-so-stories>Just-So Stories</h2><p>We have many pleasing <a href=https://en.wikipedia.org/wiki/Just_So_Stories target=_blank rel=noopener>just-so stories</a> in NLP. Much has been made of interpretable artifacts such as <a href=https://aclanthology.org/2022.acl-long.269.pdf target=_blank rel=noopener>syntactic attention distributions</a> or <a href=https://openai.com/blog/unsupervised-sentiment-neuron/ target=_blank rel=noopener>selective neurons</a>. But how can we know if such a pattern of behavior is actually used by the model?
Causal modeling can help, but interventions to test the influence of particular features and patterns may target only particular types of behavior explicitly. In practice, it may be possible only to perform certain types of slight interventions on specific units within a representation, failing to reflect interactions between features properly. Furthermore, in staging these interventions, we create distribution shifts that a model may not be robust to, regardless of whether that behavior is part of a core strategy. Significant distribution shifts can cause erratic behavior, so why shouldn&rsquo;t they cause spurious interpretable artifacts? In practice, we find <a href=https://arxiv.org/pdf/2010.12016.pdf target=_blank rel=noopener>no shortage</a> of incidental observations construed as crucial.</p><p>Fortunately, the study of evolution has provided a number of ways to interpret the artifacts produced by a model. They might be vestigial, like a human tailbone. They may have dependencies, with some features and structures relying on the presence of other properties earlier in training, like the requirement for light sensing before a complex eye can develop. Some artifacts might represent side effects of training, like how junk DNA constitutes a majority of our genetic code without influencing our phenotypes.</p><p>We have a number of theories for how such unused artifacts might emerge while training models. For example, the <a href=https://arxiv.org/abs/1703.00810 target=_blank rel=noopener>Information Bottleneck Hypothesis</a> predicts how inputs may be memorized early in training, before representations are compressed to only retain information about the output. These early memorized interpolations may not ultimately be useful when generalizing to unseen data, but they are essential in order to eventually learn to specifically represent the output. We also can infer the possibility of vestigial features, because early training behavior is so distinct from late training: <a href=http://arxiv.org/abs/1905.11604 target=_blank rel=noopener>earlier models are more simplistic</a>. In the case of language models, they <a href=http://arxiv.org/abs/2109.06096 target=_blank rel=noopener>behave similarly to ngram models</a> early on and <a href=https://www.aclweb.org/anthology/2020.emnlp-main.16 target=_blank rel=noopener>exhibit linguistic patterns</a> later. Side effects of such a heteroskedastic training process could easily be mistaken for crucial components of a trained model.</p><h2 id=the-evolutionary-view>The Evolutionary View</h2><p>I may be unimpressed by &ldquo;interpretability creationist&rdquo; explanations of static fully trained models, but I have engaged in similar analysis myself. I&rsquo;ve published papers on <a href=https://arxiv.org/pdf/2010.02180.pdf target=_blank rel=noopener>probing static representations</a>, and the results often seem intuitive and explanatory. However, the presence of a feature at the end of training is hardly informative about the inductive bias of a model on its own! Consider <a href="https://openreview.net/forum?id=mNtmhaDkAr" target=_blank rel=noopener>Lovering et al.</a>, who found that the ease of extracting a feature at the start of training, along with an analysis of the finetuning data, has deeper implications for finetuned performance than we get by simply probing at the end of training.</p><p>Let us consider an explanation usually based on analyzing static models: hierarchical behavior in language models. An example of this approach is the claim that <a href=https://nlp.stanford.edu/pubs/hewitt2019structural.pdf target=_blank rel=noopener>words that are closely linked on a syntax tree have representations that are closer together</a>, compared to words that are syntactically farther. How can we know that the model is behaving hierarchically by grouping words according to syntactic proximity? Alternatively, syntactic neighbors may be more strongly linked due to a strong correlation between nearby words because they have higher joint frequency distributions. For example, perhaps constituents like &ldquo;football match&rdquo; are more predictable due to the frequency of their co-occurrence, compared to more distant relations like that between &ldquo;uncle&rdquo; and &ldquo;football&rdquo; in the sentence, &ldquo;My uncle drove me to a football match&rdquo;. In fact, we can be more confident that some language models are hierarchical, because early models encode more local information in <a href=https://arxiv.org/abs/1811.00225 target=_blank rel=noopener>LSTMs</a> and <a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#argument-phase-change target=_blank rel=noopener>Transformers</a>, and they learn longer distance dependencies more easily when those dependencies can be <a href=https://arxiv.org/abs/2010.04650 target=_blank rel=noopener>stacked onto short familiar constituents</a> hierarchically.</p><h2 id=an-example>An Example</h2><p>I recently had to manage the trap of interpretability creationism myself. My coauthors had found that, when training text classifiers repeatedly with different random seeds, <a href=https://arxiv.org/abs/2205.12411 target=_blank rel=noopener>models can occur in a number of distinct clusters</a>. Further, we could predict the generalization behavior of a model based on which other models it was connected to on the loss surface. Now, we suspected that different finetuning runs found models with different generalization behavior because their trajectories entered different basins on the loss surface.</p><p>But could we actually make this claim? What if one cluster actually corresponded to earlier stages of a model? Eventually those models would leave for the cluster with better generalization, so our only real result would be that some finetuning runs were slower than others. We had to demonstrate that training trajectories could actually become trapped in a basin, providing an explanation for the diversity of generalization behavior in trained models. Indeed, when we looked at several checkpoints, we confirmed that models that were very central to either cluster would become <em>even more</em> strongly connected to the rest of their cluster over the course of training. Instead of offering a just-so story based on a static model, we explored the evolution of observed behavior to confirm our hypothesis.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/qqp_training.png alt=k loading=lazy data-zoomable></div></div></figure></p><img src=images/qqp_training.png width=600><h2 id=a-proposal>A Proposal</h2><p>To be clear, not every question can be answered by <em>only</em> observing the training process. Causal claims require interventions! In biology, for example, research about antibiotic resistance requires us to deliberately expose bacteria to antibiotics, rather than waiting and hoping to find a natural experiment. Even the claims currently being made based on observations of training dynamics may require experimental confirmation.</p><p>Furthermore, not all claims require <em>any</em> observation of the training process. Even to ancient humans, many organs had obvious purpose: eyes see, hearts pump blood, and <a href=https://www.scientificamerican.com/article/aristotle-thought-the-brain-was-a-radiator/ target=_blank rel=noopener>brains are refrigerators</a>. Likewise in NLP, just by analyzing static models we can make simple claims: that particular neurons activate in the presence of particular properties, or that some types of information remain accessible within a model. However, the training dimension can still clarify the meaning of many observations made in a static model.</p><p>My proposal is simple. Are you developing a method of interpretation or analyzing some property of a trained model? Don&rsquo;t just look at final checkpoint in training. Apply that analysis to several intermediate checkpoints. If you are finetuning a model, check several points both early and late in training. If you are analyzing a large language model, <a href=https://arxiv.org/abs/2106.16163 target=_blank rel=noopener>MultiBERTs</a> and <a href=https://nlp.stanford.edu/mistral/getting_started/download.html target=_blank rel=noopener>Mistral</a> both provide intermediate checkpoints sampled from throughout training on masked and autoregressive language models, respectively. Does the behavior that you&rsquo;ve analyzed change over the course of training? Does your belief about the model&rsquo;s strategy actually make sense after observing what happens early in training? There&rsquo;s very little overhead to an experiment like this, and you never know what you&rsquo;ll find!</p></div><div class=article-tags><a class="badge badge-light" href=/tag/training-dynamics/>training dynamics</a>
<a class="badge badge-light" href=/tag/rant/>rant</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F&amp;text=Interpretability+Creationism" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F&amp;t=Interpretability+Creationism" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Interpretability%20Creationism&amp;body=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F&amp;title=Interpretability+Creationism" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Interpretability+Creationism%20https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fcreationism%2F&amp;title=Interpretability+Creationism" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>