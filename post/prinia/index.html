<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: October 23, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Naomi Saphra"><meta name=description content="I discuss what counts as strong evidence for an explanation of model behavior."><link rel=alternate hreflang=en-us href=https://nsaphra.github.io/post/prinia/><link rel=canonical href=https://nsaphra.github.io/post/prinia/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nsaphra"><meta property="twitter:creator" content="@nsaphra"><meta property="twitter:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Naomi Saphra"><meta property="og:url" content="https://nsaphra.github.io/post/prinia/"><meta property="og:title" content="The Parable of the Prinia’s Egg: An Allegory for AI Science | Naomi Saphra"><meta property="og:description" content="I discuss what counts as strong evidence for an explanation of model behavior."><meta property="og:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-09-17T11:00:00+01:00"><meta property="article:modified_time" content="2023-09-17T11:00:00+01:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsaphra.github.io/post/prinia/"},"headline":"The Parable of the Prinia’s Egg: An Allegory for AI Science","datePublished":"2023-09-17T11:00:00+01:00","dateModified":"2023-09-17T11:00:00+01:00","author":{"@type":"Person","name":"Naomi Saphra"},"publisher":{"@type":"Organization","name":"Naomi Saphra","logo":{"@type":"ImageObject","url":"https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_192x192_fill_lanczos_center_3.png"}},"description":"I discuss what counts as strong evidence for an explanation of model behavior."}</script><title>The Parable of the Prinia’s Egg: An Allegory for AI Science | Naomi Saphra</title><link rel=me href=https://mastodon.online/@nsaphra></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=7c21fb1311bacc9e9610bb7ddb198485><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Naomi Saphra</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Naomi Saphra</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/uploads/academic_cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://sigmoid.social/@nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Mastodon" target=_blank rel=noopener aria-label="Follow me on Mastodon"><i class="fab fa-mastodon" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>The Parable of the Prinia’s Egg: An Allegory for AI Science</h1><div class=article-metadata><span class=article-date>Sep 17, 2023</span>
<span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div><div class=article-container><div class=article-style><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/eggs.jpg alt="Prinia eggs" loading=lazy data-zoomable></div></div></figure></p><p>When European scientists first encountered the eggs of the tawny-flanked prinia <em>Prinia subflava</em>, an African nesting bird, they believed they understood what they had found. The prinia lay eggs that exhibited swirls, speckles, and coloration unique to each individual bird. What purpose do such patterns serve in nature? Surely, scientists agreed, these markings allowed the eggs to camouflage and blend into the nest.</p><p>One 19th century naturalist, Charles Francis Massey Swynnerton, offered an alternative explanation. Why would each bird have its own unique patterns? Swynnerton believed that the markings functioned as watermarks, allowing the nesting bird to differentiate its own eggs from those of the cuckoo finch <em>Anomalospiza imberbis</em>. The cuckoo finch is an <strong>obligate brood parasite</strong>, meaning that it does not build its own nest, but instead lays its eggs in the nests of other birds, particularly the prinia.</p><p>Since its proposal, evidence for Swynnerton&rsquo;s hypothesis has accumulated. The camouflage hypothesis was based on anecdotal observations of eggs, without correlational or experimental data. In contrast, evidence for the brood parasite hypothesis comes in diverse forms, using many different frameworks for reasoning about causality and explanation, adding up to an indisputable scientific arsenal. What can AI researchers learn from the mature science of biology to strengthen our own evidence and reliably interpret how neural network traits contribute to model behavior?</p><h2 id=instance-level-interpretability>Instance level interpretability</h2><p>Most interpretability work, and much work in science of deep learning, relies on passive observations of phenomena with post-hoc explanations for how artifacts in the model contribute to model outputs. Like the early ornithologists suggesting that prinia eggs were camouflaged, interpretability researchers often identify some phenomenon and link it to model decisions based solely on their intuitions. Some work attempts a more principled approach, by intervening on the trained model to demonstrate that a given feature or circuit has a predictable effect on model judgment. Both approaches might be classed as <strong>instance-level</strong>, as they consider the behavior of a fully trained model on a given input sample.</p><p>In the past, I have called this reliance on fully trained models <a href=https://thegradient.pub/interpretability-creationism/ target=_blank rel=noopener>Interpretability Creationism</a>, and suggested that interpretability researchers instead measure the indicators they focus on throughout training. However, I have not detailed how these creationist approaches can lead us astray, nor have I proposed a clear framework for understanding development. Such a framework falls under the philosophical field of <strong>epistemology</strong>, the study of what makes a belief justified.</p><h3 id=syntactic-attention-structure>Syntactic Attention Structure</h3><p>To address the epistemology of interpretability, let us construct a case study of the scientific literature on an observed phenomenon in neural networks. One well-documented and intuitive behavior is how Transformer-based <strong>masked language models</strong> (MLMs) have specialized attention heads that focus on a specific dependency relation, a trait we will call <strong>Syntactic Attention Structure</strong> (SAS). We can illustrate SAS with an example sentence.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/parse_example.png alt="My bird builds ugly nests" loading=lazy data-zoomable></div></div></figure></p><p>In the preceding sentence, the model might be called upon to predict the masked-out target word <em>builds</em>. During inference, its specialized <code>nsubj</code> head will place its highest weight on <em>bird</em>, while its specialized <code>dobj</code> head will place its highest weight on <em>nests</em>. This specialization behavior emerges naturally, without any explicit inductive bias, over the normal course of training for models like BERT.</p><p>SAS was discovered concurrently in two papers. First, <a href=https://aclanthology.org/W19-4828/ target=_blank rel=noopener>Clark et al. (2019)</a> observed that specialized attention heads provide an implicit parse, which aligns with prior notions of dependency syntax, as measured by <strong>Unlabeled Attachment Score</strong> (UAS). This observation provides instance-level observational evidence for the role of SAS in masked language modeling. Second, <a href=https://aclanthology.org/P19-1580/ target=_blank rel=noopener>Voita et al. (2019)</a> discovered that pruning specialized syntactic heads damaged model performance more than pruning other heads, providing instance-level causal evidence for the role of SAS. What&rsquo;s missing from these studies, from an epistemological standpoint?</p><h3 id=when-does-instance-level-analysis-fail>When does instance-level analysis fail?</h3><p>There is an assumption underlying claims in interpretability: that the observed phenomenon measured by the interpretable <strong>indicator</strong> metric, such as implicit parse UAS, plays a role in determining some <strong>target</strong> metric, such as MLM validation loss or grammatical capabilities. However, instance-level evidence like the prior work on SAS might not support this assumption.</p><p>First, instance-level <em>observational</em> evidence might highlight artifacts that arose as a side effect of training, rather than as a crucial element in model decisions. That is, the poorly-understood dynamics of training might lead to structures emerging that are not required at test time. In evolutionary biology, such artifacts are called <a href=https://en.wikipedia.org/wiki/Spandrel_%28biology%29 target=_blank rel=noopener>spandrels</a>; proposed examples include human chins and musicality.</p><p>Second, instance-level <em>causal</em> evidence may be stronger, but remains flawed as evidence for the effect of SAS. For example, what if SAS emerges early in training, and gradually the model develops a more complex representation that does not rely on SAS, but the specialized heads have already become entangled with the rest of the high dimensional representation space? Then these specialized heads are <a href=https://en.wikipedia.org/wiki/Vestigiality target=_blank rel=noopener>vestigial</a>, like a human tailbone, but are integrated into distributed subnetworks that generate and cancel noise, so the network may be particularly brittle to their removal. Another weakness of instance-level causal interpretation is that it may claim that a given behavior is unimportant, when in fact the model relied on that trait to converge on its final solution, as webbing evolves for gliding before an organism can develop wings.</p><h2 id=epistemology-and-evidence>Epistemology and evidence</h2><p>As we see in the example of SAS, treating a model <strong>atemporally</strong>, in a manner detached from its development, might yield intriguing phenomena and even suggest insights. However, in order to strengthen the evidence for these insights, we argue for applying <strong>developmental</strong> analysis. Developmental history may shed more light on the effect of a given indicator on a target metric, compared to atemporal data.</p><p>Beyond the question of whether we have access to development data, there are three widely used categories of evidence in the scientific literature. Any one of the following categories might apply in either atemporal or developmental settings.</p><ul><li><strong>Observational</strong>: For example, <em>BERT exhibits SAS and also demonstrates grammatical capabilities</em>. This is evidence based on passive observation of a given phenomenon. Atemporal observations are not strong evidence of a relationship between that phenomenon and the target of analysis, like a generalization metric. However, observations of development may support a relationship between the indicator and target metrics.</li><li><strong>Correlational</strong>: For example, <em>SAS is correlated with grammatical capabilities across a population of multiple models</em>. This is evidence based on natural variation of a given phenomenon.</li><li><strong>Causal</strong>: For example, <em>grammatical capabilities are affected by intervening on SAS</em>. This is evidence based on experimental interventions on the indicator phenomenon.</li></ul><h3 id=cracking-the-case-of-the-cuckoo-culprit>Cracking the case of the cuckoo culprit</h3><p>Returning to the example of the prinia, the assumption that their egg markings served as camouflage was based on atemporal observational evidence. Brood parasites (the indicator) do, in fact, drive the evolution of egg markings (the target of analysis). There are several epistemically strong pieces of evidence that ornithologists have found for that influence, including:</p><ul><li><strong>Atemporal correlational</strong>: A brood parasite&rsquo;s favored hosts, like the prinia, have more individualized markings than other nesting birds.</li><li><strong>Developmental observational</strong>: On continents with a longer evolutionary history of brood parasitism, such as Africa and Australia, nesting birds have more elaborate egg patterns compared to Europe, which has a shorter history of parasitism, or North America, which has no obligate brood parasites at all. If we treat each continent as though we are observing a checkpoint at some point in the evolutionary arms race between parasite and host, we can see that egg markings appear to be dependent on brood parasitism during evolutionary development.</li><li><strong>Developmental causal</strong>: Invasive species of nesting birds provide a natural experiment for testing the link between brood parasitism and egg markings. When a host species spreads from its native range to an island without brood parasites, Australian ornithologists have observed that the species gradually loses its intricate egg markings over subsequent generations.</li></ul><h2 id=whats-the-evidence-for-syntactic-attention-structure>What&rsquo;s the evidence for Syntactic Attention Structure?</h2><p>Does SAS play a significant role in the capabilities of a masked language model? In our latest paper, <a href=https://arxiv.org/abs/2309.07311 target=_blank rel=noopener>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</a>, we test the impact of SAS by mirroring the variety of evidence collected by ornithologists. Let&rsquo;s walk through the relevant results.</p><h3 id=atemporal-correlational>Atemporal correlational</h3><p>Correlational evidence is stronger when the variation in a population is known to be random, other than the metrics in focus, because this scenario allows us to control for the effect of confounding factors. This is the reason why, for example, genetics studies that use adopted children are stronger than those that study children raised by their biological parents, thereby confounding nature and nurture. Fortunately, deep learning allows us to generate random variation across models easily, by varying the training seed.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/corr_plots.png alt="Correlation scatterplots." loading=lazy data-zoomable></div></div></figure></p><p>Unfortunately, the correlational results above seem to be bleak for the idea that SAS is crucial to model capabilities. Using 25 independently trained MLMs from <a href=https://arxiv.org/abs/2106.16163 target=_blank rel=noopener>MultiBERTs</a>, we see no significant correlation, or even a clear pattern, between UAS (our SAS metric) and either validation loss or linguistic capabilities (in the form of <a href=https://aclanthology.org/2020.tacl-1.25/ target=_blank rel=noopener>BLiMP score</a>).</p><p>Should we abandon SAS as a phenomenon unrelated to performance in practice? Not so fast. It may be that random variation does not elicit strong enough differences in SAS to register a behavioral difference. In order to dig deeper, we now turn to using a developmental lens by considering the training process.</p><h3 id=developmental-observational>Developmental observational</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/development_plots.png alt="BERT training." loading=lazy data-zoomable></div></div></figure></p><p>Next, we passively observe SAS in an MLM model (a retrained BERT_base run with 3 different random seeds), but consider it during the entire course of training rather than at a single checkpoint. Results here appear to be far more promising, thanks to the clarity of an abrupt breakthrough in UAS about 20K steps into training. First, we see that there is a precipitous drop in loss coinciding with this UAS spike (marked ▲). Then, after UAS reaches its peak and begins to plateau, we observe an immediate jump in the MLM&rsquo;s particular linguistic capabilities, as measured by BLiMP (marked ⏺). It certainly appears that the former is dependent on the latter. While we can directly observe that internal SAS structure precedes the acquisition of linguistic capabilities, though, can we guarantee that SAS <em>precipitates</em> linguistic capabilities?</p><h3 id=developmental-causal>Developmental causal</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/causal_plots.png alt="Bert training with causal interventions." loading=lazy data-zoomable></div></div></figure></p><p>To study how connected these consecutive phase transitions are, we intervene on the training process to suppress and promote SAS, yielding models respectively called BERT_SAS- and BERT_SAS+. Although neither promoting nor suppressing helps long term MLM performance, observing the training process shows clear evidence of a dependency. Promoting SAS leads to an earlier UAS spike, which indeed precipitates an earlier spike in linguistic capabilities. Meanwhile, suppressing SAS prevents any UAS spike and leads to persistently poor linguistic capabilities throughout training.</p><h2 id=conclusion>Conclusion</h2><p>By monitoring MLM training, we have found epistemically strong evidence of a connection between SAS and model performance, in particular through linguistic capabilities. These results are described in a new paper led by <a href=https://angie-chen55.github.io/ target=_blank rel=noopener>Angelica Chen</a> working with <a href=https://www.ravid-shwartz-ziv.com/ target=_blank rel=noopener>Ravid Schwartz-Ziv</a>, <a href=https://kyunghyuncho.me/ target=_blank rel=noopener>Kyunghyun Cho</a>, <a href=https://mleavitt.net/ target=_blank rel=noopener>Matthew Leavitt</a>, and me: <a href=https://arxiv.org/abs/2309.07311 target=_blank rel=noopener>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</a>.</p><p>Beyond the results obtained through our approach to interpretability epistemology, this paper also presents a large number of related findings. The full paper is worth reading, I think, as the results described here might not even be the most interesting part! We link these interpretable training dynamics to the broader literature on simplicity bias, model complexity, and phase transitions. We even show how our methods can be used to improve MLM performance by suppressing SAS briefly at the beginning of training. By questioning a seemingly settled result in interpretability, we developed a deeper understanding of MLM training.</p><p><em>All preceding discussion of the research on brood parasites comes from one of my favorite books, <a href=https://www.goodreads.com/en/book/show/22529402 target=_blank rel=noopener>Cuckoo: Cheating by Nature</a> by Nick Davies. I highly recommend it to anyone interested in a deep dive on the evolution of a single survival strategy.</em></p></div><div class=article-tags><a class="badge badge-light" href=/tag/training-dynamics/>training dynamics</a>
<a class="badge badge-light" href=/tag/interpretability/>interpretability</a>
<a class="badge badge-light" href=/tag/manifesto/>manifesto</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F&amp;text=The+Parable+of+the+Prinia%E2%80%99s+Egg%3A+An+Allegory+for+AI+Science" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F&amp;t=The+Parable+of+the+Prinia%E2%80%99s+Egg%3A+An+Allegory+for+AI+Science" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=The%20Parable%20of%20the%20Prinia%E2%80%99s%20Egg%3A%20An%20Allegory%20for%20AI%20Science&amp;body=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F&amp;title=The+Parable+of+the+Prinia%E2%80%99s+Egg%3A+An+Allegory+for+AI+Science" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=The+Parable+of+the+Prinia%E2%80%99s+Egg%3A+An+Allegory+for+AI+Science%20https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Fprinia%2F&amp;title=The+Parable+of+the+Prinia%E2%80%99s+Egg%3A+An+Allegory+for+AI+Science" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://nsaphra.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu90b276abf902758fc55813a02decabb9_157010_270x270_fill_q75_lanczos_center.jpg alt="Naomi Saphra"></a><div class=media-body><h5 class=card-title><a href=https://nsaphra.github.io>Naomi Saphra</a></h5><h6 class=card-subtitle>Gradient Descent Spectator</h6><p class=card-text>Naomi Saphra is a researcher in NLP and machine learning.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:nsaphra@nsaphra.net><i class="fas fa-envelope"></i></a></li><li><a href=https://sigmoid.social/@nsaphra target=_blank rel=noopener><i class="fab fa-mastodon"></i></a></li><li><a href=https://twitter.com/nsaphra target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.co.uk/citations?user=TPhVfX8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.semanticscholar.org/author/Naomi-Saphra/2362960 target=_blank rel=noopener><i class="ai ai-semantic-scholar"></i></a></li><li><a href=https://github.com/nsaphra target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>