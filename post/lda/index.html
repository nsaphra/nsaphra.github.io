<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: August 15, 2023 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.0f229d4b7ebad1917a9a357cba2effab.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Naomi Saphra"><meta name=description content="An explanation of Latent Dirichlet Allocation (LDA), a common method of topic modeling."><link rel=alternate hreflang=en-us href=https://nsaphra.github.io/post/lda/><link rel=canonical href=https://nsaphra.github.io/post/lda/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nsaphra"><meta property="twitter:creator" content="@nsaphra"><meta property="twitter:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Naomi Saphra"><meta property="og:url" content="https://nsaphra.github.io/post/lda/"><meta property="og:title" content="Understanding Latent Dirichlet Allocation | Naomi Saphra"><meta property="og:description" content="An explanation of Latent Dirichlet Allocation (LDA), a common method of topic modeling."><meta property="og:image" content="https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2012-07-09T00:00:00+00:00"><meta property="article:modified_time" content="2012-07-09T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nsaphra.github.io/post/lda/"},"headline":"Understanding Latent Dirichlet Allocation","datePublished":"2012-07-09T00:00:00Z","dateModified":"2012-07-09T00:00:00Z","author":{"@type":"Person","name":"Naomi Saphra"},"publisher":{"@type":"Organization","name":"Naomi Saphra","logo":{"@type":"ImageObject","url":"https://nsaphra.github.io/media/icon_huf0b254c5fb6316cc66f507f70b041c15_37368_192x192_fill_lanczos_center_3.png"}},"description":"An explanation of Latent Dirichlet Allocation (LDA), a common method of topic modeling."}</script><title>Understanding Latent Dirichlet Allocation | Naomi Saphra</title><link rel=me href=https://mastodon.online/@nsaphra></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=d2d78946109f7933bf8e431faf1e7526><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Naomi Saphra</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Naomi Saphra</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/uploads/academic_cv.pdf><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://sigmoid.social/@nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Mastodon" target=_blank rel=noopener aria-label="Follow me on Mastodon"><i class="fab fa-mastodon" aria-hidden=true></i></a></li><li class="nav-item d-none d-lg-inline-flex"><a class=nav-link href=https://twitter.com/nsaphra data-toggle=tooltip data-placement=bottom title="Follow me on Twitter" target=_blank rel=noopener aria-label="Follow me on Twitter"><i class="fab fa-twitter" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Understanding Latent Dirichlet Allocation</h1><div class=article-metadata><span class=article-date>Jul 9, 2012</span>
<span class=middot-divider></span>
<span class=article-reading-time>5 min read</span></div></div><div class=article-container><div class=article-style><p><em>[Note - This is a repost of a post I made on my old blog while I was in undergrad. I&rsquo;m including it in case someone finds it useful, since my old blog is defunct. I haven&rsquo;t significantly edited it, so I&rsquo;m sorry if it doesn&rsquo;t fit into my current style.]</em></p><p>This post is directed to a lay CS audience. I am an undergraduate in CS, so I consider myself part of that audience. If you&rsquo;re awesome at machine learning already and don&rsquo;t want to help me along here, just read the paper.</p><p>Latent Dirichlet Allocation (LDA) is a common method of topic modeling. That is, if I have a document and want to figure out if it&rsquo;s a sports article or a mathematics paper, I can use LDA to build a system that looks at other sports articles or mathematics papers and automatically decides whether this unseen document&rsquo;s topic is sports or math.</p><p>To LDA, a document is just a collection of topics where each topic has some particular probability of generating a particular word. For our potential sports article, the word &ldquo;average&rdquo; appears 4 times. What&rsquo;s the probability of a sports topic generating that many instances of &ldquo;average&rdquo;? We determine this by looking at each training document as a &ldquo;bag of words&rdquo; pulled from a distribution selected by a Dirichlet process.</p><p>Dirichlet is a distribution specified by a vector parameter $$\alpha$$ containing some \(\alpha_i\) corresponding to each topic \(i\), which we write as \(\textrm{Dir}(\alpha)\). The formula for computing the probability density function for each topic vector \(x\) is proportional to the product over all topics \(i\) of \(x_i \alpha_i\). \(x_i\) is the probability that the topic is \(i\), so the items in \(x\) must sum to 1. That keeps you from getting arbitrarily large probabilities by giving arbitrarily large values of \(x\).</p><p>Confused? Ready for a picture ripped off Wikipedia?</p><p><a href=http://commons.wikimedia.org/wiki/File:Dirichlet_distributions.png target=_blank rel=noopener><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dirichlet_distributions.png/695px-Dirichlet_distributions.png alt="Dirichlet Distributions" loading=lazy data-zoomable></div></div></figure></a></p><p>Those graphs all show Dirichlet distributions for three topics. That triangle at the bottom has one side for each topic, and the closer a point on the triangle is to side \(i\) the higher the probability of topic \(i\). The purple curve is the probability density function over the mixture of topics. See how the edges of the triangle all have probability 0? We said that the pdf is proportional to \(x_i \alpha_i\), so if \(x_i\) is 0, the probability of that mixture of topics is 0. That restricts our model a bit and ensures that we never are totally certain about the topic of a document.</p><p>Okay, we&rsquo;ve got &ldquo;Dirichlet&rdquo;, so let&rsquo;s pop back up to the concept of LDA. If we want to find that mixture of topics for a document, we first need to determine the value of each \(x_i\). That means we&rsquo;ve got another Dirichlet distribution in our model for each topic i where instead of the sides of the triangle being topics, they&rsquo;re words. Picture the topic &ldquo;sports article&rdquo; like those distributions above, but instead of sitting on triangles they&rsquo;re on shapes with so many sides the shapes go into as many dimensions as we have topics.. If &ldquo;average&rdquo; appears in a sports article, the bump pushes closer to the side for &ldquo;average&rdquo;.</p><p>The Latent part of LDA comes into play because in statistics, a variable we have to infer rather than directly observing is called a &ldquo;latent variable&rdquo;. We&rsquo;re only directly observing the words and not the topics, so the topics themselves are latent variables (along with the distributions themselves).</p><p>LDA assumes that each document k is generated by:</p><ol><li>From our Dirichlet distribution for k, sample a random distribution of topics. That is, pick a place on that triangle that is associated with a certain probability of generating each topic. If we choose a place very close to the &ldquo;sports article&rdquo; edge, we have a higher probability of picking &ldquo;sports article&rdquo;. The probability of picking a particular place on the triangle is described by the pdf of the Dirichlet distribution (the placement of the purple mound).</li><li>For each topic, pick a distribution of words for that topic from the Dirichlet for that topic.</li><li>For each word in document \(k\),<ol><li>From the distribution of topics selected for \(k\), sample a topic, like &ldquo;sports article&rdquo;.</li><li>From the distribution selected for &ldquo;sports article&rdquo;, pick the current word.</li></ol></li></ol><p>So let&rsquo;s say your first four words all come from baseball and your document maybe starts off &ldquo;average the bat bat&rdquo;. If that&rsquo;s not how you tend to write, that&rsquo;s okay. All models are wrong.</p><p>The important thing to understand is that your Dirichlet priors are distribution of distributions, which are selected to generate each word.</p><p>We&rsquo;re generally not just making these distributions for the heck of it or to actually generate documents. We want to figure out what topic was probably used for each word by our lazy writer who randomly generates each word. Maybe it&rsquo;s been a while since you took probability, but do you remember this guy?
\[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]</p><p>This is Bayes&rsquo; Theorem. We already know the probability of generating a particular word given a topic according to our model. That&rsquo;s the probability of sampling that word from the topic&rsquo;s word distribution. So that&rsquo;s \( P(B|A) \) where \( B \) is the event of &ldquo;average&rdquo; being generated for the current word and A is the event of picking the topic &ldquo;sports article&rdquo;. \(P(A)\) is the probability of &ldquo;sports article&rdquo; being picked from the document&rsquo;s topic distribution. \( P(B) \) is the probability of &ldquo;average&rdquo; being generated at all, which is the sum over all topic selections \(A\) of \(P(B|A)P(A)\). Now we can use Bayes to find \(P(A|B)\), the probability that topic \(A\) generated word \(B\).</p><p>So now we know how to figure out the probability of each topic per word. Now we already know that our documents are assumed to be a mix of topics, but we want to find the most likely dominant topic. The lazy writer generates each word independently of each other word, so the overall probability of a topic throughout the document is the product of \(P(A|B)\) at each word \(B\). Just pick the most likely topic across the words.</p></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F&amp;text=Understanding+Latent+Dirichlet+Allocation" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F&amp;t=Understanding+Latent+Dirichlet+Allocation" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Understanding%20Latent%20Dirichlet%20Allocation&amp;body=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F&amp;title=Understanding+Latent+Dirichlet+Allocation" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Understanding+Latent+Dirichlet+Allocation%20https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fnsaphra.github.io%2Fpost%2Flda%2F&amp;title=Understanding+Latent+Dirichlet+Allocation" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://nsaphra.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu90b276abf902758fc55813a02decabb9_157010_270x270_fill_q75_lanczos_center.jpg alt="Naomi Saphra"></a><div class=media-body><h5 class=card-title><a href=https://nsaphra.github.io>Naomi Saphra</a></h5><h6 class=card-subtitle>Gradient Descent Spectator</h6><p class=card-text>Naomi Saphra is a researcher in NLP and machine learning.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:nsaphra@nsaphra.net><i class="fas fa-envelope"></i></a></li><li><a href=https://sigmoid.social/@nsaphra target=_blank rel=noopener><i class="fab fa-mastodon"></i></a></li><li><a href=https://twitter.com/nsaphra target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.co.uk/citations?user=TPhVfX8AAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.semanticscholar.org/author/Naomi-Saphra/2362960 target=_blank rel=noopener><i class="ai ai-semantic-scholar"></i></a></li><li><a href=https://github.com/nsaphra target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.32ee83730ed883becad04bc5170512cc.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/en/js/wowchemy.min.91534f6cb18c3621254d412c69186d7c.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>