<!doctype html><html lang=en-us dir=ltr data-wc-theme-default=system><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 0.3.1"><meta name=author content="Naomi Saphra"><meta name=description content="Notes on incrementally constraining the architecture of a neural network as a method of regularization."><link rel=alternate hreflang=en-us href=https://nsaphra.net/post/model-scheduling/><link rel=stylesheet href=/css/themes/emerald.min.css><link href=/dist/wc.min.css rel=stylesheet><script>window.hbb={defaultTheme:document.documentElement.dataset.wcThemeDefault,setDarkTheme:()=>{document.documentElement.classList.add("dark"),document.documentElement.style.colorScheme="dark"},setLightTheme:()=>{document.documentElement.classList.remove("dark"),document.documentElement.style.colorScheme="light"}},console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`),"wc-color-theme"in localStorage?localStorage.getItem("wc-color-theme")==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme():(window.hbb.defaultTheme==="dark"?window.hbb.setDarkTheme():window.hbb.setLightTheme(),window.hbb.defaultTheme==="system"&&(window.matchMedia("(prefers-color-scheme: dark)").matches?window.hbb.setDarkTheme():window.hbb.setLightTheme()))</script><script>document.addEventListener("DOMContentLoaded",function(){let e=document.querySelectorAll("li input[type='checkbox'][disabled]");e.forEach(e=>{e.parentElement.parentElement.classList.add("task-list")});const t=document.querySelectorAll(".task-list li");t.forEach(e=>{let t=Array.from(e.childNodes).filter(e=>e.nodeType===3&&e.textContent.trim().length>1);if(t.length>0){const n=document.createElement("label");t[0].after(n),n.appendChild(e.querySelector("input[type='checkbox']")),n.appendChild(t[0])}})})</script><link rel=icon type=image/png href=/media/icon_hu_4d36134051bb7026.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu_d9b2d55b74fabcc9.png><link rel=canonical href=https://nsaphra.net/post/model-scheduling/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nsaphra"><meta property="twitter:creator" content="@nsaphra"><meta property="og:site_name" content="Naomi Saphra"><meta property="og:url" content="https://nsaphra.net/post/model-scheduling/"><meta property="og:title" content="Model Scheduling | Naomi Saphra"><meta property="og:description" content="Notes on incrementally constraining the architecture of a neural network as a method of regularization."><meta property="og:image" content="https://nsaphra.net/media/icon_hu_ecc3d54b494abbac.png"><meta property="twitter:image" content="https://nsaphra.net/media/icon_hu_ecc3d54b494abbac.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2018-08-13T00:00:00+00:00"><meta property="article:modified_time" content="2018-08-13T00:00:00+00:00"><title>Model Scheduling | Naomi Saphra</title><style>@font-face{font-family:inter var;font-style:normal;font-weight:100 900;font-display:swap;src:url(/dist/font/Inter.var.woff2)format(woff2)}</style><link type=text/css rel=stylesheet href=/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css integrity="sha256-vnZutBkxehTsdp0hbpd5v+jzc3yA54D0ug2vtXpBpII="><script src=/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script><script>window.hbb.pagefind={baseUrl:"/"}</script><style>html.dark{--pagefind-ui-primary:#eeeeee;--pagefind-ui-text:#eeeeee;--pagefind-ui-background:#152028;--pagefind-ui-border:#152028;--pagefind-ui-tag:#152028}</style><script>window.addEventListener("DOMContentLoaded",e=>{new PagefindUI({element:"#search",showSubResults:!0,baseUrl:window.hbb.pagefind.baseUrl,bundlePath:window.hbb.pagefind.baseUrl+"pagefind/"})}),document.addEventListener("DOMContentLoaded",()=>{let e=document.getElementById("search"),t=document.getElementById("search_toggle");t&&t.addEventListener("click",()=>{if(e.classList.toggle("hidden"),e.querySelector("input").value="",e.querySelector("input").focus(),!e.classList.contains("hidden")){let t=document.querySelector(".pagefind-ui__search-clear");t&&!t.hasAttribute("listenerOnClick")&&(t.setAttribute("listenerOnClick","true"),t.addEventListener("click",()=>{e.classList.toggle("hidden")}))}})})</script><link type=text/css rel=stylesheet href=/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM="><script defer src=/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js integrity="sha256-3ISyluw+iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script><script defer src=/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script><script defer src=/js/hugo-blox-en.min.8c8ea06bd0420f5067e52fa727b9f92303757322ba4431774153d59a9735eadb.js integrity="sha256-jI6ga9BCD1Bn5S+nJ7n5IwN1cyK6RDF3QVPVmpc16ts="></script><script async defer src=https://buttons.github.io/buttons.js></script></head><body class="dark:bg-hb-dark dark:text-white page-wrapper" id=top><div id=page-bg></div><div class="page-header sticky top-0 z-30"><header id=site-header class=header><nav class="navbar px-3 flex justify-left"><div class="order-0 h-100"><a class=navbar-brand href=/ title="Naomi Saphra">Naomi Saphra</a></div><input id=nav-toggle type=checkbox class=hidden>
<label for=nav-toggle class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1"><svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20"><title>Open Menu</title><path d="M0 3h20v2H0V3zm0 6h20v2H0V9zm0 6h20v2H0V0z"/></svg><svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20"><title>Close Menu</title><polygon points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2" transform="rotate(45 10 10)"/></svg></label><ul id=nav-menu class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left"><li class=nav-item><a class=nav-link href=/>Bio</a></li><li class=nav-item><a class=nav-link href=/#posts>Posts</a></li><li class=nav-item><a class=nav-link href=/#papers>Publications</a></li></ul><div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0"><button aria-label=search class="text-black hover:text-primary inline-block px-3 text-xl dark:text-white" id=search_toggle><svg height="16" width="16" viewBox="0 0 512 512" fill="currentcolor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8.0 45.3s-32.8 12.5-45.3.0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9.0 208S93.1.0 208 0 416 93.1 416 208zM208 352a144 144 0 100-288 144 144 0 100 288z"/></svg></button><div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
[&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white"><button class="theme-toggle mt-1" accesskey=t title=appearance><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:hidden"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="dark:block [&:not(dark)]:hidden"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div></nav></header><div id=search class="hidden p-3"></div></div><div class="page-body my-10"><div class="mx-auto flex max-w-screen-xl"><aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:hidden xl:block"><div class="px-4 pt-4 lg:hidden"></div><div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]"><ul class="flex flex-col gap-1 lg:hidden"><li><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/>Publications
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/hublog/>How to visualize training dynamics in neural networks</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/polypythias/>PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/prashanth-2024-recitereconstructrecollectmemorization/>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/rosie/>Distributional Scaling Laws for Emergent Capabilities</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sunny/>Sometimes I am a Tree: Data drives fragile hierarchical generalization</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/systematicity/>Attribute Diversity Determines the Systematicity Gap in VQA</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/bench/>Benchmarks as Microscopes: A Call for Model Metrology</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/jenny/>Causation Does Not Imply Correlation: A Study of Circuit Mechanisms and Model Behaviors</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/li-2024-chatgptdoesnttrustchargers/>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/ankner/>Dynamic Masking Rate Schedules for MLM Pretraining</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/ff/>Fast Forwarding Low-Rank Training</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/parse/>First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sara-loss/>Loss in the Crowd: Hidden Breakthroughs in Language Model Training</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sarah-mech/>Mechanistic?</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/chen/>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sherborne-tram-2023/>TRAM: Bridging Trust Regions and Sharpness Aware Minimization</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/zhang-2024-transcendence/>Transcendence: Generative Models Can Outperform The Experts That Train Them</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/johnson-yu-2024-understanding/>Understanding biological active sensing behaviors by interpreting learned artificial agent policies</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/hu/>Delays, Detours, and Forks in the Road: Latent State Models of Training Dynamics</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/saphra-2023-interp/>Interpretability Creationism</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/juneja-linear-2023/>Linear Connectivity Reveals Generalization Strategies</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/attrib/>Shapley Interactions for Complex Feature Attribution</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/hupkes-state-art-2023/>State-of-the-art generalisation research in NLP: a taxonomy and review</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sultan/>Towards out-of-distribution generalization in large-scale astronomical surveys: robust networks learn similar representations</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/valvoda-learning-2022/>Learning Transductions to Test Systematic Compositionality</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/zhao-one-2022/>One Venue, Two Conferences: The Separation of Chinese and American Citation Networks</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/sellam-multiberts-2021/>The MultiBERTs: BERT Reproductions for Robustness Analysis</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/white-nonlinear-2020/>A Non-Linear Structural Probe</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/saphra-lstms-2020/>LSTMs Compose---and Learn---Bottom-Up</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/pimentel-pareto-2020/>Pareto Probing: Trading Off Accuracy for Complexity</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/tahaei/>Understanding Privacy-Related Questions on Stack Overflow</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/kate/>Carbon AI and the Concentration of Computational Work</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/saphra-sparsity/>Sparsity Emerges Naturally in Neural Language Models</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/saphra-understand-2019/>Understanding Learning Dynamics Of Language Models with SVCCA</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/dynet/>DyNet: The Dynamic Neural Network Toolkit</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/pos-first/>Evaluating Informal-Domain Word Representations with UrbanDictionary</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/naomi-saphra-amrica-2015/>AMRICA: an AMR Inspector for Cross-language Alignments</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/schneider-framework-2014/>A framework for (under) specifying dependency syntax without overloading annotators</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/cotterell-algerian-2014/>An Algerian Arabic-French Code-Switched Corpus</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/publication/vedaldi-understanding-2014/>Understanding Objects in Detail with Fine-grained Attributes</a></li></ul></div></li><li class=open><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/>Blog
<span data-hb-sidebar-toggle><svg fill="none" viewBox="0 0 24 24" stroke="currentcolor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"/></svg></span></a><div class="ltr:pr-0 overflow-hidden"><ul class=hb-sidebar-list><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/prinia/>The Parable of the Prinia's Egg: An Allegory for AI Science</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/creationism/>Interpretability Creationism</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/monodomainism/>Against Monodomainism</a></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/hands/>What Does a Coder Do If They Can't Type?</a></li><li class="flex flex-col open"><a class="hb-sidebar-custom-link
sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900" href=/post/model-scheduling/>Model Scheduling</a><ul class=hb-sidebar-mobile-toc><li><a href=#dynamic-hyperparameters class=hb-docs-link>Dynamic Hyperparameters</a></li><li><a href=#adaptive-architectures class=hb-docs-link>Adaptive Architectures</a></li><li><a href=#teacherstudent-approaches class=hb-docs-link>Teacher/Student Approaches</a></li></ul></li><li class="flex flex-col"><a class="hb-sidebar-custom-link
text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50" href=/post/lda/>Understanding Latent Dirichlet Allocation</a></li></ul></div></li></ul><div class="max-xl:hidden h-0 w-64 shrink-0"></div></div></aside><nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents"><div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul><li class="my-2 scroll-my-6 scroll-py-6"><a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#dynamic-hyperparameters>Dynamic Hyperparameters</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#curriculum-dropout>Curriculum Dropout</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#mollifying-networks>Mollifying Networks</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#adaptive-architectures>Adaptive Architectures</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#architecture-growth>Architecture Growth</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#architecture-pruning>Architecture Pruning</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#pruning-edges>Pruning Edges</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#pruning-nodes>Pruning Nodes</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-12 rtl:pr-12 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#merging-nodes>Merging Nodes</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#nonparametric-neural-networks>Nonparametric Neural Networks</a></li><li class="my-2 scroll-my-6 scroll-py-6"><a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href=#teacherstudent-approaches>Teacher/Student Approaches</a></li></ul>
    
    
  </div></nav><article class="w-full break-words flex min-h-[calc(100vh-var(--navbar-height))] min-w-0 justify-center pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]"><main class="w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12"><h1 class="mt-2 text-4xl font-bold tracking-tight text-slate-900 dark:text-slate-100">Model Scheduling</h1><div class="mt-4 mb-16"><div class="text-gray-500 dark:text-gray-300 text-sm flex items-center flex-wrap gap-y-2"><span class=mr-1>Aug 13, 2018</span><span class=mx-1>·</span><div class="group inline-flex items-center text-current gap-x-1.5 mx-1"><img src=/author/naomi-saphra/avatar_hu_173659a9cfad1518.webp alt="Naomi Saphra" class="inline-block h-4 w-4 rounded-full border border-current" loading=lazy><div>Naomi Saphra</div></div><span class=mx-1>·</span>
<span class=mx-1>15 min read</span></div><div class=mt-3></div></div><div class="prose prose-slate lg:prose-xl dark:prose-invert"><p>Models can be built incrementally by modifying their hyperparameters
during training. This is most common in transfer learning settings, in
which we seek to adapt the knowledge in an existing model for a new
domain or task. The more general problem of continuous learning is also
an obvious application. Even with a predefined data set, however,
incrementally constraining the topology of the network can offer
benefits as regularization.</p><h2 id=dynamic-hyperparameters>Dynamic Hyperparameters</h2><p>The easiest incrementally modified models to train may be those in which
hyperparameters are updated at each epoch. In this case, we do not mean
those hyperparameters associated with network topology, such as the
number or dimension of layers. There are many opportunities to adjust
the topology during training, but the model often requires heavy
retraining in order to impose reasonable structure again, as demonstrated clearly in the case of memory networks<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. If we
instead focus on the weights associated with regularizers and gates, we
can gradually learn structure without frequent retraining to accommodate
radically altered topologies.</p><h3 id=curriculum-dropout>Curriculum Dropout</h3><p>Hinton et al.<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> describes dropout as reducing overfitting by preventing
co-adaptation of feature detectors which happened to perfectly fit the
data. In this interpretation, co-adaptive clusters of neurons are
concurrently activated. Randomly suppressing these neurons forces them
to develop independence.</p><p>In standard dropout, these co-adaptive neurons are treated as equally
problematic at all stages of training. However, Morerio et. al.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> posit that early
in training, co-adaptation may represent the beginnings of an optimal
self organization of the network. In this view, these structures mainly
pose the threat of overfitting later in training. The authors therefore
introduce a hyperparameter schedule for the dropout ratio, increasing
the rate of dropout as training continues. To the best of my knowledge,
this is the only proposal of adaptive regularization published.</p><h3 id=mollifying-networks>Mollifying Networks</h3><p>Mollifying networks<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> are, to my knowledge, the only existing
attempt to combine techniques focused on incrementally manipulating the
distribution of data with techniques focused on incrementally
manipulating the representational capacity of the model. Mollifying
networks incrementally lower the temperature of the data through
simulated annealing while simultaneously modifying various
hyperparameters to permit longer-range dependencies. In the case of an
LSTM, they set the output gate to 1, input gate to \(\frac{1}{t}\), and
forget gate to \(1 - \frac{1}{t}\), where \(t\) is the annealing time step.
Using this system, the LSTM initially behaves as a bag-of-words model,
gradually adding the capacity to handle more context at each time step.</p><p>Mollifying networks use a different data schedule for each layer,
annealing the noise in lower layers faster than in higher layers because
lower-level representations are assumed to learn faster.</p><h2 id=adaptive-architectures>Adaptive Architectures</h2><p>The hyperparameters most difficult to modify during training may be
those which dictate the topology of the model architecture itself.
Nonetheless, the deep learning literature contains a long history of
techniques which adapt the model architecture during training, often in
response to the parameters being learned. Methods like these can help
search optimally by smoothing functions at the beginning of training,
speed up learning by starting with a simpler model, or compress a model
to fit easily on a phone or embedded device. Most of these methods could
be classified as either growing a model by adding parameters
mid-training or shrinking a model by pruning edges or nodes.</p><h3 id=architecture-growth>Architecture Growth</h3><p>Some recent transfer learning strategies have relied on growing
architectures by creating entire new modules focused on the new task
with connections to the existing network<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>. If the goal is to
instead augment an existing network by adding a small number of
parameters, the problem bears a resemblance to traditional nonparametric
learning, because we need not explicitly limit the model space to begin
with.</p><p>Classical techniques in neural networks such as Cascade Correlation
Networks<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup> and Dynamic Node Creation<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> added new nodes at random
one by one and trained them individually. On modern large-scale
architectures and problems, this is intractable. Furthermore, the main
advantage of such methods is that they approach a minimal model, which
is an aim that modern deep learning practitioners no longer consider
valuable thanks to leaps in computing power in the decades since. Modern
techniques for incrementally growing networks must make 2 decisions: 1)
When (and where) do we add new parameters? 2) How do we train new
parameters?</p><p>Warde-Farley et. al.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> add parameters in bulk after training an entire network. The
augmentation takes the form of specialized auxiliary layers added to the
existing network in parallel. These layers are trained on class
boundaries that the original generalist model struggles with. The class
boundaries that require special attention are selected by performing
spectral clustering on the confusion matrix of a holdout data set,
partitioning the classes into challenging subproblems.</p><p>The auxiliary layers are initialized randomly in parallel with the
original generalist system, and then are each trained only on examples
from their assigned partition of the classes. The original generalist
network is held fixed, other than fine-tuning the final classification
layer. The resulting network is a mixture of experts, which was shown to
improve results on an image classification problem.</p><p>Neurogenesis Deep Learning (NDL)<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>, meanwhile, makes autoencoders
capable of lifelong learning. This strategy updates the topology of an
autoencoder by adding neurons when the model encounters outliers that it
performs especially poorly on. These new parameters are trained
exclusively on those outliers, allowing the existing decoder parameters
to update with much smaller step sizes. Existing encoder parameters
update only if they are connected directly to the new neuron.</p><p>After introducing and training these new neurons, NDL stabilizes the
existing structure of the network using a method the authors call
&ldquo;intrinsic replay&rdquo;. They reconstruct approximations of previously seen
samples and train on these reconstructions.</p><p>Another system that permits lifelong learning is the infinite Restricted
Boltzmann Machine (RBM) <sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>. This extension of the classic RBM
parameterizes hidden units by unique indices, expressing an ordering.
These indices are used to enforce an order on the growth of the network
by favoring older nodes until they have converged, permitting the system
to grow arbitrarily large. An intriguing approach, but it is not obvious
how to apply similar modifications to networks other than the
idiosyncratic generative architecture of the RBM.</p><p>None of these augmentation techniques support recurrent architectures.
In modern natural language processing settings, this is a fatal
limitation. However, it is possible that some of these techniques may be
adapted for RNNs, especially since training specialized subsystems has
been recently tackled in these environments <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>.</p><h3 id=architecture-pruning>Architecture Pruning</h3><p>Much recent research has focused on the possibility of pruning edges or
entire neurons from trained networks. This approach is promising not
only for the purpose of compression, but potentially as a way of
increasing the generalizability of a network.</p><h4 id=pruning-edges>Pruning Edges</h4><p>Procedures that prune edges rather than entire neurons may not reduce
the dimensional type of the network. However, they will make the network
sparser, leading to possible memory savings. A sparser network also
occupies a smaller parameter space, and may therefore still more
general.</p><p>Han et. al.<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> takes the basic approach of setting weights to 0 if they fall
below a certain threshold. This approach is highly effective for
compression, because the number of weights to be pruned can be easily
modified through the threshold.</p><p>LeCun et. al.<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> and Hassibi et. al.<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup> both select weights to prune based on Taylor series
approximation of the change in error resulting from trimming. While
these methods were successful for older shallow networks, performing
these operations on an entire network requires a Hessian matrix to be
computed over all parameters, which is generally intractable for deep
modern architectures. Dong et. al.<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup> presents a more efficient alternative by
performing optimal brain surgery over individual layers instead.</p><h4 id=pruning-nodes>Pruning Nodes</h4><p>Pruning entire nodes has the advantage of reducing the entire
dimensionality of the network. It also may be faster than choosing
individual edges to prune, because having more nodes than constituent
edges reduces the number of candidates to consider for pruning.</p><p>He et al.<sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup> selects which neuron \(w_i^\ell\) to prune from layer \(\ell\)
with width \(d_{\ell}\) by calculating the importance of each node. They
test several importance metrics, finding that the highest performance
results from using the &lsquo;onorm&rsquo;, or average \(l_1\) norm of the activation
pattern of the node:</p><p>\(\mathrm{onorm}(w_i^\ell) = \frac{1}{d_{\ell+1}} \sum_{j = 1}^{d_{\ell+1}} |w_{ij}^{\ell+1}|\)</p><p>Net-trim <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup> likewise relies on the \(l_1\) norm to induce sparsity.</p><p>Wolfe et al.<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> compares the results of importance based pruning to a brute force
method that will greedily select a node to be sacrificed based on its
impact on performance. In the brute force method, they rerun the network
on the test data without each node and sort the nodes according to the
error of the resulting network. Their importance metrics are based on
neuron-level versions of the Taylor series approximations of that impact<sup id=fnref1:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>.</p><p>In the first algorithm tested, they rank all nodes according to their
importance and then remove each node in succession. In the second
algorithm, they re-rank the nodes after each removal, in order to
account for the effects of subnetworks that generate and then cancel. In
the second case, they find that it is possible to prune up to 60% of
nodes in a network trained on mnist without significant loss in
performance. This supports an early observation<sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> that the majority
of parameters in a network are unnecessary, and their effect is limited
to generating and then canceling their own noise. The strength of this
effect supports the idea that backpropagation implicitly trains a
minimal network for the task given.</p><p>Srinivas and Babu<sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> prune with the goal of reducing the redundancy of the network, so
they select nodes to remove based on the similarity of their weights to
other neurons in the same layer. Diversity networks<sup id=fnref:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup>, meanwhile,
choose based on the diversity of their activation patterns. In order to
sample a diverse selection of nodes, they use a Determinantal Point
Process. This technique minimizes the dependency between nodes sampled.
They followed this pruning process by <a href=#merging-nodes>fusing</a> the nodes pruned back into
the network.</p><p>An intriguing difference emerges between the observations in these
papers. While Mariet and Sra<sup id=fnref1:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> find that in deeper layers they sample more nodes
from the DPP, Philipp and Carbonell <sup id=fnref1:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup> prune more nodes by brute force in the deeper
layer of a 2-layer network. In other words, diversity networks retain
more nodes at deeper layers while greedy brute force approaches remove
more from the same layers. These results point to fundamental
differences between the respective outcomes of these algorithms and
warrant further investigation.</p><h5 id=merging-nodes>Merging Nodes</h5><p>Mariet and Sra<sup id=fnref2:22><a href=#fn:22 class=footnote-ref role=doc-noteref>22</a></sup> found that performance increased after their DPP-based pruning if
they then merged the pruned nodes back into the network. They achieved
this by re-weighting the remaining nodes in the pruned layer to minimize
the difference in activation outputs before and after pruning:</p><p>\( \min_{\tilde{w}_{ij} \in \mathbb{R}} | \sum_{i=1}^k \tilde{w}_{ij} v_i - \sum_{i=1}^{d_{\ell}} w_{ij} v_i |_2 \)</p><p>Because the DPP is focused on selecting an independent set of neurons,
it seems likely that pruning will select at least 1 node within any
given noise cancellation system to keep, since those cancellation
subnetworks are by necessity highly dependent. The merging step in that
case would merge the noise canceling components back into the noise
generating nodes or vice versa. This would make merging a particular
necessity in diversity networks, but it may still present a tractable
alternative to retraining after a different pruning algorithm.</p><h3 id=nonparametric-neural-networks>Nonparametric Neural Networks</h3><p>The pruning and growing strategies are combined in only one work, to my
knowledge. Nonparametric Neural Networks (NNNs)<sup id=fnref:23><a href=#fn:23 class=footnote-ref role=doc-noteref>23</a></sup> combine adding
neurons with imposing a sparsity-inducing penalty over neurons. For a
feedforward network with \(N^L\) layers, authors introduce 2 such
regularizers, a &ldquo;fan-in&rdquo; and a &ldquo;fan-out&rdquo; variant:</p><p>\(
\Omega_{\mathrm{in}} = \sum_{\ell = 1}^{N^L} \sum_{j = 1}^{d_\ell} \left( \sum_{i = 1}^{d_{\ell}} |w_{ij}^{\ell+1}|^p \right)^{\frac{1}{p}}\)</p><p>\(\Omega_{\mathrm{out}} = \sum_{\ell = 1}^{N^L} \sum_{i = 1}^{d_{\ell}} \left( \sum_{j = 1}^{d_\ell+1} |w_{ij}^{\ell}|^p \right)^{\frac{1}{p}}\)</p><p>In other words, the fan-in variant penalizes the \(p\)-norm of the inputs
to each neuron, while the fan-out of variant penalizes the \(p\)-norm of
the outputs from each neuron. In the case of feedforward networks,
either of these regularizers can be added to the loss function with any
positive weight \(\lambda\) and \(0 &lt; p &lt; \infty\) to guarantee that the
objective will converge at some finite number of neurons.</p><p>NNNs offer a combination of beneficial strategies for adapting the
network. In particular with \(p = 1\) or 2, induces sparsity by applying
pressure to form <em>zero-valued neurons</em>, or neurons which have either a
fan-in or fan-out value of 0. At intervals we can remove these
zero-valued neurons which result. At the same time, we can introduce new
zero-valued neurons at different locations in the network, and the
regularizer guarantees the objective will converge, so we can stop
adding neurons at any point that performance begins to decline.</p><p>However, there are clear issues with this approach. The first obvious
limitation is that this regularizer cannot be applied in any network
with recurrences. This constraint reduces the strategy&rsquo;s usefulness in
many natural language domains where state-of-the-art performance
requires a RNN.</p><p>Another disadvantage to this method is the choice to insert zero-valued
neurons by initializing either the input or output weight vector as 0
and randomly initializing the other associated vector. We therefore
retrain the entire network with each interval, rather than intelligently
initializing and training the new node to accelerate convergence. While
this approach may converge to an optimal number of nodes, it does
nothing to accelerate training or help new nodes specialize.</p><p>Finally, this approach adds and removes entire neurons to create a final
dense network. It therefore forfeits the potential regularization
advantages of the sparser networks which result from instead pruning
weights.</p><h2 id=teacherstudent-approaches>Teacher/Student Approaches</h2><p>It is also possible to produce a larger or smaller model based on an
existing network by fresh training. When investigating any adaptive
architecture, it is crucial to compare with a baseline which uses the
previous state of the network as a teacher to a student network which
has the new architecture.</p><p>The approach of teacher/student learning, in which the teacher network&rsquo;s
outputs layer are used in lieu of or in addition to true labels, was
introduced in the particular case of distillation learning <sup id=fnref:24><a href=#fn:24 class=footnote-ref role=doc-noteref>24</a></sup>.
Distillation is a technique for compressing a large ensemble or
generally expensive classifier with high performance. A smaller network
is trained using an objective that combines a loss function applied to true labels with cross-entropy against the logit layer of the large teacher network. In addition to compression, teacher/student learning is effective for domain adaptation technique <sup id=fnref:25><a href=#fn:25 class=footnote-ref role=doc-noteref>25</a></sup>, suggesting it may be useful for adapting to a new time step in a data schedule.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Sachan, Mrinmaya, and Eric Xing. &ldquo;Easy questions first? a case study on curriculum learning for question answering.&rdquo; <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</em> (Volume 1: Long Papers). Vol. 1. 2016.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. <em>CoRR</em> abs/1207.0580.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal, and Vittorio Murino. 2017. Curriculum Dropout. <em>arXiv:1703.06229 [^cs, stat]:</em>. Retrieved March 22, 2017 from <a href=http://arxiv.org/abs/1703.06229 target=_blank rel=noopener>http://arxiv.org/abs/1703.06229</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. 2016. Mollifying Networks. <em>arXiv:1608.04980 [^cs]:</em>. Retrieved October 7, 2016 from <a href=http://arxiv.org/abs/1608.04980 target=_blank rel=noopener>http://arxiv.org/abs/1608.04980</a>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In <em>arXiv:1611.01587 [^cs]:</em>. Retrieved November 11, 2016 from <a href=http://arxiv.org/abs/1611.01587 target=_blank rel=noopener>http://arxiv.org/abs/1611.01587</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive Neural Networks. <em>arXiv:1606.04671 [^cs]:</em>. Retrieved September 14, 2016 from <a href=http://arxiv.org/abs/1606.04671 target=_blank rel=noopener>http://arxiv.org/abs/1606.04671</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>Scott E. Fahlman and Christian Lebiere. 1989. The cascade-correlation learning architecture. Retrieved November 30, 2016 from <a href=http://repository.cmu.edu/compsci/1938/ target=_blank rel=noopener>http://repository.cmu.edu/compsci/1938/</a>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Ash. 1989. Dynamic node creation in backpropagation networks. In <em>International 1989 Joint Conference on Neural Networks</em>, 623 vol.2. <a href=https://doi.org/10.1109/IJCNN.1989.118509 target=_blank rel=noopener>https://doi.org/10.1109/IJCNN.1989.118509</a>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>David Warde-Farley, Andrew Rabinovich, and Dragomir Anguelov. 2014. Self-informed neural network structure learning. <em>arXiv preprint arXiv:1412.6563</em>. Retrieved June 9, 2016 from <a href=http://arxiv.org/abs/1412.6563 target=_blank rel=noopener>http://arxiv.org/abs/1412.6563</a>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Craig M. Vineyard, Kristofor D. Carlson, Conrad D. James, and James B. Aimone. 2016. Neurogenesis Deep Learning. <em>arXiv:1612.03770 [^cs, stat]:</em>. Retrieved February 27, 2017 from <a href=http://arxiv.org/abs/1612.03770 target=_blank rel=noopener>http://arxiv.org/abs/1612.03770</a>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>Marc-Alexandre Cote and Hugo Larochelle. 2015. An Infinite Restricted Boltzmann Machine. <em>arXiv:1502.02476 [^cs]:</em>. Retrieved February 8, 2018 from <a href=http://arxiv.org/abs/1502.02476 target=_blank rel=noopener>http://arxiv.org/abs/1502.02476</a>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>Shlomo E. Chazan, Jacob Goldberger, and Sharon Gannot. 2017. Deep recurrent mixture of experts for speech enhancement. <em>2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em>: 359&ndash;363.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both Weights and Connections for Efficient Neural Networks. <em>arXiv:1506.02626 [^cs]:</em>. Retrieved May 26, 2016 from <a href=http://arxiv.org/abs/1506.02626 target=_blank rel=noopener>http://arxiv.org/abs/1506.02626</a>&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In <em>Advances in neural information processing systems</em>, 598&ndash;605.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>Babak Hassibi, David G. Stork, and Gregory J. Wolff. 1993. Optimal brain surgeon and general network pruning. In <em>Neural Networks, 1993., IEEE International Conference on</em>, 293&ndash;299.&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon. <em>arXiv:1705.07565 [^cs]:</em>. Retrieved from <a href=http://arxiv.org/abs/1705.07565 target=_blank rel=noopener>http://arxiv.org/abs/1705.07565</a>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu. 2014. Reshaping deep neural network for fast decoding by node-pruning. <em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>: 245&ndash;249.&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>Aghasi, Alireza, et al. &ldquo;Net-trim: Convex pruning of deep neural networks with performance guarantee.&rdquo; <em>Advances in Neural Information Processing Systems</em>. 2017. <a href=http://arxiv.org/abs/1611.05162 target=_blank rel=noopener>http://arxiv.org/abs/1611.05162</a>&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>Nikolas Wolfe, Aditya Sharma, Lukas Drude, and Bhiksha Raj. 2017. &ldquo;The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning.&rdquo;&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>Michael C. Mozer and Paul Smolensky. 1989. Using Relevance to Reduce Network Size Automatically.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>Suraj Srinivas and R. Venkatesh Babu. 2015. Data-free parameter pruning for deep neural networks. <em>arXiv preprint arXiv:1507.06149</em>. Retrieved October 5, 2016 from <a href=http://arxiv.org/abs/1507.06149 target=_blank rel=noopener>http://arxiv.org/abs/1507.06149</a>&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:22><p>Zelda Mariet and Suvrit Sra. 2015. Diversity Networks: Neural Network Compression Using Determinantal Point Processes. <em>arXiv:1511.05077 [^cs]:</em>. Retrieved February 9, 2018 from <a href=http://arxiv.org/abs/1511.05077 target=_blank rel=noopener>http://arxiv.org/abs/1511.05077</a>&#160;<a href=#fnref:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:22 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:23><p>George Philipp and Jaime G. Carbonell. 2017. Nonparametric Neural Networks. In <em>arXiv:1712.05440 [^cs]:</em>. Retrieved February 18, 2018 from <a href=http://arxiv.org/abs/1712.05440 target=_blank rel=noopener>http://arxiv.org/abs/1712.05440</a>&#160;<a href=#fnref:23 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:24><p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. <em>arXiv:1503.02531 [^cs, stat]:</em>. Retrieved September 22, 2016 from <a href=http://arxiv.org/abs/1503.02531 target=_blank rel=noopener>http://arxiv.org/abs/1503.02531</a>&#160;<a href=#fnref:24 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:25><p>Jinyu Li, Michael L. Seltzer, Xi Wang, Rui Zhao, and Yifan Gong. 2017. Large-Scale Domain Adaptation via Teacher-Student Learning. <em>arXiv:1708.05466 [^cs]:</em>. Retrieved August 26, 2017 from <a href=http://arxiv.org/abs/1708.05466 target=_blank rel=noopener>http://arxiv.org/abs/1708.05466</a>&#160;<a href=#fnref:25 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><time class="mt-12 mb-8 block text-xs text-gray-500 ltr:text-right rtl:text-left dark:text-gray-400" datetime=2018-08-13T00:00:00.000Z><span>Last updated on</span>
Aug 13, 2018</time><div class="container mx-auto prose prose-slate lg:prose-xl dark:prose-invert mt-5"><div class="max-w-prose print:hidden"><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank rel=noopener class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fnsaphra.net%2Fpost%2Fmodel-scheduling%2F&amp;text=Model+Scheduling" title=X aria-label=X id=share-link-x><svg style="height:1em" viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</a><a target=_blank rel=noopener class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fnsaphra.net%2Fpost%2Fmodel-scheduling%2F&amp;t=Model+Scheduling" title=Facebook aria-label=Facebook id=share-link-facebook><svg style="height:1em" viewBox="0 0 24 24"><path fill="currentcolor" d="M22 12c0-5.52-4.48-10-10-10S2 6.48 2 12c0 4.84 3.44 8.87 8 9.8V15H8v-3h2V9.5C10 7.57 11.57 6 13.5 6H16v3h-2c-.55.0-1 .45-1 1v2h3v3h-3v6.95c5.05-.5 9-4.76 9-9.95z"/></svg>
</a><a target=_blank rel=noopener class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?subject=Model%20Scheduling&amp;body=https%3A%2F%2Fnsaphra.net%2Fpost%2Fmodel-scheduling%2F" title=Email aria-label=Email id=share-link-email><svg style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg>
</a><a target=_blank rel=noopener class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fnsaphra.net%2Fpost%2Fmodel-scheduling%2F&amp;title=Model+Scheduling" title=LinkedIn aria-label=LinkedIn id=share-link-linkedin><svg style="height:1em" height="1em" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</a><a target=_blank rel=noopener class="m-1 rounded-md bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral-300 dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="whatsapp://send?text=Model+Scheduling%20https%3A%2F%2Fnsaphra.net%2Fpost%2Fmodel-scheduling%2F" title=WhatsApp aria-label=WhatsApp id=share-link-whatsapp><svg style="height:1em" viewBox="0 0 256 256" fill="currentcolor"><path d="m187.58 144.84-32-16a8 8 0 00-8 .5l-14.69 9.8a40.55 40.55.0 01-16-16l9.8-14.69a8 8 0 00.5-8l-16-32A8 8 0 00104 64a40 40 0 00-40 40 88.1 88.1.0 0088 88 40 40 0 0040-40 8 8 0 00-4.42-7.16zM152 176a72.08 72.08.0 01-72-72 24 24 0 0119.29-23.54l11.48 23L101 118a8 8 0 00-.73 7.51 56.47 56.47.0 0030.15 30.15A8 8 0 00138 155l14.61-9.74 23 11.48A24 24 0 01152 176zM128 24A104 104 0 0036.18 176.88l-11.35 34.05a16 16 0 0020.24 20.24l34.05-11.35A104 104 0 10128 24zm0 192a87.87 87.87.0 01-44.06-11.81 8 8 0 00-6.54-.67L40 216l12.47-37.4a8 8 0 00-.66-6.54A88 88 0 11128 216z"/></svg></a></section><div class="flex pt-12 pb-4"><img class="mr-4 h-24 w-24 rounded-full" width=96 height=96 alt="Naomi Saphra" src=/author/naomi-saphra/avatar_hu_f30f2983c45c7e09.jpg loading=lazy><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Authors</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300"><a href=https://nsaphra.net/ class=no-underline>Naomi Saphra</a></div><div class="text-sm font-bold text-neutral-700 dark:text-neutral-300">Research Fellow</div><div class="text-2xl sm:text-lg pt-1"><div class="flex flex-wrap text-neutral-500 dark:text-neutral-300"><a class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=/ aria-label=At-Symbol><svg style="height:1em" viewBox="0 0 24 24"><path fill="none" stroke="currentcolor" stroke-linecap="round" stroke-width="1.5" d="M16.5 12a4.5 4.5.0 11-9 0 4.5 4.5.0 019 0zm0 0c0 1.657 1.007 3 2.25 3S21 13.657 21 12a9 9 0 10-2.636 6.364M16.5 12V8.25"/></svg></a>
<a class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://bsky.app/profile/nsaphra.bsky.social target=_blank rel=noopener rel="me noopener noreferrer" aria-label=Brands/Bluesky><svg style="height:1em" viewBox="0 0 568 501"><path fill="currentcolor" d="M123.121 33.6637C188.241 82.5526 258.281 181.681 284 234.873c25.719-53.192 95.759-152.3204 160.879-201.2093C491.866-1.61183 568-28.9064 568 57.9464 568 75.2916 558.055 203.659 552.222 224.501c-20.275 72.453-94.155 90.933-159.875 79.748C507.222 323.8 536.444 388.56 473.333 453.32c-119.86 122.992-172.272-30.859-185.702-70.281C285.169 375.812 284.017 372.431 284 375.306 283.983 372.431 282.831 375.812 280.369 383.039c-13.43 39.422-65.842 193.273-185.7023 70.281C31.5556 388.56 60.7778 323.8 175.653 304.249 109.933 315.434 36.0535 296.954 15.7778 224.501 9.94525 203.659.0 75.2916.0 57.9464.0-28.9064 76.1345-1.61183 123.121 33.6637z"/></svg></a>
<a class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://twitter.com/nsaphra target=_blank rel=noopener rel="me noopener noreferrer" aria-label=Brands/X><svg style="height:1em" viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg></a>
<a class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href="https://scholar.google.co.uk/citations?user=TPhVfX8AAAAJ" target=_blank rel=noopener rel="me noopener noreferrer" aria-label=Academicons/Google-Scholar><svg style="height:1em" viewBox="0 0 512 512"><path fill="currentcolor" d="M343.759 106.662V79.43L363.524 64h-213.89L20.476 176.274h85.656a82.339 82.339.0 00-.219 6.225c0 20.845 7.22 38.087 21.672 51.861 14.453 13.797 32.252 20.648 53.327 20.648 4.923.0 9.75-.368 14.438-1.024-2.907 6.5-4.374 12.523-4.374 18.142.0 9.875 4.499 20.43 13.467 31.642-39.234 2.67-68.061 9.732-86.437 21.163-10.531 6.5-19 14.704-25.39 24.531-6.391 9.9-9.578 20.515-9.578 31.962.0 9.648 2.062 18.336 6.219 26.062 4.156 7.726 9.578 14.07 16.312 18.984 6.718 4.968 14.469 9.101 23.219 12.469 8.734 3.344 17.406 5.718 26.061 7.062A167.052 167.052.0 00180.555 448c13.469.0 26.953-1.734 40.547-5.187 13.562-3.485 26.28-8.642 38.171-15.493 11.86-6.805 21.515-16.086 28.922-27.718 7.39-11.68 11.094-24.805 11.094-39.336.0-11.016-2.25-21.039-6.75-30.14-4.468-9.073-9.938-16.542-16.452-22.345-6.501-5.813-13-11.155-19.516-15.968-6.5-4.845-12-9.75-16.468-14.813-4.485-5.046-6.735-10.054-6.735-14.984.0-4.921 1.734-9.672 5.216-14.265 3.455-4.61 7.674-9.048 12.61-13.306 4.937-4.25 9.875-8.968 14.796-14.133 4.922-5.147 9.141-11.827 12.61-20.008 3.485-8.18 5.203-17.445 5.203-27.757.0-13.453-2.547-24.46-7.547-33.314-.594-1.022-1.218-1.803-1.875-3.022l56.907-46.672v17.119c-7.393.93-6.624 5.345-6.624 10.635V245.96c0 5.958 4.875 10.834 10.834 10.834h3.989c5.958.0 10.833-4.875 10.833-10.834V117.293c0-5.277.778-9.688-6.561-10.63zm-107.36 222.48c1.14.75 3.704 2.78 7.718 6.038 4.05 3.243 6.797 5.695 8.266 7.414a443.553 443.553.0 016.376 7.547c2.813 3.375 4.718 6.304 5.718 8.734 1 2.477 2.016 5.461 3.047 8.946a38.27 38.27.0 011.485 10.562c0 17.048-6.564 29.68-19.656 37.859-13.125 8.18-28.767 12.274-46.938 12.274-9.187.0-18.203-1.093-27.063-3.196-8.843-2.116-17.311-5.336-25.39-9.601-8.078-4.258-14.577-10.204-19.5-17.797-4.938-7.64-7.407-16.415-7.407-26.25.0-10.32 2.797-19.29 8.422-26.906 5.594-7.625 12.938-13.391 22.032-17.315 9.063-3.946 18.25-6.742 27.562-8.398a157.865 157.865.0 0128.438-2.555c4.47.0 7.936.25 10.405.696.455.219 3.032 2.07 7.735 5.563 4.704 3.462 7.625 5.595 8.75 6.384zm-3.359-100.579c-7.406 8.86-17.734 13.288-30.953 13.288-11.86.0-22.298-4.764-31.266-14.312-9-9.523-15.422-20.328-19.344-32.43-3.937-12.109-5.906-23.984-5.906-35.648.0-13.694 3.596-25.352 10.781-34.976 7.187-9.65 17.5-14.485 30.938-14.485 11.875.0 22.374 5.038 31.437 15.157 9.094 10.085 15.61 21.413 19.517 33.968 3.922 12.54 5.873 24.53 5.873 35.984.0 13.446-3.702 24.61-11.076 33.454z"/></svg></a>
<a class="pr-2 transition-transform hover:scale-125 hover:text-primary-700 dark:hover:text-primary-400" style=will-change:transform href=https://www.semanticscholar.org/author/Naomi-Saphra/2362960 target=_blank rel=noopener rel="me noopener noreferrer" aria-label=Academicons/Semantic-Scholar><svg style="height:1em" viewBox="0 0 512 512"><path fill="currentcolor" d="M379.087 75.202c18.168 40.684 25.533 83.89 32.421 127.21-1.265.358-2.528.72-3.794 1.082-.91-2.534-1.984-5.021-2.707-7.61-5.218-18.653-10.48-37.296-15.474-56.011-1.797-6.733-6.035-10.084-12.096-13.381-8.901-4.842-17.313-11.084-24.69-18.046-4.707-4.44-8.735-7.149-15.413-7.078-44.46.47-88.925.515-133.384.924-2.963.03-6.63 1.124-8.728 3.065-8.089 7.484-15.671 15.514-25.642 25.556 26.3 64.04 39.522 133.84 33.845 208.044-12.626-8.084-22.4-14.48-22.981-31.418-2.904-84.661-29.02-161.225-83.58-227.108-1.228-1.482-1.838-3.476-2.738-5.23h284.96zM48.73 107.847c12.663.0 25.332-.2 37.984.172 2.51.072 6.022 1.668 7.277 3.68 37.836 60.79 67.334 124.635 71.155 197.682.018.29-.282.594-1.362 2.716-22.612-77.293-63.404-142.735-115.872-201.39.274-.952.545-1.906.819-2.86zM8 161.029c18.09-.658 33.39-1.318 48.692-1.602 1.541-.03 3.36 2.009 4.65 3.443 29.848 33.202 56.936 68.281 73.633 110.235 3.177 7.98 5.351 16.36 7.989 24.555C108.379 243.235 60.254 202.538 8 161.028zm194.474 275.77c-31.481-50.066-61.803-98.29-92.128-146.513l1.112-1.428c2.542 2.047 56.622 45.412 80.91 65.302 6.766 5.541 11.878 5.441 18.915-.274 82.584-67.085 174.737-117.862 272.583-158.809 5.223-2.185 10.64-3.916 15.983-5.816 1.186-.42 2.44-.654 4.151-.222-113.623 65.987-222.022 138.239-301.526 247.76z"/></svg></a></div></div></div></div><div class="pt-1 no-prose w-full"><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex flex-col md:flex-row flex-nowrap justify-between gap-5 pt-2"><div><a class="group flex no-underline" href=/post/hands/><span class="mt-[-0.3rem] me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">What Does a Coder Do If They Can't Type?</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">Aug 8, 2019</span></span></a></div><div><a class="group flex text-right no-underline" href=/post/lda/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Understanding Latent Dirichlet Allocation</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">Jul 9, 2012
</span></span><span class="mt-[-0.3rem] ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class=ltr:inline>&rarr;</span></span></a></div></div></div></div></div></main></article></div></div><div class=page-footer><footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200"><p class="powered-by text-center">© 2025 Me. This work is licensed under CC BY NC ND 4.0</p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class="powered-by text-center">Published with Hugo Blox Builder — the free, open source website builder that empowers creators.</p></footer></div></body></html>