<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Naomi Saphra</title><link>https://nsaphra.net/</link><atom:link href="https://nsaphra.net/index.xml" rel="self" type="application/rss+xml"/><description>Naomi Saphra</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Feb 2025 00:00:00 +0000</lastBuildDate><image><url>https://nsaphra.net/media/icon_hu_ecc3d54b494abbac.png</url><title>Naomi Saphra</title><link>https://nsaphra.net/</link></image><item><title>Example Talk</title><link>https://nsaphra.net/event/example/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://nsaphra.net/event/example/</guid><description>&lt;!-- raw HTML omitted -->
&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Hugo Blox Builder&amp;rsquo;s &lt;a href="https://docs.hugoblox.com/reference/content-types/" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://docs.hugoblox.com/reference/markdown/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including &lt;a href="https://docs.hugoblox.com/reference/markdown/" target="_blank" rel="noopener">page elements&lt;/a> such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>How to visualize training dynamics in neural networks</title><link>https://nsaphra.net/publication/hublog/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/hublog/</guid><description/></item><item><title>PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</title><link>https://nsaphra.net/publication/polypythias/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/polypythias/</guid><description/></item><item><title>Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</title><link>https://nsaphra.net/publication/prashanth-2024-recitereconstructrecollectmemorization/</link><pubDate>Thu, 01 May 2025 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/prashanth-2024-recitereconstructrecollectmemorization/</guid><description/></item><item><title>Attribute Diversity Determines the Systematicity Gap in VQA</title><link>https://nsaphra.net/publication/systematicity/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/systematicity/</guid><description/></item><item><title>Benchmarks as Microscopes: A Call for Model Metrology</title><link>https://nsaphra.net/publication/bench/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/bench/</guid><description/></item><item><title>Causation Does Not Imply Correlation: A Study of Circuit Mechanisms and Model Behaviors</title><link>https://nsaphra.net/publication/jenny/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/jenny/</guid><description/></item><item><title>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</title><link>https://nsaphra.net/publication/li-2024-chatgptdoesnttrustchargers/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/li-2024-chatgptdoesnttrustchargers/</guid><description/></item><item><title>Distributional Scaling Laws for Emergent Capabilities</title><link>https://nsaphra.net/publication/rosie/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/rosie/</guid><description/></item><item><title>Dynamic Masking Rate Schedules for MLM Pretraining</title><link>https://nsaphra.net/publication/ankner/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/ankner/</guid><description/></item><item><title>Fast Forwarding Low-Rank Training</title><link>https://nsaphra.net/publication/ff/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/ff/</guid><description/></item><item><title>First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models</title><link>https://nsaphra.net/publication/parse/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/parse/</guid><description/></item><item><title>Loss in the Crowd: Hidden Breakthroughs in Language Model Training</title><link>https://nsaphra.net/publication/sara-loss/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sara-loss/</guid><description/></item><item><title>Mechanistic?</title><link>https://nsaphra.net/publication/sarah-mech/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sarah-mech/</guid><description/></item><item><title>Sometimes I am a Tree: Data drives fragile hierarchical generalization</title><link>https://nsaphra.net/publication/sunny/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sunny/</guid><description/></item><item><title>Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs</title><link>https://nsaphra.net/publication/chen/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/chen/</guid><description/></item><item><title>TRAM: Bridging Trust Regions and Sharpness Aware Minimization</title><link>https://nsaphra.net/publication/sherborne-tram-2023/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sherborne-tram-2023/</guid><description/></item><item><title>Transcendence: Generative Models Can Outperform The Experts That Train Them</title><link>https://nsaphra.net/publication/zhang-2024-transcendence/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/zhang-2024-transcendence/</guid><description/></item><item><title>Understanding biological active sensing behaviors by interpreting learned artificial agent policies</title><link>https://nsaphra.net/publication/johnson-yu-2024-understanding/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/johnson-yu-2024-understanding/</guid><description/></item><item><title>The Parable of the Prinia's Egg: An Allegory for AI Science</title><link>https://nsaphra.net/post/prinia/</link><pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/prinia/</guid><description>&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/eggs.jpg" alt="Prinia eggs" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>When European scientists first encountered the eggs of the tawny-flanked prinia &lt;em>Prinia subflava&lt;/em>, an African nesting bird, they believed they understood what they had found. The prinia lay eggs that exhibited swirls, speckles, and coloration unique to each individual bird. What purpose do such patterns serve in nature? Surely, scientists agreed, these markings allowed the eggs to camouflage and blend into the nest.&lt;/p>
&lt;p>One 19th century naturalist, Charles Francis Massey Swynnerton, offered an alternative explanation. Why would each bird have its own unique patterns? Swynnerton believed that the markings functioned as watermarks, allowing the nesting bird to differentiate its own eggs from those of the cuckoo finch &lt;em>Anomalospiza imberbis&lt;/em>. The cuckoo finch is an &lt;strong>obligate brood parasite&lt;/strong>, meaning that it does not build its own nest, but instead lays its eggs in the nests of other birds, particularly the prinia.&lt;/p>
&lt;p>Since its proposal, evidence for Swynnerton&amp;rsquo;s hypothesis has accumulated. The camouflage hypothesis was based on anecdotal observations of eggs, without correlational or experimental data. In contrast, evidence for the brood parasite hypothesis comes in diverse forms, using many different frameworks for reasoning about causality and explanation, adding up to an indisputable scientific arsenal. What can AI researchers learn from the mature science of biology to strengthen our own evidence and reliably interpret how neural network traits contribute to model behavior?&lt;/p>
&lt;h2 id="instance-level-interpretability">Instance level interpretability&lt;/h2>
&lt;p>Most interpretability work, and much work in science of deep learning, relies on passive observations of phenomena with post-hoc explanations for how artifacts in the model contribute to model outputs. Like the early ornithologists suggesting that prinia eggs were camouflaged, interpretability researchers often identify some phenomenon and link it to model decisions based solely on their intuitions. Some work attempts a more principled approach, by intervening on the trained model to demonstrate that a given feature or circuit has a predictable effect on model judgment. Both approaches might be classed as &lt;strong>instance-level&lt;/strong>, as they consider the behavior of a fully trained model on a given input sample.&lt;/p>
&lt;p>In the past, I have called this reliance on fully trained models &lt;a href="https://thegradient.pub/interpretability-creationism/" target="_blank" rel="noopener">Interpretability Creationism&lt;/a>, and suggested that interpretability researchers instead measure the indicators they focus on throughout training. However, I have not detailed how these creationist approaches can lead us astray, nor have I proposed a clear framework for understanding development. Such a framework falls under the philosophical field of &lt;strong>epistemology&lt;/strong>, the study of what makes a belief justified.&lt;/p>
&lt;h3 id="syntactic-attention-structure">Syntactic Attention Structure&lt;/h3>
&lt;p>To address the epistemology of interpretability, let us construct a case study of the scientific literature on an observed phenomenon in neural networks. One well-documented and intuitive behavior is how Transformer-based &lt;strong>masked language models&lt;/strong> (MLMs) have specialized attention heads that focus on a specific dependency relation, a trait we will call &lt;strong>Syntactic Attention Structure&lt;/strong> (SAS). We can illustrate SAS with an example sentence.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/parse_example.png" alt="My bird builds ugly nests" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>In the preceding sentence, the model might be called upon to predict the masked-out target word &lt;em>builds&lt;/em>. During inference, its specialized &lt;code>nsubj&lt;/code> head will place its highest weight on &lt;em>bird&lt;/em>, while its specialized &lt;code>dobj&lt;/code> head will place its highest weight on &lt;em>nests&lt;/em>. This specialization behavior emerges naturally, without any explicit inductive bias, over the normal course of training for models like BERT.&lt;/p>
&lt;p>SAS was discovered concurrently in two papers. First, &lt;a href="https://aclanthology.org/W19-4828/" target="_blank" rel="noopener">Clark et al. (2019)&lt;/a> observed that specialized attention heads provide an implicit parse, which aligns with prior notions of dependency syntax, as measured by &lt;strong>Unlabeled Attachment Score&lt;/strong> (UAS). This observation provides instance-level observational evidence for the role of SAS in masked language modeling. Second, &lt;a href="https://aclanthology.org/P19-1580/" target="_blank" rel="noopener">Voita et al. (2019)&lt;/a> discovered that pruning specialized syntactic heads damaged model performance more than pruning other heads, providing instance-level causal evidence for the role of SAS. What&amp;rsquo;s missing from these studies, from an epistemological standpoint?&lt;/p>
&lt;h3 id="when-does-instance-level-analysis-fail">When does instance-level analysis fail?&lt;/h3>
&lt;p>There is an assumption underlying claims in interpretability: that the observed phenomenon measured by the interpretable &lt;strong>indicator&lt;/strong> metric, such as implicit parse UAS, plays a role in determining some &lt;strong>target&lt;/strong> metric, such as MLM validation loss or grammatical capabilities. However, instance-level evidence like the prior work on SAS might not support this assumption.&lt;/p>
&lt;p>First, instance-level &lt;em>observational&lt;/em> evidence might highlight artifacts that arose as a side effect of training, rather than as a crucial element in model decisions. That is, the poorly-understood dynamics of training might lead to structures emerging that are not required at test time. In evolutionary biology, such artifacts are called &lt;a href="https://en.wikipedia.org/wiki/Spandrel_%28biology%29" target="_blank" rel="noopener">spandrels&lt;/a>; proposed examples include human chins and musicality.&lt;/p>
&lt;p>Second, instance-level &lt;em>causal&lt;/em> evidence may be stronger, but remains flawed as evidence for the effect of SAS. For example, what if SAS emerges early in training, and gradually the model develops a more complex representation that does not rely on SAS, but the specialized heads have already become entangled with the rest of the high dimensional representation space? Then these specialized heads are &lt;a href="https://en.wikipedia.org/wiki/Vestigiality" target="_blank" rel="noopener">vestigial&lt;/a>, like a human tailbone, but are integrated into distributed subnetworks that generate and cancel noise, so the network may be particularly brittle to their removal. Another weakness of instance-level causal interpretation is that it may claim that a given behavior is unimportant, when in fact the model relied on that trait to converge on its final solution, as webbing evolves for gliding before an organism can develop wings.&lt;/p>
&lt;h2 id="epistemology-and-evidence">Epistemology and evidence&lt;/h2>
&lt;p>As we see in the example of SAS, treating a model &lt;strong>atemporally&lt;/strong>, in a manner detached from its development, might yield intriguing phenomena and even suggest insights. However, in order to strengthen the evidence for these insights, we argue for applying &lt;strong>developmental&lt;/strong> analysis. Developmental history may shed more light on the effect of a given indicator on a target metric, compared to atemporal data.&lt;/p>
&lt;p>Beyond the question of whether we have access to development data, there are three widely used categories of evidence in the scientific literature. Any one of the following categories might apply in either atemporal or developmental settings.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Observational&lt;/strong>: For example, &lt;em>BERT exhibits SAS and also demonstrates grammatical capabilities&lt;/em>. This is evidence based on passive observation of a given phenomenon. Atemporal observations are not strong evidence of a relationship between that phenomenon and the target of analysis, like a generalization metric. However, observations of development may support a relationship between the indicator and target metrics.&lt;/li>
&lt;li>&lt;strong>Correlational&lt;/strong>: For example, &lt;em>SAS is correlated with grammatical capabilities across a population of multiple models&lt;/em>. This is evidence based on natural variation of a given phenomenon.&lt;/li>
&lt;li>&lt;strong>Causal&lt;/strong>: For example, &lt;em>grammatical capabilities are affected by intervening on SAS&lt;/em>. This is evidence based on experimental interventions on the indicator phenomenon.&lt;/li>
&lt;/ul>
&lt;h3 id="cracking-the-case-of-the-cuckoo-culprit">Cracking the case of the cuckoo culprit&lt;/h3>
&lt;p>Returning to the example of the prinia, the assumption that their egg markings served as camouflage was based on atemporal observational evidence. Brood parasites (the indicator) do, in fact, drive the evolution of egg markings (the target of analysis). There are several epistemically strong pieces of evidence that ornithologists have found for that influence, including:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Atemporal correlational&lt;/strong>: A brood parasite&amp;rsquo;s favored hosts, like the prinia, have more individualized markings than other nesting birds.&lt;/li>
&lt;li>&lt;strong>Developmental observational&lt;/strong>: On continents with a longer evolutionary history of brood parasitism, such as Africa and Australia, nesting birds have more elaborate egg patterns compared to Europe, which has a shorter history of parasitism, or North America, which has no obligate brood parasites at all. If we treat each continent as though we are observing a checkpoint at some point in the evolutionary arms race between parasite and host, we can see that egg markings appear to be dependent on brood parasitism during evolutionary development.&lt;/li>
&lt;li>&lt;strong>Developmental causal&lt;/strong>: Invasive species of nesting birds provide a natural experiment for testing the link between brood parasitism and egg markings. When a host species spreads from its native range to an island without brood parasites, Australian ornithologists have observed that the species gradually loses its intricate egg markings over subsequent generations.&lt;/li>
&lt;/ul>
&lt;h2 id="whats-the-evidence-for-syntactic-attention-structure">What&amp;rsquo;s the evidence for Syntactic Attention Structure?&lt;/h2>
&lt;p>Does SAS play a significant role in the capabilities of a masked language model? In our latest paper, &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>, we test the impact of SAS by mirroring the variety of evidence collected by ornithologists. Let&amp;rsquo;s walk through the relevant results.&lt;/p>
&lt;h3 id="atemporal-correlational">Atemporal correlational&lt;/h3>
&lt;p>Correlational evidence is stronger when the variation in a population is known to be random, other than the metrics in focus, because this scenario allows us to control for the effect of confounding factors. This is the reason why, for example, genetics studies that use adopted children are stronger than those that study children raised by their biological parents, thereby confounding nature and nurture. Fortunately, deep learning allows us to generate random variation across models easily, by varying the training seed.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/corr_plots.png" alt="Correlation scatterplots." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Unfortunately, the correlational results above seem to be bleak for the idea that SAS is crucial to model capabilities. Using 25 independently trained MLMs from &lt;a href="https://arxiv.org/abs/2106.16163" target="_blank" rel="noopener">MultiBERTs&lt;/a>, we see no significant correlation, or even a clear pattern, between UAS (our SAS metric) and either validation loss or linguistic capabilities (in the form of &lt;a href="https://aclanthology.org/2020.tacl-1.25/" target="_blank" rel="noopener">BLiMP score&lt;/a>).&lt;/p>
&lt;p>Should we abandon SAS as a phenomenon unrelated to performance in practice? Not so fast. It may be that random variation does not elicit strong enough differences in SAS to register a behavioral difference. In order to dig deeper, we now turn to using a developmental lens by considering the training process.&lt;/p>
&lt;h3 id="developmental-observational">Developmental observational&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/development_plots.png" alt="BERT training." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Next, we passively observe SAS in an MLM model (a retrained BERT_base run with 3 different random seeds), but consider it during the entire course of training rather than at a single checkpoint. Results here appear to be far more promising, thanks to the clarity of an abrupt breakthrough in UAS about 20K steps into training. First, we see that there is a precipitous drop in loss coinciding with this UAS spike (marked ▲). Then, after UAS reaches its peak and begins to plateau, we observe an immediate jump in the MLM&amp;rsquo;s particular linguistic capabilities, as measured by BLiMP (marked ⏺). It certainly appears that the former is dependent on the latter. While we can directly observe that internal SAS structure precedes the acquisition of linguistic capabilities, though, can we guarantee that SAS &lt;em>precipitates&lt;/em> linguistic capabilities?&lt;/p>
&lt;h3 id="developmental-causal">Developmental causal&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/causal_plots.png" alt="Bert training with causal interventions." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>To study how connected these consecutive phase transitions are, we intervene on the training process to suppress and promote SAS, yielding models respectively called BERT_SAS- and BERT_SAS+. Although neither promoting nor suppressing helps long term MLM performance, observing the training process shows clear evidence of a dependency. Promoting SAS leads to an earlier UAS spike, which indeed precipitates an earlier spike in linguistic capabilities. Meanwhile, suppressing SAS prevents any UAS spike and leads to persistently poor linguistic capabilities throughout training.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>By monitoring MLM training, we have found epistemically strong evidence of a connection between SAS and model performance, in particular through linguistic capabilities. These results are described in a new paper led by &lt;a href="https://angie-chen55.github.io/" target="_blank" rel="noopener">Angelica Chen&lt;/a> working with &lt;a href="https://www.ravid-shwartz-ziv.com/" target="_blank" rel="noopener">Ravid Schwartz-Ziv&lt;/a>, &lt;a href="https://kyunghyuncho.me/" target="_blank" rel="noopener">Kyunghyun Cho&lt;/a>, &lt;a href="https://mleavitt.net/" target="_blank" rel="noopener">Matthew Leavitt&lt;/a>, and me: &lt;a href="https://arxiv.org/abs/2309.07311" target="_blank" rel="noopener">Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs&lt;/a>.&lt;/p>
&lt;p>Beyond the results obtained through our approach to interpretability epistemology, this paper also presents a large number of related findings. The full paper is worth reading, I think, as the results described here might not even be the most interesting part! We link these interpretable training dynamics to the broader literature on simplicity bias, model complexity, and phase transitions. We even show how our methods can be used to improve MLM performance by suppressing SAS briefly at the beginning of training. By questioning a seemingly settled result in interpretability, we developed a deeper understanding of MLM training.&lt;/p>
&lt;p>&lt;em>All preceding discussion of the research on brood parasites comes from one of my favorite books, &lt;a href="https://www.goodreads.com/en/book/show/22529402" target="_blank" rel="noopener">Cuckoo: Cheating by Nature&lt;/a> by Nick Davies. I highly recommend it to anyone interested in a deep dive on the evolution of a single survival strategy.&lt;/em>&lt;/p></description></item><item><title>Delays, Detours, and Forks in the Road: Latent State Models of Training Dynamics</title><link>https://nsaphra.net/publication/hu/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/hu/</guid><description/></item><item><title>Interpretability Creationism</title><link>https://nsaphra.net/publication/saphra-2023-interp/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-2023-interp/</guid><description/></item><item><title>Linear Connectivity Reveals Generalization Strategies</title><link>https://nsaphra.net/publication/juneja-linear-2023/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/juneja-linear-2023/</guid><description/></item><item><title>Shapley Interactions for Complex Feature Attribution</title><link>https://nsaphra.net/publication/attrib/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/attrib/</guid><description/></item><item><title>State-of-the-art generalisation research in NLP: a taxonomy and review</title><link>https://nsaphra.net/publication/hupkes-state-art-2023/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/hupkes-state-art-2023/</guid><description/></item><item><title>Towards out-of-distribution generalization in large-scale astronomical surveys: robust networks learn similar representations</title><link>https://nsaphra.net/publication/sultan/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sultan/</guid><description/></item><item><title>Interpretability Creationism</title><link>https://nsaphra.net/post/creationism/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/creationism/</guid><description>&lt;p>For centuries, Europeans agreed that the presence of a cuckoo egg was a great honor to a nesting bird, as it granted an opportunity to exhibit Christian hospitality. The devout bird enthusiastically fed her holy guest, even more so than she would her own (evicted) chicks &lt;a href="https://app.thestorygraph.com/books/37ed3b62-8a3a-448b-9e37-cd5e5f51c640" target="_blank" rel="noopener">(Davies, 2015)&lt;/a>. In 1859, Charles Darwin’s studies of another occasional brood parasite, finches, called into question any rosy, cooperative view of bird behavior &lt;a href="https://app.thestorygraph.com/books/44185106-8198-42ef-bacf-8a9bf691e654" target="_blank" rel="noopener">(Darwin, 1859)&lt;/a>. Without considering the evolution of the cuckoo’s role, it would have been difficult to recognize the nesting bird not as a gracious host to the cuckoo chick, but as an unfortunate dupe. The historical process is essential to understanding its biological consequences; as evolutionary biologist Theodosius Dobzhansky put it, &lt;a href="https://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution#cite_note-Dobz_Nothing-1" target="_blank" rel="noopener">Nothing in Biology Makes Sense Except in the Light of Evolution&lt;/a>.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://upload.wikimedia.org/wikipedia/commons/5/5c/Reed_warbler_cuckoo.jpg" alt="By Per Harald Olsen - Own work, CC BY-SA 3.0" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>Certainly SGD is not literally biological evolution, but post-hoc analysis in machine learning &lt;a href="https://twitter.com/ch402/status/1533164918886703104" target="_blank" rel="noopener">has a lot in common&lt;/a> with scientific approaches in biology, and likewise often requires an understanding of the origin of model behavior. Therefore, the following holds whether looking at parasitic brooding behavior or at the inner representations of a neural network: if we do not consider how a system develops, it is difficult to distinguish a pleasing story from a useful analysis.&lt;/p>
&lt;h2 id="just-so-stories">Just-So Stories&lt;/h2>
&lt;p>We have many pleasing &lt;a href="https://en.wikipedia.org/wiki/Just_So_Stories" target="_blank" rel="noopener">just-so stories&lt;/a> in NLP. Much has been made of interpretable artifacts such as &lt;a href="https://aclanthology.org/2022.acl-long.269.pdf" target="_blank" rel="noopener">syntactic attention distributions&lt;/a> or &lt;a href="https://openai.com/blog/unsupervised-sentiment-neuron/" target="_blank" rel="noopener">selective neurons&lt;/a>. But how can we know if such a pattern of behavior is actually used by the model?
Causal modeling can help, but interventions to test the influence of particular features and patterns may target only particular types of behavior explicitly. In practice, it may be possible only to perform certain types of slight interventions on specific units within a representation, failing to reflect interactions between features properly. Furthermore, in staging these interventions, we create distribution shifts that a model may not be robust to, regardless of whether that behavior is part of a core strategy. Significant distribution shifts can cause erratic behavior, so why shouldn&amp;rsquo;t they cause spurious interpretable artifacts? In practice, we find &lt;a href="https://arxiv.org/pdf/2010.12016.pdf" target="_blank" rel="noopener">no shortage&lt;/a> of incidental observations construed as crucial.&lt;/p>
&lt;p>Fortunately, the study of evolution has provided a number of ways to interpret the artifacts produced by a model. They might be vestigial, like a human tailbone. They may have dependencies, with some features and structures relying on the presence of other properties earlier in training, like the requirement for light sensing before a complex eye can develop. Some artifacts might represent side effects of training, like how junk DNA constitutes a majority of our genetic code without influencing our phenotypes.&lt;/p>
&lt;p>We have a number of theories for how such unused artifacts might emerge while training models. For example, the &lt;a href="https://arxiv.org/abs/1703.00810" target="_blank" rel="noopener">Information Bottleneck Hypothesis&lt;/a> predicts how inputs may be memorized early in training, before representations are compressed to only retain information about the output. These early memorized interpolations may not ultimately be useful when generalizing to unseen data, but they are essential in order to eventually learn to specifically represent the output. We also can infer the possibility of vestigial features, because early training behavior is so distinct from late training: &lt;a href="http://arxiv.org/abs/1905.11604" target="_blank" rel="noopener">earlier models are more simplistic&lt;/a>. In the case of language models, they &lt;a href="http://arxiv.org/abs/2109.06096" target="_blank" rel="noopener">behave similarly to ngram models&lt;/a> early on and &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.16" target="_blank" rel="noopener">exhibit linguistic patterns&lt;/a> later. Side effects of such a heteroskedastic training process could easily be mistaken for crucial components of a trained model.&lt;/p>
&lt;h2 id="the-evolutionary-view">The Evolutionary View&lt;/h2>
&lt;p>I may be unimpressed by &amp;ldquo;interpretability creationist&amp;rdquo; explanations of static fully trained models, but I have engaged in similar analysis myself. I&amp;rsquo;ve published papers on &lt;a href="https://arxiv.org/pdf/2010.02180.pdf" target="_blank" rel="noopener">probing static representations&lt;/a>, and the results often seem intuitive and explanatory. However, the presence of a feature at the end of training is hardly informative about the inductive bias of a model on its own! Consider &lt;a href="https://openreview.net/forum?id=mNtmhaDkAr" target="_blank" rel="noopener">Lovering et al.&lt;/a>, who found that the ease of extracting a feature at the start of training, along with an analysis of the finetuning data, has deeper implications for finetuned performance than we get by simply probing at the end of training.&lt;/p>
&lt;p>Let us consider an explanation usually based on analyzing static models: hierarchical behavior in language models. An example of this approach is the claim that &lt;a href="https://nlp.stanford.edu/pubs/hewitt2019structural.pdf" target="_blank" rel="noopener">words that are closely linked on a syntax tree have representations that are closer together&lt;/a>, compared to words that are syntactically farther. How can we know that the model is behaving hierarchically by grouping words according to syntactic proximity? Alternatively, syntactic neighbors may be more strongly linked due to a strong correlation between nearby words because they have higher joint frequency distributions. For example, perhaps constituents like &amp;ldquo;football match&amp;rdquo; are more predictable due to the frequency of their co-occurrence, compared to more distant relations like that between &amp;ldquo;uncle&amp;rdquo; and &amp;ldquo;football&amp;rdquo; in the sentence, &amp;ldquo;My uncle drove me to a football match&amp;rdquo;. In fact, we can be more confident that some language models are hierarchical, because early models encode more local information in &lt;a href="https://arxiv.org/abs/1811.00225" target="_blank" rel="noopener">LSTMs&lt;/a> and &lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#argument-phase-change" target="_blank" rel="noopener">Transformers&lt;/a>, and they learn longer distance dependencies more easily when those dependencies can be &lt;a href="https://arxiv.org/abs/2010.04650" target="_blank" rel="noopener">stacked onto short familiar constituents&lt;/a> hierarchically.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>I recently had to manage the trap of interpretability creationism myself. My coauthors had found that, when training text classifiers repeatedly with different random seeds, &lt;a href="https://arxiv.org/abs/2205.12411" target="_blank" rel="noopener">models can occur in a number of distinct clusters&lt;/a>. Further, we could predict the generalization behavior of a model based on which other models it was connected to on the loss surface. Now, we suspected that different finetuning runs found models with different generalization behavior because their trajectories entered different basins on the loss surface.&lt;/p>
&lt;p>But could we actually make this claim? What if one cluster actually corresponded to earlier stages of a model? Eventually those models would leave for the cluster with better generalization, so our only real result would be that some finetuning runs were slower than others. We had to demonstrate that training trajectories could actually become trapped in a basin, providing an explanation for the diversity of generalization behavior in trained models. Indeed, when we looked at several checkpoints, we confirmed that models that were very central to either cluster would become &lt;em>even more&lt;/em> strongly connected to the rest of their cluster over the course of training. Instead of offering a just-so story based on a static model, we explored the evolution of observed behavior to confirm our hypothesis.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://nsaphra.net/images/qqp_training.png" alt="k" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;h2 id="a-proposal">A Proposal&lt;/h2>
&lt;p>To be clear, not every question can be answered by &lt;em>only&lt;/em> observing the training process. Causal claims require interventions! In biology, for example, research about antibiotic resistance requires us to deliberately expose bacteria to antibiotics, rather than waiting and hoping to find a natural experiment. Even the claims currently being made based on observations of training dynamics may require experimental confirmation.&lt;/p>
&lt;p>Furthermore, not all claims require &lt;em>any&lt;/em> observation of the training process. Even to ancient humans, many organs had obvious purpose: eyes see, hearts pump blood, and &lt;a href="https://www.scientificamerican.com/article/aristotle-thought-the-brain-was-a-radiator/" target="_blank" rel="noopener">brains are refrigerators&lt;/a>. Likewise in NLP, just by analyzing static models we can make simple claims: that particular neurons activate in the presence of particular properties, or that some types of information remain accessible within a model. However, the training dimension can still clarify the meaning of many observations made in a static model.&lt;/p>
&lt;p>My proposal is simple. Are you developing a method of interpretation or analyzing some property of a trained model? Don&amp;rsquo;t just look at final checkpoint in training. Apply that analysis to several intermediate checkpoints. If you are finetuning a model, check several points both early and late in training. If you are analyzing a large language model, &lt;a href="https://arxiv.org/abs/2106.16163" target="_blank" rel="noopener">MultiBERTs&lt;/a> and &lt;a href="https://nlp.stanford.edu/mistral/getting_started/download.html" target="_blank" rel="noopener">Mistral&lt;/a> both provide intermediate checkpoints sampled from throughout training on masked and autoregressive language models, respectively. Does the behavior that you&amp;rsquo;ve analyzed change over the course of training? Does your belief about the model&amp;rsquo;s strategy actually make sense after observing what happens early in training? There&amp;rsquo;s very little overhead to an experiment like this, and you never know what you&amp;rsquo;ll find!&lt;/p></description></item><item><title>Learning Transductions to Test Systematic Compositionality</title><link>https://nsaphra.net/publication/valvoda-learning-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/valvoda-learning-2022/</guid><description/></item><item><title>One Venue, Two Conferences: The Separation of Chinese and American Citation Networks</title><link>https://nsaphra.net/publication/zhao-one-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/zhao-one-2022/</guid><description/></item><item><title>The MultiBERTs: BERT Reproductions for Robustness Analysis</title><link>https://nsaphra.net/publication/sellam-multiberts-2021/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/sellam-multiberts-2021/</guid><description/></item><item><title>Against Monodomainism</title><link>https://nsaphra.net/post/monodomainism/</link><pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/monodomainism/</guid><description>&lt;p>Reaching the endpoint of a PhD studying how language models learn, I have spent several years telling people that I study &amp;ldquo;machine learning and natural language processing&amp;rdquo;. However, my colleagues who tried to understand or augment image classifiers would describe themselves only as working in &amp;ldquo;machine learning&amp;rdquo;. I argue that this pattern reflects thinking about what it means to be &amp;ldquo;application&amp;rdquo; work or &amp;ldquo;core&amp;rdquo; machine learning that damages our understanding of statistical modeling and deep learning as a whole.&lt;/p>
&lt;p>Why do we know so little about how language models learn? This gap is in part because consideration of NLP as a domain is historically rare in venues that publish most training dynamics research, or analytic work in learning theory. A current search&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> of ICML 2020 publications returned 169 papers with citations to “Association for Computational Linguistics” or “ACL”, even including citations to many potential sister conferences: NAACL, AACL, or EACL. A search for citations to a single vision conference, “Computer Vision and Pattern Recognition” or “CVPR”, turned up 541 papers. In COLT publications since 2017, the same searches turned up 13 and 23 papers, respectively. In ICML 2020, Wikitext-* or PTB references found only 16 results, while the most popular small corpus for image classification, MNIST, found 264 ICML publications&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>Linguistics provides us with the salient concept of &lt;em>markedness&lt;/em> &lt;a href="https://www.degruyter.com/document/doi/10.1515/9783110862010.11/html" target="_blank" rel="noopener">(Andersen, 1989)&lt;/a>. In language, some forms of a word are the default form, while others are explicitly marked by some additional inflection. An example would be contrast between the word “marked”, which is an &lt;em>unmarked&lt;/em> form compared to “unmarked”, which is &lt;em>marked&lt;/em> by the prefix “un-”. In machine learning, we might call CV an unmarked domain by convention, in contrast to the &lt;em>marked&lt;/em> NLP. This convention means that certain tasks and architectures are considered the default environments to understand. Such a convention privileges understanding continuous data over discrete; ConvNets over LSTMs; ResNets over Transformers; geometric tasks over structured prediction.&lt;/p>
&lt;p>Understanding one machine learning domain will always extend analysis of others. For example, latent tree structure is inherent to both domains, but in CV, it is obscured by the image data from which we must compose eyes and mouth into a face—and subsequently, body and face into a cow &lt;a href="https://ieeexplore.ieee.org/document/6909858" target="_blank" rel="noopener">(Vedaldi et al., 2014)&lt;/a>. Image classification is also a language task, because it is our language that provides the intuitions which we use to construct ontologies that turn into image classes; English does not provide us with common distinctions for different packs of wolves, but it names every dog breed, and so the image labels are chosen according to available terminology.&lt;/p>
&lt;p>Many researchers think of text data as arcane, but the unmarked domain of CV displays many idiosyncrasies on which to overfit our understanding of statistical modeling. CV provides us with many interesting geometric phenomena, but the underlying structure of language &lt;em>without&lt;/em> the added noisy channel of an image can provide a clear and simple domain worth analyzing, as well. A true understanding of statistical models must be a multi-domain understanding, not a mono-domain view focused on one task and its peculiarities.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Searches were performed with Google Scholar.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>*CL venues have also become distanced from work in computational linguistics &lt;a href="https://www.aclweb.org/anthology/J07-2013.pdf" target="_blank" rel="noopener">(Reiter, 2007)&lt;/a>, leaving NLP as a field deprived of new scientific work in its data domain as well as new scientific work in its methodologies.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>A Non-Linear Structural Probe</title><link>https://nsaphra.net/publication/white-nonlinear-2020/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/white-nonlinear-2020/</guid><description/></item><item><title>LSTMs Compose---and Learn---Bottom-Up</title><link>https://nsaphra.net/publication/saphra-lstms-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-lstms-2020/</guid><description/></item><item><title>Pareto Probing: Trading Off Accuracy for Complexity</title><link>https://nsaphra.net/publication/pimentel-pareto-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/pimentel-pareto-2020/</guid><description/></item><item><title>Understanding Privacy-Related Questions on Stack Overflow</title><link>https://nsaphra.net/publication/tahaei/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/tahaei/</guid><description/></item><item><title>What Does a Coder Do If They Can't Type?</title><link>https://nsaphra.net/post/hands/</link><pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/hands/</guid><description>&lt;p>In August of 2015, my hands stopped working. I could still control them, but every movement accumulated more pain, so every motion came with a cost: getting dressed in the morning, sending a text, lifting a glass. I was interning at Google that summer about to begin a PhD in Scotland, but coding all day would have left me in agony. In relating this story, I often mention that for months before I learned to work without my hands, I had nothing to do but go to a bar and order a shot of vodka with a straw in it. This is a very funny joke.&lt;/p>
&lt;p>I have been in pain for four years.&lt;/p>
&lt;hr>
&lt;h2 id="talon">Talon&lt;/h2>
&lt;p>Due to this disability, I cannot type or write by hand. Many people have asked me about the stack that enables me to be productive in spite of this limitation. I hope this information is helpful both for people with more severe limitations, and for programmers with mild repetitive stress injuries who can benefit from reducing their keyboard use.&lt;/p>
&lt;p>The star of the show is &lt;a href="https://talonvoice.com/" target="_blank" rel="noopener">Talon&lt;/a>, a system which makes it easy to write customized grammars and scripts that work with speech recognition systems to enable programming. Commands range from simple aliases for common symbols to complex meta-commands which repeat a previous utterance or change dictation modes. For example, just in the case of parentheses, I have separate commands for &lt;code>(&lt;/code>, &lt;code>)&lt;/code>, &lt;code>()&lt;/code>, and &lt;code>()⬅️&lt;/code> (which leaves the cursor between parentheses so my next utterance is bracketed).&lt;/p>
&lt;p>Each Talon user has a number of personal scripts. The most precious script that I&amp;rsquo;ve written is probably my indexed clipboard:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">from&lt;/span> &lt;span class="nn">talon.voice&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">press&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Str&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Context&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">from&lt;/span> &lt;span class="nn">talon&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">clip&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kn">from&lt;/span> &lt;span class="nn">.talon_community.utils&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="o">*&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ctx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Context&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;clipboard&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">copy_selection&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">with&lt;/span> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">capture&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">sel&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">press&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;cmd-c&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">_words&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">key&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39; &amp;#39;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">parse_words&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sel&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">keymap&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;paste &lt;/span>&lt;span class="si">%s&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">value&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">keymap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">keymap&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reload&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">clip&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sel&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">keymap&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;paste&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Key&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;cmd-v&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;clip [&amp;lt;dgndictation&amp;gt;]&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">copy_selection&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ctx&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">keymap&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">keymap&lt;/span>&lt;span class="p">)&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;p>The use is simple. After selecting a particular phrase using my cursor control commands, I say &amp;ldquo;clip [foo]&amp;rdquo;, and every time I want to enter the same phrase after, I say &amp;ldquo;paste [foo]&amp;rdquo;. I therefore only have to dictate a particularly obnoxious variable name once. However, it does introduce a new challenge: every variable has two names, its written name and its spoken name. This unfortunate side effect exacerbates the difficulty of naming variables, which has been called &amp;ldquo;the hardest problem in computer science&amp;rdquo;.&lt;/p>
&lt;p>If you are a vim or Emacs power user, this may all feel familiar to you. I have commands for searching, moving a cursor, selection, and manipulating the clipboard. Learning to dictate code is a lot like learning a new text editor very thoroughly, down to the challenge of customizing for your particular languages and needs.&lt;/p>
&lt;p>The &lt;a href="https://github.com/dwiel/talon_community" target="_blank" rel="noopener">Talon community&lt;/a> has specialized commands that take effect depending on application or programming language. For a Perl user, for example, a good starting point might be to borrow settings from Emily Shea:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/Mz3JeYfBTcY?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;p>My Talon setup relies on Dragon for the speech recognition side. Unfortunately, Nuance has discontinued OSX Dragon editions that make scripting possible. The coder behind Talon, &lt;a href="http://ryanhileman.com/" target="_blank" rel="noopener">Ryan Hileman&lt;/a>, is working on a suitable replacement but at time of writing, it is not yet ready.&lt;/p>
&lt;hr>
&lt;h3 id="interlude">Interlude&lt;/h3>
&lt;p>People often ask for my diagnosis, but it officially depends on the country I&amp;rsquo;m in. After an initial assumption that carpal tunnel was to blame, a rheumatologist gave me my first American diagnosis: &lt;em>fibromyalgia&lt;/em>, a word which is Doctorspeak for &amp;ldquo;go away&amp;rdquo;.&lt;/p>
&lt;p>I did not go away. A neurologist performed a skin biopsy that led to my official American diagnosis of &amp;ldquo;idiopathic small fiber neuropathy&amp;rdquo;, meaning that I am missing crucial nerve fibers that transmit heat and pain but nobody knows why. &lt;em>Idiopathic&lt;/em> is also Doctorspeak for &amp;ldquo;go away&amp;rdquo;.&lt;/p>
&lt;p>I went away to the UK. I brought my medical records from America, but my British neurologist did not read my records or perform examinations. After a brief conversation, he gave me my British diagnosis by submitting a note that he had no evidence of any physical cause, and he &amp;ldquo;suspected significant functional overlay&amp;rdquo;, which is how they teach you to call someone delusional in medical school.&lt;/p>
&lt;p>My GP read the note and informed me: He would not prescribe me painkillers. He would not send me for a second opinion from a neurologist, or treatment from any other specialist. The only referral he would write would be to a psychologist to help me &amp;ldquo;resolve the underlying issues behind my pain&amp;rdquo;.&lt;/p>
&lt;p>He then kicked me out of his office for using the word &amp;ldquo;fucking&amp;rdquo;. &amp;ldquo;We do not tolerate cursing&amp;rdquo;, said a sign in the lobby.&lt;/p>
&lt;hr>
&lt;h2 id="equipment">Equipment&lt;/h2>
&lt;p>For dictating, I use two different microphones. In the office, I use a &lt;a href="https://en-uk.sennheiser.com/me-3-ii" target="_blank" rel="noopener">Sennheiser ME-3&lt;/a>, while for travel I use a Bluetooth headset, the &lt;a href="https://en-uk.sennheiser.com/mb-pro-1-uc-ml-and-mb-pro-2-uc-ml" target="_blank" rel="noopener">Sennheiser MB Pro 2&lt;/a>.&lt;/p>
&lt;p>Another essential piece of equipment for me is my foot pedal, a &lt;a href="https://www.pageflip.com/products/firefly" target="_blank" rel="noopener">PageFlip Firefly&lt;/a>. It is programmable, so I have modified the settings to include one that is useful for reading papers in &lt;a href="https://skim-app.sourceforge.io/" target="_blank" rel="noopener">Skim&lt;/a>, with the left pedal corresponding to a click and the right pedal corresponding to down arrow. I can use my feet to scroll, and to click for annotations. Another pedal setting I have added maps the pedals to click and shift+enter. This setting is useful for Jupyter notebooks and writing my research notes and mathematical scratch work in &lt;a href="https://happenapps.com/" target="_blank" rel="noopener">Quiver&lt;/a>.&lt;/p>
&lt;p>When my hands are unusually aggravated, I cannot nudge my mouse around anymore and I fall back on &lt;a href="https://shortcatapp.com/" target="_blank" rel="noopener">shortcat&lt;/a>, which allows me to press buttons by dictating keyboard strokes instead of using a mouse.&lt;/p>
&lt;p>My final essential piece of equipment is a pair of &lt;a href="https://www.futuro-usa.com/3M/en_US/futuro-us/products/~/FUTURO-Night-Wrist-Support/?N=4318&amp;#43;3294508029&amp;#43;3294529207&amp;amp;rt=rud" target="_blank" rel="noopener">large wrist braces&lt;/a>. The primary purpose of my braces is to discourage me from habitual hand use. I always wear them at conferences, because wearing them is easier than constantly repeating, &amp;ldquo;I cannot shake hands due to a disability&amp;rdquo;.&lt;/p>
&lt;hr>
&lt;h3 id="interlude-1">Interlude&lt;/h3>
&lt;p>I struggle with sleep. I dream that my thumbs fall off. I dream that every bone in my hands breaks. I dream that my arms break out in open bleeding sores. I wake up and the pain remains like an invisible nightmare.&lt;/p>
&lt;hr>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>Maybe ironically, the largest concern if you begin to dictate code is that you do not develop a repetitive stress injury in your vocal tract. Speaking quietly can actually cause more damage, hydration is important, and better posture will prevent damage in your voice as well as the rest of your body. I strongly recommend finding a vocal coach who teaches actors and singers how to protect their voices. It is important to take breaks, and you may find talking tiring outside of work.&lt;/p>
&lt;p>Speech recognition technology is not perfect, and the error rate is even higher if you have an unusual accent. Furthermore, it may force you to take time off from programming every time you develop a cold or sore throat. I live in fear of even minor colds.&lt;/p>
&lt;p>Having a private space to dictate in is essential. I was unable to be productive working from home, but as soon as I had a private office I developed momentum on several research projects. I know that this is a huge limitation for a lot of people because of the productivity-destroying, soul-sucking trend towards open offices for all programming work. If your workplace has fallen prey to this trend, you may still have options. In many countries, large companies will be obligated to provide a space to work in if you are disabled.&lt;/p>
&lt;hr>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Life with my disability is not easy, but thanks to &lt;a href="https://en.wikipedia.org/wiki/Hedonic_treadmill" target="_blank" rel="noopener">hedonic adaptation&lt;/a> as well as satisfying &lt;a href="http://nsaphra.github.io/publication/" target="_blank" rel="noopener">work&lt;/a> and &lt;a href="https://auldreekierollerderby.com/2019/08/10/the-one-gift-i-received-along-with-my-disability/" target="_blank" rel="noopener">hobbies&lt;/a>, I am actually very happy. If you have recently developed a disability or chronic pain condition, it may feel like you could never adjust to the lifestyle required. That is why I have tried to give you a lens into my challenges as well as my successes. It is easy to respond to anyone who has overcome adversity with one of two reactions: &amp;ldquo;It can&amp;rsquo;t be that hard,&amp;rdquo; or &amp;ldquo;I could never do that&amp;rdquo;. Move past both reactions. It is that hard. You can do it.&lt;/p>
&lt;p>If you are currently able-bodied, please support your disabled colleagues, coworkers, and anyone you have power over in their quest to do valuable and fulfilling work. I encourage other disabled scientists and programmers to reach out to me with any questions they have.&lt;/p>
&lt;hr>
&lt;p>&lt;em>Thank you for comments on early drafts: &lt;a href="http://www.cs.jhu.edu/~vandurme/Carrell.html" target="_blank" rel="noopener">Annabelle Carrell&lt;/a>, &lt;a href="http://www.craiginnes.com/" target="_blank" rel="noopener">Craig Innes&lt;/a>, &lt;a href="https://twitter.com/uscm_" target="_blank" rel="noopener">Matthew Summers&lt;/a>, &lt;a href="https://www.dinalevitan.com/" target="_blank" rel="noopener">Dina Lev&lt;/a>, &lt;a href="https://www.ims.uni-stuttgart.de/institut/mitarbeiter/schlecdk/index.en.html" target="_blank" rel="noopener">Dominik Schlechtweg&lt;/a>, and &lt;a href="https://americanstudies.yale.edu/people/yuhe-faye-wang" target="_blank" rel="noopener">Yuhe Faye Wang&lt;/a> (who is in The Humanities!). Thank you to &lt;a href="https://www.recurse.com/" target="_blank" rel="noopener">The Recurse Center&lt;/a> for providing a private space for me to learn to dictate code. Thank you to my PhD advisor, &lt;a href="https://alopez.github.io/" target="_blank" rel="noopener">Adam Lopez&lt;/a>, who has unfailingly supported me and made all of this possible.&lt;/em>&lt;/p></description></item><item><title>Carbon AI and the Concentration of Computational Work</title><link>https://nsaphra.net/publication/kate/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/kate/</guid><description/></item><item><title>Sparsity Emerges Naturally in Neural Language Models</title><link>https://nsaphra.net/publication/saphra-sparsity/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-sparsity/</guid><description/></item><item><title>Understanding Learning Dynamics Of Language Models with SVCCA</title><link>https://nsaphra.net/publication/saphra-understand-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/saphra-understand-2019/</guid><description/></item><item><title>Model Scheduling</title><link>https://nsaphra.net/post/model-scheduling/</link><pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/model-scheduling/</guid><description>&lt;p>Models can be built incrementally by modifying their hyperparameters
during training. This is most common in transfer learning settings, in
which we seek to adapt the knowledge in an existing model for a new
domain or task. The more general problem of continuous learning is also
an obvious application. Even with a predefined data set, however,
incrementally constraining the topology of the network can offer
benefits as regularization.&lt;/p>
&lt;h2 id="dynamic-hyperparameters">Dynamic Hyperparameters&lt;/h2>
&lt;p>The easiest incrementally modified models to train may be those in which
hyperparameters are updated at each epoch. In this case, we do not mean
those hyperparameters associated with network topology, such as the
number or dimension of layers. There are many opportunities to adjust
the topology during training, but the model often requires heavy
retraining in order to impose reasonable structure again, as demonstrated clearly in the case of memory networks&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. If we
instead focus on the weights associated with regularizers and gates, we
can gradually learn structure without frequent retraining to accommodate
radically altered topologies.&lt;/p>
&lt;h3 id="curriculum-dropout">Curriculum Dropout&lt;/h3>
&lt;p>Hinton et al.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> describes dropout as reducing overfitting by preventing
co-adaptation of feature detectors which happened to perfectly fit the
data. In this interpretation, co-adaptive clusters of neurons are
concurrently activated. Randomly suppressing these neurons forces them
to develop independence.&lt;/p>
&lt;p>In standard dropout, these co-adaptive neurons are treated as equally
problematic at all stages of training. However, Morerio et. al.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> posit that early
in training, co-adaptation may represent the beginnings of an optimal
self organization of the network. In this view, these structures mainly
pose the threat of overfitting later in training. The authors therefore
introduce a hyperparameter schedule for the dropout ratio, increasing
the rate of dropout as training continues. To the best of my knowledge,
this is the only proposal of adaptive regularization published.&lt;/p>
&lt;h3 id="mollifying-networks">Mollifying Networks&lt;/h3>
&lt;p>Mollifying networks&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> are, to my knowledge, the only existing
attempt to combine techniques focused on incrementally manipulating the
distribution of data with techniques focused on incrementally
manipulating the representational capacity of the model. Mollifying
networks incrementally lower the temperature of the data through
simulated annealing while simultaneously modifying various
hyperparameters to permit longer-range dependencies. In the case of an
LSTM, they set the output gate to 1, input gate to \(\frac{1}{t}\), and
forget gate to \(1 - \frac{1}{t}\), where \(t\) is the annealing time step.
Using this system, the LSTM initially behaves as a bag-of-words model,
gradually adding the capacity to handle more context at each time step.&lt;/p>
&lt;p>Mollifying networks use a different data schedule for each layer,
annealing the noise in lower layers faster than in higher layers because
lower-level representations are assumed to learn faster.&lt;/p>
&lt;h2 id="adaptive-architectures">Adaptive Architectures&lt;/h2>
&lt;p>The hyperparameters most difficult to modify during training may be
those which dictate the topology of the model architecture itself.
Nonetheless, the deep learning literature contains a long history of
techniques which adapt the model architecture during training, often in
response to the parameters being learned. Methods like these can help
search optimally by smoothing functions at the beginning of training,
speed up learning by starting with a simpler model, or compress a model
to fit easily on a phone or embedded device. Most of these methods could
be classified as either growing a model by adding parameters
mid-training or shrinking a model by pruning edges or nodes.&lt;/p>
&lt;h3 id="architecture-growth">Architecture Growth&lt;/h3>
&lt;p>Some recent transfer learning strategies have relied on growing
architectures by creating entire new modules focused on the new task
with connections to the existing network&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. If the goal is to
instead augment an existing network by adding a small number of
parameters, the problem bears a resemblance to traditional nonparametric
learning, because we need not explicitly limit the model space to begin
with.&lt;/p>
&lt;p>Classical techniques in neural networks such as Cascade Correlation
Networks&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup> and Dynamic Node Creation&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> added new nodes at random
one by one and trained them individually. On modern large-scale
architectures and problems, this is intractable. Furthermore, the main
advantage of such methods is that they approach a minimal model, which
is an aim that modern deep learning practitioners no longer consider
valuable thanks to leaps in computing power in the decades since. Modern
techniques for incrementally growing networks must make 2 decisions: 1)
When (and where) do we add new parameters? 2) How do we train new
parameters?&lt;/p>
&lt;p>Warde-Farley et. al.&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> add parameters in bulk after training an entire network. The
augmentation takes the form of specialized auxiliary layers added to the
existing network in parallel. These layers are trained on class
boundaries that the original generalist model struggles with. The class
boundaries that require special attention are selected by performing
spectral clustering on the confusion matrix of a holdout data set,
partitioning the classes into challenging subproblems.&lt;/p>
&lt;p>The auxiliary layers are initialized randomly in parallel with the
original generalist system, and then are each trained only on examples
from their assigned partition of the classes. The original generalist
network is held fixed, other than fine-tuning the final classification
layer. The resulting network is a mixture of experts, which was shown to
improve results on an image classification problem.&lt;/p>
&lt;p>Neurogenesis Deep Learning (NDL)&lt;sup id="fnref:10">&lt;a href="#fn:10" class="footnote-ref" role="doc-noteref">10&lt;/a>&lt;/sup>, meanwhile, makes autoencoders
capable of lifelong learning. This strategy updates the topology of an
autoencoder by adding neurons when the model encounters outliers that it
performs especially poorly on. These new parameters are trained
exclusively on those outliers, allowing the existing decoder parameters
to update with much smaller step sizes. Existing encoder parameters
update only if they are connected directly to the new neuron.&lt;/p>
&lt;p>After introducing and training these new neurons, NDL stabilizes the
existing structure of the network using a method the authors call
&amp;ldquo;intrinsic replay&amp;rdquo;. They reconstruct approximations of previously seen
samples and train on these reconstructions.&lt;/p>
&lt;p>Another system that permits lifelong learning is the infinite Restricted
Boltzmann Machine (RBM) &lt;sup id="fnref:11">&lt;a href="#fn:11" class="footnote-ref" role="doc-noteref">11&lt;/a>&lt;/sup>. This extension of the classic RBM
parameterizes hidden units by unique indices, expressing an ordering.
These indices are used to enforce an order on the growth of the network
by favoring older nodes until they have converged, permitting the system
to grow arbitrarily large. An intriguing approach, but it is not obvious
how to apply similar modifications to networks other than the
idiosyncratic generative architecture of the RBM.&lt;/p>
&lt;p>None of these augmentation techniques support recurrent architectures.
In modern natural language processing settings, this is a fatal
limitation. However, it is possible that some of these techniques may be
adapted for RNNs, especially since training specialized subsystems has
been recently tackled in these environments &lt;sup id="fnref:12">&lt;a href="#fn:12" class="footnote-ref" role="doc-noteref">12&lt;/a>&lt;/sup>.&lt;/p>
&lt;h3 id="architecture-pruning">Architecture Pruning&lt;/h3>
&lt;p>Much recent research has focused on the possibility of pruning edges or
entire neurons from trained networks. This approach is promising not
only for the purpose of compression, but potentially as a way of
increasing the generalizability of a network.&lt;/p>
&lt;h4 id="pruning-edges">Pruning Edges&lt;/h4>
&lt;p>Procedures that prune edges rather than entire neurons may not reduce
the dimensional type of the network. However, they will make the network
sparser, leading to possible memory savings. A sparser network also
occupies a smaller parameter space, and may therefore still more
general.&lt;/p>
&lt;p>Han et. al.&lt;sup id="fnref:13">&lt;a href="#fn:13" class="footnote-ref" role="doc-noteref">13&lt;/a>&lt;/sup> takes the basic approach of setting weights to 0 if they fall
below a certain threshold. This approach is highly effective for
compression, because the number of weights to be pruned can be easily
modified through the threshold.&lt;/p>
&lt;p>LeCun et. al.&lt;sup id="fnref:14">&lt;a href="#fn:14" class="footnote-ref" role="doc-noteref">14&lt;/a>&lt;/sup> and Hassibi et. al.&lt;sup id="fnref:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup> both select weights to prune based on Taylor series
approximation of the change in error resulting from trimming. While
these methods were successful for older shallow networks, performing
these operations on an entire network requires a Hessian matrix to be
computed over all parameters, which is generally intractable for deep
modern architectures. Dong et. al.&lt;sup id="fnref:16">&lt;a href="#fn:16" class="footnote-ref" role="doc-noteref">16&lt;/a>&lt;/sup> presents a more efficient alternative by
performing optimal brain surgery over individual layers instead.&lt;/p>
&lt;h4 id="pruning-nodes">Pruning Nodes&lt;/h4>
&lt;p>Pruning entire nodes has the advantage of reducing the entire
dimensionality of the network. It also may be faster than choosing
individual edges to prune, because having more nodes than constituent
edges reduces the number of candidates to consider for pruning.&lt;/p>
&lt;p>He et al.&lt;sup id="fnref:17">&lt;a href="#fn:17" class="footnote-ref" role="doc-noteref">17&lt;/a>&lt;/sup> selects which neuron \(w_i^\ell\) to prune from layer \(\ell\)
with width \(d_{\ell}\) by calculating the importance of each node. They
test several importance metrics, finding that the highest performance
results from using the &amp;lsquo;onorm&amp;rsquo;, or average \(l_1\) norm of the activation
pattern of the node:&lt;/p>
&lt;p>\(\mathrm{onorm}(w_i^\ell) = \frac{1}{d_{\ell+1}} \sum_{j = 1}^{d_{\ell+1}} |w_{ij}^{\ell+1}|\)&lt;/p>
&lt;p>Net-trim &lt;sup id="fnref:18">&lt;a href="#fn:18" class="footnote-ref" role="doc-noteref">18&lt;/a>&lt;/sup> likewise relies on the \(l_1\) norm to induce sparsity.&lt;/p>
&lt;p>Wolfe et al.&lt;sup id="fnref:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup> compares the results of importance based pruning to a brute force
method that will greedily select a node to be sacrificed based on its
impact on performance. In the brute force method, they rerun the network
on the test data without each node and sort the nodes according to the
error of the resulting network. Their importance metrics are based on
neuron-level versions of the Taylor series approximations of that impact&lt;sup id="fnref1:15">&lt;a href="#fn:15" class="footnote-ref" role="doc-noteref">15&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>In the first algorithm tested, they rank all nodes according to their
importance and then remove each node in succession. In the second
algorithm, they re-rank the nodes after each removal, in order to
account for the effects of subnetworks that generate and then cancel. In
the second case, they find that it is possible to prune up to 60% of
nodes in a network trained on mnist without significant loss in
performance. This supports an early observation&lt;sup id="fnref:20">&lt;a href="#fn:20" class="footnote-ref" role="doc-noteref">20&lt;/a>&lt;/sup> that the majority
of parameters in a network are unnecessary, and their effect is limited
to generating and then canceling their own noise. The strength of this
effect supports the idea that backpropagation implicitly trains a
minimal network for the task given.&lt;/p>
&lt;p>Srinivas and Babu&lt;sup id="fnref:21">&lt;a href="#fn:21" class="footnote-ref" role="doc-noteref">21&lt;/a>&lt;/sup> prune with the goal of reducing the redundancy of the network, so
they select nodes to remove based on the similarity of their weights to
other neurons in the same layer. Diversity networks&lt;sup id="fnref:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup>, meanwhile,
choose based on the diversity of their activation patterns. In order to
sample a diverse selection of nodes, they use a Determinantal Point
Process. This technique minimizes the dependency between nodes sampled.
They followed this pruning process by &lt;a href="#merging-nodes">fusing&lt;/a> the nodes pruned back into
the network.&lt;/p>
&lt;p>An intriguing difference emerges between the observations in these
papers. While Mariet and Sra&lt;sup id="fnref1:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup> find that in deeper layers they sample more nodes
from the DPP, Philipp and Carbonell &lt;sup id="fnref1:19">&lt;a href="#fn:19" class="footnote-ref" role="doc-noteref">19&lt;/a>&lt;/sup> prune more nodes by brute force in the deeper
layer of a 2-layer network. In other words, diversity networks retain
more nodes at deeper layers while greedy brute force approaches remove
more from the same layers. These results point to fundamental
differences between the respective outcomes of these algorithms and
warrant further investigation.&lt;/p>
&lt;h5 id="merging-nodes">Merging Nodes&lt;/h5>
&lt;p>Mariet and Sra&lt;sup id="fnref2:22">&lt;a href="#fn:22" class="footnote-ref" role="doc-noteref">22&lt;/a>&lt;/sup> found that performance increased after their DPP-based pruning if
they then merged the pruned nodes back into the network. They achieved
this by re-weighting the remaining nodes in the pruned layer to minimize
the difference in activation outputs before and after pruning:&lt;/p>
&lt;p>\( \min_{\tilde{w}_{ij} \in \mathbb{R}} | \sum_{i=1}^k \tilde{w}_{ij} v_i - \sum_{i=1}^{d_{\ell}} w_{ij} v_i |_2 \)&lt;/p>
&lt;p>Because the DPP is focused on selecting an independent set of neurons,
it seems likely that pruning will select at least 1 node within any
given noise cancellation system to keep, since those cancellation
subnetworks are by necessity highly dependent. The merging step in that
case would merge the noise canceling components back into the noise
generating nodes or vice versa. This would make merging a particular
necessity in diversity networks, but it may still present a tractable
alternative to retraining after a different pruning algorithm.&lt;/p>
&lt;h3 id="nonparametric-neural-networks">Nonparametric Neural Networks&lt;/h3>
&lt;p>The pruning and growing strategies are combined in only one work, to my
knowledge. Nonparametric Neural Networks (NNNs)&lt;sup id="fnref:23">&lt;a href="#fn:23" class="footnote-ref" role="doc-noteref">23&lt;/a>&lt;/sup> combine adding
neurons with imposing a sparsity-inducing penalty over neurons. For a
feedforward network with \(N^L\) layers, authors introduce 2 such
regularizers, a &amp;ldquo;fan-in&amp;rdquo; and a &amp;ldquo;fan-out&amp;rdquo; variant:&lt;/p>
&lt;p>\(
\Omega_{\mathrm{in}} = \sum_{\ell = 1}^{N^L} \sum_{j = 1}^{d_\ell} \left( \sum_{i = 1}^{d_{\ell}} |w_{ij}^{\ell+1}|^p \right)^{\frac{1}{p}}\)&lt;/p>
&lt;p>\(\Omega_{\mathrm{out}} = \sum_{\ell = 1}^{N^L} \sum_{i = 1}^{d_{\ell}} \left( \sum_{j = 1}^{d_\ell+1} |w_{ij}^{\ell}|^p \right)^{\frac{1}{p}}\)&lt;/p>
&lt;p>In other words, the fan-in variant penalizes the \(p\)-norm of the inputs
to each neuron, while the fan-out of variant penalizes the \(p\)-norm of
the outputs from each neuron. In the case of feedforward networks,
either of these regularizers can be added to the loss function with any
positive weight \(\lambda\) and \(0 &amp;lt; p &amp;lt; \infty\) to guarantee that the
objective will converge at some finite number of neurons.&lt;/p>
&lt;p>NNNs offer a combination of beneficial strategies for adapting the
network. In particular with \(p = 1\) or 2, induces sparsity by applying
pressure to form &lt;em>zero-valued neurons&lt;/em>, or neurons which have either a
fan-in or fan-out value of 0. At intervals we can remove these
zero-valued neurons which result. At the same time, we can introduce new
zero-valued neurons at different locations in the network, and the
regularizer guarantees the objective will converge, so we can stop
adding neurons at any point that performance begins to decline.&lt;/p>
&lt;p>However, there are clear issues with this approach. The first obvious
limitation is that this regularizer cannot be applied in any network
with recurrences. This constraint reduces the strategy&amp;rsquo;s usefulness in
many natural language domains where state-of-the-art performance
requires a RNN.&lt;/p>
&lt;p>Another disadvantage to this method is the choice to insert zero-valued
neurons by initializing either the input or output weight vector as 0
and randomly initializing the other associated vector. We therefore
retrain the entire network with each interval, rather than intelligently
initializing and training the new node to accelerate convergence. While
this approach may converge to an optimal number of nodes, it does
nothing to accelerate training or help new nodes specialize.&lt;/p>
&lt;p>Finally, this approach adds and removes entire neurons to create a final
dense network. It therefore forfeits the potential regularization
advantages of the sparser networks which result from instead pruning
weights.&lt;/p>
&lt;h2 id="teacherstudent-approaches">Teacher/Student Approaches&lt;/h2>
&lt;p>It is also possible to produce a larger or smaller model based on an
existing network by fresh training. When investigating any adaptive
architecture, it is crucial to compare with a baseline which uses the
previous state of the network as a teacher to a student network which
has the new architecture.&lt;/p>
&lt;p>The approach of teacher/student learning, in which the teacher network&amp;rsquo;s
outputs layer are used in lieu of or in addition to true labels, was
introduced in the particular case of distillation learning &lt;sup id="fnref:24">&lt;a href="#fn:24" class="footnote-ref" role="doc-noteref">24&lt;/a>&lt;/sup>.
Distillation is a technique for compressing a large ensemble or
generally expensive classifier with high performance. A smaller network
is trained using an objective that combines a loss function applied to true labels with cross-entropy against the logit layer of the large teacher network. In addition to compression, teacher/student learning is effective for domain adaptation technique &lt;sup id="fnref:25">&lt;a href="#fn:25" class="footnote-ref" role="doc-noteref">25&lt;/a>&lt;/sup>, suggesting it may be useful for adapting to a new time step in a data schedule.&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Sachan, Mrinmaya, and Eric Xing. &amp;ldquo;Easy questions first? a case study on curriculum learning for question answering.&amp;rdquo; &lt;em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics&lt;/em> (Volume 1: Long Papers). Vol. 1. 2016.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. &lt;em>CoRR&lt;/em> abs/1207.0580.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal, and Vittorio Murino. 2017. Curriculum Dropout. &lt;em>arXiv:1703.06229 [^cs, stat]:&lt;/em>. Retrieved March 22, 2017 from &lt;a href="http://arxiv.org/abs/1703.06229" target="_blank" rel="noopener">http://arxiv.org/abs/1703.06229&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio. 2016. Mollifying Networks. &lt;em>arXiv:1608.04980 [^cs]:&lt;/em>. Retrieved October 7, 2016 from &lt;a href="http://arxiv.org/abs/1608.04980" target="_blank" rel="noopener">http://arxiv.org/abs/1608.04980&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In &lt;em>arXiv:1611.01587 [^cs]:&lt;/em>. Retrieved November 11, 2016 from &lt;a href="http://arxiv.org/abs/1611.01587" target="_blank" rel="noopener">http://arxiv.org/abs/1611.01587&lt;/a>&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Progressive Neural Networks. &lt;em>arXiv:1606.04671 [^cs]:&lt;/em>. Retrieved September 14, 2016 from &lt;a href="http://arxiv.org/abs/1606.04671" target="_blank" rel="noopener">http://arxiv.org/abs/1606.04671&lt;/a>&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>Scott E. Fahlman and Christian Lebiere. 1989. The cascade-correlation learning architecture. Retrieved November 30, 2016 from &lt;a href="http://repository.cmu.edu/compsci/1938/" target="_blank" rel="noopener">http://repository.cmu.edu/compsci/1938/&lt;/a>&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>Ash. 1989. Dynamic node creation in backpropagation networks. In &lt;em>International 1989 Joint Conference on Neural Networks&lt;/em>, 623 vol.2. &lt;a href="https://doi.org/10.1109/IJCNN.1989.118509" target="_blank" rel="noopener">https://doi.org/10.1109/IJCNN.1989.118509&lt;/a>&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>David Warde-Farley, Andrew Rabinovich, and Dragomir Anguelov. 2014. Self-informed neural network structure learning. &lt;em>arXiv preprint arXiv:1412.6563&lt;/em>. Retrieved June 9, 2016 from &lt;a href="http://arxiv.org/abs/1412.6563" target="_blank" rel="noopener">http://arxiv.org/abs/1412.6563&lt;/a>&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:10">
&lt;p>Timothy J. Draelos, Nadine E. Miner, Christopher C. Lamb, Craig M. Vineyard, Kristofor D. Carlson, Conrad D. James, and James B. Aimone. 2016. Neurogenesis Deep Learning. &lt;em>arXiv:1612.03770 [^cs, stat]:&lt;/em>. Retrieved February 27, 2017 from &lt;a href="http://arxiv.org/abs/1612.03770" target="_blank" rel="noopener">http://arxiv.org/abs/1612.03770&lt;/a>&amp;#160;&lt;a href="#fnref:10" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:11">
&lt;p>Marc-Alexandre Cote and Hugo Larochelle. 2015. An Infinite Restricted Boltzmann Machine. &lt;em>arXiv:1502.02476 [^cs]:&lt;/em>. Retrieved February 8, 2018 from &lt;a href="http://arxiv.org/abs/1502.02476" target="_blank" rel="noopener">http://arxiv.org/abs/1502.02476&lt;/a>&amp;#160;&lt;a href="#fnref:11" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:12">
&lt;p>Shlomo E. Chazan, Jacob Goldberger, and Sharon Gannot. 2017. Deep recurrent mixture of experts for speech enhancement. &lt;em>2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)&lt;/em>: 359&amp;ndash;363.&amp;#160;&lt;a href="#fnref:12" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:13">
&lt;p>Song Han, Jeff Pool, John Tran, and William J. Dally. 2015. Learning both Weights and Connections for Efficient Neural Networks. &lt;em>arXiv:1506.02626 [^cs]:&lt;/em>. Retrieved May 26, 2016 from &lt;a href="http://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">http://arxiv.org/abs/1506.02626&lt;/a>&amp;#160;&lt;a href="#fnref:13" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:14">
&lt;p>Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In &lt;em>Advances in neural information processing systems&lt;/em>, 598&amp;ndash;605.&amp;#160;&lt;a href="#fnref:14" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:15">
&lt;p>Babak Hassibi, David G. Stork, and Gregory J. Wolff. 1993. Optimal brain surgeon and general network pruning. In &lt;em>Neural Networks, 1993., IEEE International Conference on&lt;/em>, 293&amp;ndash;299.&amp;#160;&lt;a href="#fnref:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:15" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:16">
&lt;p>Xin Dong, Shangyu Chen, and Sinno Jialin Pan. 2017. Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon. &lt;em>arXiv:1705.07565 [^cs]:&lt;/em>. Retrieved from &lt;a href="http://arxiv.org/abs/1705.07565" target="_blank" rel="noopener">http://arxiv.org/abs/1705.07565&lt;/a>&amp;#160;&lt;a href="#fnref:16" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:17">
&lt;p>Tianxing He, Yuchen Fan, Yanmin Qian, Tian Tan, and Kai Yu. 2014. Reshaping deep neural network for fast decoding by node-pruning. &lt;em>2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/em>: 245&amp;ndash;249.&amp;#160;&lt;a href="#fnref:17" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:18">
&lt;p>Aghasi, Alireza, et al. &amp;ldquo;Net-trim: Convex pruning of deep neural networks with performance guarantee.&amp;rdquo; &lt;em>Advances in Neural Information Processing Systems&lt;/em>. 2017. &lt;a href="http://arxiv.org/abs/1611.05162" target="_blank" rel="noopener">http://arxiv.org/abs/1611.05162&lt;/a>&amp;#160;&lt;a href="#fnref:18" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:19">
&lt;p>Nikolas Wolfe, Aditya Sharma, Lukas Drude, and Bhiksha Raj. 2017. &amp;ldquo;The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning.&amp;rdquo;&amp;#160;&lt;a href="#fnref:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:19" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:20">
&lt;p>Michael C. Mozer and Paul Smolensky. 1989. Using Relevance to Reduce Network Size Automatically.&amp;#160;&lt;a href="#fnref:20" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:21">
&lt;p>Suraj Srinivas and R. Venkatesh Babu. 2015. Data-free parameter pruning for deep neural networks. &lt;em>arXiv preprint arXiv:1507.06149&lt;/em>. Retrieved October 5, 2016 from &lt;a href="http://arxiv.org/abs/1507.06149" target="_blank" rel="noopener">http://arxiv.org/abs/1507.06149&lt;/a>&amp;#160;&lt;a href="#fnref:21" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:22">
&lt;p>Zelda Mariet and Suvrit Sra. 2015. Diversity Networks: Neural Network Compression Using Determinantal Point Processes. &lt;em>arXiv:1511.05077 [^cs]:&lt;/em>. Retrieved February 9, 2018 from &lt;a href="http://arxiv.org/abs/1511.05077" target="_blank" rel="noopener">http://arxiv.org/abs/1511.05077&lt;/a>&amp;#160;&lt;a href="#fnref:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref2:22" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:23">
&lt;p>George Philipp and Jaime G. Carbonell. 2017. Nonparametric Neural Networks. In &lt;em>arXiv:1712.05440 [^cs]:&lt;/em>. Retrieved February 18, 2018 from &lt;a href="http://arxiv.org/abs/1712.05440" target="_blank" rel="noopener">http://arxiv.org/abs/1712.05440&lt;/a>&amp;#160;&lt;a href="#fnref:23" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:24">
&lt;p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. &lt;em>arXiv:1503.02531 [^cs, stat]:&lt;/em>. Retrieved September 22, 2016 from &lt;a href="http://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">http://arxiv.org/abs/1503.02531&lt;/a>&amp;#160;&lt;a href="#fnref:24" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:25">
&lt;p>Jinyu Li, Michael L. Seltzer, Xi Wang, Rui Zhao, and Yifan Gong. 2017. Large-Scale Domain Adaptation via Teacher-Student Learning. &lt;em>arXiv:1708.05466 [^cs]:&lt;/em>. Retrieved August 26, 2017 from &lt;a href="http://arxiv.org/abs/1708.05466" target="_blank" rel="noopener">http://arxiv.org/abs/1708.05466&lt;/a>&amp;#160;&lt;a href="#fnref:25" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>DyNet: The Dynamic Neural Network Toolkit</title><link>https://nsaphra.net/publication/dynet/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/dynet/</guid><description/></item><item><title>Evaluating Informal-Domain Word Representations with UrbanDictionary</title><link>https://nsaphra.net/publication/pos-first/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/pos-first/</guid><description/></item><item><title>AMRICA: an AMR Inspector for Cross-language Alignments</title><link>https://nsaphra.net/publication/naomi-saphra-amrica-2015/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/naomi-saphra-amrica-2015/</guid><description/></item><item><title>A framework for (under) specifying dependency syntax without overloading annotators</title><link>https://nsaphra.net/publication/schneider-framework-2014/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/schneider-framework-2014/</guid><description/></item><item><title>An Algerian Arabic-French Code-Switched Corpus</title><link>https://nsaphra.net/publication/cotterell-algerian-2014/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/cotterell-algerian-2014/</guid><description/></item><item><title>Understanding Objects in Detail with Fine-grained Attributes</title><link>https://nsaphra.net/publication/vedaldi-understanding-2014/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://nsaphra.net/publication/vedaldi-understanding-2014/</guid><description/></item><item><title>Understanding Latent Dirichlet Allocation</title><link>https://nsaphra.net/post/lda/</link><pubDate>Mon, 09 Jul 2012 00:00:00 +0000</pubDate><guid>https://nsaphra.net/post/lda/</guid><description>&lt;p>&lt;em>[Note - This is a repost of a post I made on my old blog while I was in undergrad. I&amp;rsquo;m including it in case someone finds it useful, since my old blog is defunct. I haven&amp;rsquo;t significantly edited it, so I&amp;rsquo;m sorry if it doesn&amp;rsquo;t fit into my current style.]&lt;/em>&lt;/p>
&lt;p>This post is directed to a lay CS audience. I am an undergraduate in CS, so I consider myself part of that audience. If you&amp;rsquo;re awesome at machine learning already and don&amp;rsquo;t want to help me along here, just read the paper.&lt;/p>
&lt;p>Latent Dirichlet Allocation (LDA) is a common method of topic modeling. That is, if I have a document and want to figure out if it&amp;rsquo;s a sports article or a mathematics paper, I can use LDA to build a system that looks at other sports articles or mathematics papers and automatically decides whether this unseen document&amp;rsquo;s topic is sports or math.&lt;/p>
&lt;p>To LDA, a document is just a collection of topics where each topic has some particular probability of generating a particular word. For our potential sports article, the word &amp;ldquo;average&amp;rdquo; appears 4 times. What&amp;rsquo;s the probability of a sports topic generating that many instances of &amp;ldquo;average&amp;rdquo;? We determine this by looking at each training document as a &amp;ldquo;bag of words&amp;rdquo; pulled from a distribution selected by a Dirichlet process.&lt;/p>
&lt;p>Dirichlet is a distribution specified by a vector parameter $$\alpha$$ containing some \(\alpha_i\) corresponding to each topic \(i\), which we write as \(\textrm{Dir}(\alpha)\). The formula for computing the probability density function for each topic vector \(x\) is proportional to the product over all topics \(i\) of \(x_i \alpha_i\). \(x_i\) is the probability that the topic is \(i\), so the items in \(x\) must sum to 1. That keeps you from getting arbitrarily large probabilities by giving arbitrarily large values of \(x\).&lt;/p>
&lt;p>Confused? Ready for a picture ripped off Wikipedia?&lt;/p>
&lt;p>&lt;a href="http://commons.wikimedia.org/wiki/File:Dirichlet_distributions.png" target="_blank" rel="noopener">
&lt;figure >
&lt;div class="flex justify-center ">
&lt;div class="w-100" >&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dirichlet_distributions.png/695px-Dirichlet_distributions.png" alt="Dirichlet Distributions" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/a>&lt;/p>
&lt;p>Those graphs all show Dirichlet distributions for three topics. That triangle at the bottom has one side for each topic, and the closer a point on the triangle is to side \(i\) the higher the probability of topic \(i\). The purple curve is the probability density function over the mixture of topics. See how the edges of the triangle all have probability 0? We said that the pdf is proportional to \(x_i \alpha_i\), so if \(x_i\) is 0, the probability of that mixture of topics is 0. That restricts our model a bit and ensures that we never are totally certain about the topic of a document.&lt;/p>
&lt;p>Okay, we&amp;rsquo;ve got &amp;ldquo;Dirichlet&amp;rdquo;, so let&amp;rsquo;s pop back up to the concept of LDA. If we want to find that mixture of topics for a document, we first need to determine the value of each \(x_i\). That means we&amp;rsquo;ve got another Dirichlet distribution in our model for each topic i where instead of the sides of the triangle being topics, they&amp;rsquo;re words. Picture the topic &amp;ldquo;sports article&amp;rdquo; like those distributions above, but instead of sitting on triangles they&amp;rsquo;re on shapes with so many sides the shapes go into as many dimensions as we have topics.. If &amp;ldquo;average&amp;rdquo; appears in a sports article, the bump pushes closer to the side for &amp;ldquo;average&amp;rdquo;.&lt;/p>
&lt;p>The Latent part of LDA comes into play because in statistics, a variable we have to infer rather than directly observing is called a &amp;ldquo;latent variable&amp;rdquo;. We&amp;rsquo;re only directly observing the words and not the topics, so the topics themselves are latent variables (along with the distributions themselves).&lt;/p>
&lt;p>LDA assumes that each document k is generated by:&lt;/p>
&lt;ol>
&lt;li>From our Dirichlet distribution for k, sample a random distribution of topics. That is, pick a place on that triangle that is associated with a certain probability of generating each topic. If we choose a place very close to the &amp;ldquo;sports article&amp;rdquo; edge, we have a higher probability of picking &amp;ldquo;sports article&amp;rdquo;. The probability of picking a particular place on the triangle is described by the pdf of the Dirichlet distribution (the placement of the purple mound).&lt;/li>
&lt;li>For each topic, pick a distribution of words for that topic from the Dirichlet for that topic.&lt;/li>
&lt;li>For each word in document \(k\),
&lt;ol>
&lt;li>From the distribution of topics selected for \(k\), sample a topic, like &amp;ldquo;sports article&amp;rdquo;.&lt;/li>
&lt;li>From the distribution selected for &amp;ldquo;sports article&amp;rdquo;, pick the current word.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>So let&amp;rsquo;s say your first four words all come from baseball and your document maybe starts off &amp;ldquo;average the bat bat&amp;rdquo;. If that&amp;rsquo;s not how you tend to write, that&amp;rsquo;s okay. All models are wrong.&lt;/p>
&lt;p>The important thing to understand is that your Dirichlet priors are distribution of distributions, which are selected to generate each word.&lt;/p>
&lt;p>We&amp;rsquo;re generally not just making these distributions for the heck of it or to actually generate documents. We want to figure out what topic was probably used for each word by our lazy writer who randomly generates each word. Maybe it&amp;rsquo;s been a while since you took probability, but do you remember this guy?
\[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]&lt;/p>
&lt;p>This is Bayes&amp;rsquo; Theorem. We already know the probability of generating a particular word given a topic according to our model. That&amp;rsquo;s the probability of sampling that word from the topic&amp;rsquo;s word distribution. So that&amp;rsquo;s \( P(B|A) \) where \( B \) is the event of &amp;ldquo;average&amp;rdquo; being generated for the current word and A is the event of picking the topic &amp;ldquo;sports article&amp;rdquo;. \(P(A)\) is the probability of &amp;ldquo;sports article&amp;rdquo; being picked from the document&amp;rsquo;s topic distribution. \( P(B) \) is the probability of &amp;ldquo;average&amp;rdquo; being generated at all, which is the sum over all topic selections \(A\) of \(P(B|A)P(A)\). Now we can use Bayes to find \(P(A|B)\), the probability that topic \(A\) generated word \(B\).&lt;/p>
&lt;p>So now we know how to figure out the probability of each topic per word. Now we already know that our documents are assumed to be a mix of topics, but we want to find the most likely dominant topic. The lazy writer generates each word independently of each other word, so the overall probability of a topic throughout the document is the product of \(P(A|B)\) at each word \(B\). Just pick the most likely topic across the words.&lt;/p></description></item></channel></rss>